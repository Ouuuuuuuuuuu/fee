import streamlit as st
import pandas as pd
import datetime
from datetime import date
import requests
import json
import base64
from io import StringIO, BytesIO
import os
import pdfplumber
import re

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="AI æ™ºèƒ½è´¦æœ¬", page_icon="ğŸ’°", layout="wide")

# --- å¸¸é‡é…ç½® ---
DEFAULT_TARGET_SPEND = 60.0  # æ¯æ—¥ä½“é¢æ”¯å‡ºæ ‡å‡†
GITHUB_API_URL = "https://api.github.com"
VISION_MODEL_NAME = "Qwen/Qwen3-VL-8B-Instruct" 
TEXT_MODEL_NAME = "deepseek-ai/DeepSeek-V3.2"

# --- å­˜å‚¨ç±» ---
class DataManager:
    """æ•°æ®ç®¡ç†ç±»ï¼Œæ”¯æŒ GitHub è¿œç¨‹å­˜å‚¨å’Œæœ¬åœ° CSV å­˜å‚¨"""
    def __init__(self, github_token=None, repo=None, filename="ledger.csv"):
        self.github_token = github_token
        if repo and repo.startswith("http"):
            self.repo = repo.rstrip("/").split("github.com/")[-1]
        else:
            self.repo = repo
        self.filename = filename
        self.use_github = bool(github_token and self.repo)

    def load_data(self):
        if self.use_github:
            return self._load_from_github()
        else:
            return self._load_from_local()

    def save_data(self, df, sha=None):
        if self.use_github:
            return self._save_to_github(df, sha)
        else:
            return self._save_to_local(df)

    def _load_from_local(self):
        if os.path.exists(self.filename):
            try:
                return pd.read_csv(self.filename), None
            except:
                return pd.DataFrame(columns=["æ—¥æœŸ", "ç±»å‹", "é‡‘é¢", "å¤‡æ³¨", "åˆ†ç±»"]), None
        return pd.DataFrame(columns=["æ—¥æœŸ", "ç±»å‹", "é‡‘é¢", "å¤‡æ³¨", "åˆ†ç±»"]), None

    def _save_to_local(self, df):
        df.to_csv(self.filename, index=False)
        return True

    def _load_from_github(self):
        headers = {
            "Authorization": f"token {self.github_token}",
            "Accept": "application/vnd.github.v3+json"
        }
        url = f"{GITHUB_API_URL}/repos/{self.repo}/contents/{self.filename}"
        response = requests.get(url, headers=headers)
        
        if response.status_code == 200:
            content = response.json()
            csv_str = base64.b64decode(content['content']).decode('utf-8')
            try:
                return pd.read_csv(StringIO(csv_str)), content['sha']
            except pd.errors.EmptyDataError:
                return pd.DataFrame(columns=["æ—¥æœŸ", "ç±»å‹", "é‡‘é¢", "å¤‡æ³¨", "åˆ†ç±»"]), content['sha']
        elif response.status_code == 404:
            return pd.DataFrame(columns=["æ—¥æœŸ", "ç±»å‹", "é‡‘é¢", "å¤‡æ³¨", "åˆ†ç±»"]), None
        else:
            st.error(f"GitHub è¯»å–é”™è¯¯: {response.status_code}")
            return pd.DataFrame(columns=["æ—¥æœŸ", "ç±»å‹", "é‡‘é¢", "å¤‡æ³¨", "åˆ†ç±»"]), None

    def _save_to_github(self, df, sha):
        headers = {
            "Authorization": f"token {self.github_token}",
            "Accept": "application/vnd.github.v3+json"
        }
        csv_str = df.to_csv(index=False)
        content_bytes = base64.b64encode(csv_str.encode('utf-8')).decode('utf-8')
        
        url = f"{GITHUB_API_URL}/repos/{self.repo}/contents/{self.filename}"
        data = {
            "message": f"Update ledger {datetime.datetime.now()}",
            "content": content_bytes
        }
        if sha:
            data["sha"] = sha
        response = requests.put(url, headers=headers, data=json.dumps(data))
        return response.status_code in [200, 201]

# --- æ™ºèƒ½è´¦å•è§£æç±» (AIæ ¸å¿ƒç‰ˆ) ---
class BillParser:
    @staticmethod
    def identify_and_parse(file, api_key):
        """æ™ºèƒ½è¯†åˆ«æ–‡ä»¶ç±»å‹å¹¶æå–æ–‡æœ¬ï¼Œäº¤ç»™AIè§£æ"""
        if not api_key:
            return None, "è¯·å…ˆé…ç½® SILICONFLOW_API_KEY ä»¥ä½¿ç”¨ AI è§£æåŠŸèƒ½"

        filename = file.name.lower()
        content_text = ""
        source_type = "æœªçŸ¥æ–‡ä»¶"

        try:
            # 1. æå–æ–‡ä»¶å†…å®¹ä¸ºçº¯æ–‡æœ¬
            if filename.endswith('.csv'):
                source_type = "CSVè´¦å•"
                try:
                    content_text = file.getvalue().decode('utf-8')
                except UnicodeDecodeError:
                    file.seek(0)
                    content_text = file.getvalue().decode('gbk', errors='ignore')
            
            elif filename.endswith(('.xls', '.xlsx')):
                source_type = "Excelè´¦å•"
                # è¯»å–Excelæ‰€æœ‰sheetï¼Œè½¬æ¢ä¸ºCSVå­—ç¬¦ä¸²æ‹¼æ¥
                try:
                    xls = pd.read_excel(file, sheet_name=None)
                    text_parts = []
                    for sheet_name, df in xls.items():
                        # å°†DataFrameè½¬ä¸ºCSVæ–‡æœ¬ï¼Œä¿ç•™ä¸Šä¸‹æ–‡ç»“æ„
                        text_parts.append(f"--- Sheet: {sheet_name} ---\n")
                        text_parts.append(df.to_csv(index=False))
                    content_text = "\n".join(text_parts)
                except Exception as e:
                    return None, f"Excel è¯»å–å¤±è´¥: {e}"

            elif filename.endswith('.pdf'):
                source_type = "PDFè´¦å•"
                try:
                    text_parts = []
                    with pdfplumber.open(file) as pdf:
                        for page in pdf.pages:
                            # ä¼˜å…ˆå°è¯•æå–è¡¨æ ¼
                            tables = page.extract_tables()
                            if tables:
                                for table in tables:
                                    # å°†è¡¨æ ¼è½¬ä¸º CSV æ ¼å¼æ–‡æœ¬
                                    df_table = pd.DataFrame(table)
                                    # æ¸…ç†None
                                    df_table = df_table.fillna("")
                                    text_parts.append(df_table.to_csv(index=False, header=False))
                            else:
                                # æå–çº¯æ–‡æœ¬ä½œä¸ºå…œåº•
                                text_parts.append(page.extract_text() or "")
                    content_text = "\n".join(text_parts)
                except Exception as e:
                    return None, f"PDF è¯»å–å¤±è´¥: {e}"
            else:
                return None, "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼"

            # 2. è°ƒç”¨ AI è¿›è¡Œè§£æ
            if not content_text.strip():
                return None, "æ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–æ— æ³•æå–æ–‡æœ¬"
                
            return BillParser._call_ai_parser(content_text, source_type, api_key)

        except Exception as e:
            return None, f"è§£æè¿‡ç¨‹å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}"

    @staticmethod
    def _call_ai_parser(content_text, source_type, api_key):
        """è°ƒç”¨ DeepSeek-V3.2 è¿›è¡Œç»“æ„åŒ–æå–"""
        
        # æˆªæ–­ä¿æŠ¤ï¼šè™½ç„¶ DeepSeek ä¸Šä¸‹æ–‡å¾ˆé•¿ï¼Œä½†é˜²æ­¢æç«¯å¤§æ–‡ä»¶ï¼Œä¿ç•™å‰ 50000 å­—ç¬¦é€šå¸¸è¶³å¤ŸåŒ…å«ä¸€ä¸ªæœˆè´¦å•çš„å…³é”®ä¿¡æ¯
        # å¦‚æœæ˜¯CSVï¼Œé€šå¸¸å¤´éƒ¨æ˜¯å…³é”®ã€‚å¦‚æœæ˜¯æµæ°´ï¼Œæœ€å¥½èƒ½å¤„ç†æ›´å¤šã€‚
        # è¿™é‡Œè®¾ç½®ä¸º 100k å­—ç¬¦ï¼ŒDeepSeekå¤„ç†å¾—è¿‡æ¥ã€‚
        truncated_content = content_text[:100000]
        
        system_prompt = """
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è´¢åŠ¡æ•°æ®æå–åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä»æ‚ä¹±çš„è´¦å•æ–‡æœ¬ä¸­æå–äº¤æ˜“æµæ°´ã€‚
        è¯·éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š
        1. è¾“å‡ºå¿…é¡»æ˜¯æ ‡å‡†çš„ JSON æ•°ç»„æ ¼å¼ `[{"date": "...", ...}, ...]`ã€‚
        2. ä¸è¦åŒ…å« markdown æ ‡è®°ï¼ˆå¦‚ ```jsonï¼‰ã€‚
        3. å­—æ®µè¯´æ˜ï¼š
           - date: äº¤æ˜“æ—¥æœŸï¼Œæ ¼å¼å¿…é¡»ç»Ÿä¸€ä¸º YYYY-MM-DDã€‚å¦‚æœå¹´ä»½ç¼ºå¤±ï¼Œé»˜è®¤2025å¹´ã€‚
           - type: "æ”¯å‡º" æˆ– "æ”¶å…¥"ã€‚æ ¹æ®é‡‘é¢æ­£è´Ÿæˆ–"æ”¶/æ”¯"åˆ—åˆ¤æ–­ã€‚é€šå¸¸é“¶è¡Œè´¦å•ä¸­è´Ÿæ•°æ˜¯æ”¯å‡ºï¼Œæˆ–è€…åœ¨"æ”¯å‡º"åˆ—çš„æ•°å­—ã€‚
           - amount: é‡‘é¢ç»å¯¹å€¼ï¼ˆæ•°å­—ç±»å‹ï¼Œä¸è¦å­—ç¬¦ä¸²ï¼‰ã€‚
           - merchant: äº¤æ˜“å¯¹è±¡/å•†æˆ·å/æ‘˜è¦ã€‚
           - category: æ ¹æ®å•†æˆ·åæ¨æ–­åˆ†ç±»ï¼ˆå¦‚ï¼šé¤é¥®ã€äº¤é€šã€è´­ç‰©ã€è½¬è´¦ã€å·¥èµ„ã€ç†è´¢ã€è¿˜æ¬¾ã€å…¶ä»–ï¼‰ã€‚
        4. è¿‡æ»¤æ‰æ— æ•ˆè¡Œï¼ˆå¦‚è¡¨å¤´ã€é¡µç ã€ç»Ÿè®¡æ±‡æ€»è¡Œã€ä½™é¢è¡Œï¼‰ã€‚åªä¿ç•™å…·ä½“äº¤æ˜“ã€‚
        5. å¯¹äº"ä¸è®¡æ”¶æ”¯"æˆ–"èµ„é‡‘è½¬ç§»"çš„æ¡ç›®ï¼Œå¦‚æœçœ‹èµ·æ¥åƒä¿¡ç”¨å¡è¿˜æ¬¾ï¼Œæ ‡è®°ä¸º"è½¬è´¦"æˆ–"è¿˜æ¬¾"ï¼Œç±»å‹è‡ªå®šï¼ˆé€šå¸¸ä¸è®°å…¥æ—¥å¸¸æ”¶æ”¯ï¼Œä½†ç”¨æˆ·å¯èƒ½éœ€è¦ï¼‰ã€‚
        6. å¦‚æœæ–‡æœ¬æ˜¯ä¹±ç æˆ–æ— æ³•è¯†åˆ«ä¸ºè´¦å•ï¼Œè¿”å›ç©ºæ•°ç»„ []ã€‚
        """

        user_prompt = f"""
        è¯·å¤„ç†è¿™ä»½ {source_type} æ•°æ®ï¼Œæå–æ‰€æœ‰äº¤æ˜“è®°å½•ã€‚
        
        æ•°æ®å†…å®¹ç‰‡æ®µï¼š
        {truncated_content}
        """

        headers = {
            "Authorization": f"Bearer {api_key}",
            "Content-Type": "application/json"
        }

        payload = {
            "model": TEXT_MODEL_NAME, # ä½¿ç”¨ deepseek-ai/DeepSeek-V3.2
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            "max_tokens": 8192, # å°½å¯èƒ½è¾“å‡ºå®Œæ•´
            "temperature": 0.1  # ä½æ¸©åº¦ä¿è¯å‡†ç¡®æ€§
        }

        try:
            # ä½¿ç”¨ SiliconFlow å…¼å®¹æ¥å£
            response = requests.post(
                "[https://api.siliconflow.cn/v1/chat/completions](https://api.siliconflow.cn/v1/chat/completions)",
                headers=headers,
                json=payload,
                timeout=120 # è§£æå¤§æ–‡ä»¶éœ€è¦æ›´å¤šæ—¶é—´
            )
            
            if response.status_code == 200:
                res_json = response.json()
                ai_content = res_json['choices'][0]['message']['content']
                
                # æ¸…æ´— Markdown
                ai_content = ai_content.replace("```json", "").replace("```", "").strip()
                
                try:
                    data_list = json.loads(ai_content)
                    if not isinstance(data_list, list):
                        return None, "AI è¿”å›æ ¼å¼é”™è¯¯ï¼ˆéæ•°ç»„ï¼‰"
                    
                    if not data_list:
                        return None, "AI æœªèƒ½æå–åˆ°ä»»ä½•æœ‰æ•ˆäº¤æ˜“è®°å½•"

                    # è½¬ä¸º DataFrame å¹¶åšåŸºç¡€æ¸…æ´—
                    df = pd.DataFrame(data_list)
                    
                    # ç¡®ä¿åˆ—å­˜åœ¨
                    required_cols = ["date", "type", "amount", "merchant", "category"]
                    for col in required_cols:
                        if col not in df.columns:
                            df[col] = ""
                    
                    # æ˜ å°„å› app ç»Ÿä¸€çš„åˆ—å
                    df = df.rename(columns={
                        "date": "æ—¥æœŸ",
                        "type": "ç±»å‹",
                        "amount": "é‡‘é¢",
                        "merchant": "å¤‡æ³¨",
                        "category": "åˆ†ç±»"
                    })
                    
                    # æ•°æ®ç±»å‹è½¬æ¢
                    df['é‡‘é¢'] = pd.to_numeric(df['é‡‘é¢'], errors='coerce').fillna(0)
                    # å¼ºåˆ¶ä¿ç•™ AI è¯†åˆ«å‡ºçš„åˆ†ç±»
                    df['åˆ†ç±»'] = df['åˆ†ç±»'].fillna("AIå¯¼å…¥")
                    
                    return df, None
                    
                except json.JSONDecodeError:
                    return None, f"AI è¿”å›äº†é JSON æ•°æ®: {ai_content[:100]}..."
            else:
                return None, f"API è¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}"
                
        except Exception as e:
            return None, f"AI è¯·æ±‚å¼‚å¸¸: {str(e)}"

    @staticmethod
    def merge_and_deduplicate(old_df, new_df):
        """
        åˆå¹¶å¹¶å»é‡
        """
        if new_df is None or new_df.empty:
            return old_df, 0, 0

        added_rows = []
        skipped_count = 0
        
        existing_keys = set()
        for _, row in old_df.iterrows():
            try:
                amt = float(row['é‡‘é¢'])
                key = f"{row['æ—¥æœŸ']}_{amt:.2f}_{row['ç±»å‹']}"
                existing_keys.add(key)
            except:
                continue

        for _, row in new_df.iterrows():
            try:
                amt = float(row['é‡‘é¢'])
                key = f"{row['æ—¥æœŸ']}_{amt:.2f}_{row['ç±»å‹']}"
            except:
                continue
            
            # ç®€å•å»é‡é€»è¾‘ï¼šåªè¦æ—¥æœŸã€é‡‘é¢ã€ç±»å‹å®Œå…¨ä¸€è‡´ï¼Œå°±è®¤ä¸ºæ˜¯é‡å¤
            # AI è§£æåï¼Œå¤‡æ³¨å¯èƒ½å’ŒåŸå§‹ CSV ä¸ä¸€æ ·ï¼Œæ‰€ä»¥ä¸ä½œä¸ºå»é‡ä¸»é”®ï¼Œåªä½œä¸ºè¾…åŠ©
            if key in existing_keys:
                skipped_count += 1
                continue
            
            added_rows.append(row)
            existing_keys.add(key) 

        if not added_rows:
            return old_df, 0, skipped_count
            
        return pd.concat([old_df, pd.DataFrame(added_rows)], ignore_index=True), len(added_rows), skipped_count

# --- AI å¤„ç†å‡½æ•° (å›¾ç‰‡ OCR) ---
def process_bill_image(image_file, api_key):
    if not api_key:
        return None, "æœªé…ç½® API Key"

    image_bytes = image_file.getvalue()
    base64_image = base64.b64encode(image_bytes).decode('utf-8')

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    prompt = """
    è¯·è¯†åˆ«è¿™å¼ è´¦å•å›¾ç‰‡ã€‚æå–ä»¥ä¸‹å­—æ®µå¹¶ä»¥JSONæ ¼å¼è¿”å›ï¼š
    1. date (æ ¼å¼YYYY-MM-DD)
    2. amount (æ•°å­—ç±»å‹ï¼Œä¸è¦å¸¦è´§å¸ç¬¦å·)
    3. merchant (å•†æˆ·åæˆ–äº¤æ˜“è¯´æ˜)
    4. category (ä»ä»¥ä¸‹é€‰æ‹©æœ€æ¥è¿‘çš„: é¤é¥®, äº¤é€š, è´­ç‰©, å±…ä½, å¨±ä¹, å·¥èµ„, å…¶ä»–)
    5. type (æ”¯å‡º æˆ– æ”¶å…¥)
    
    ç›´æ¥è¿”å›JSONï¼Œä¸éœ€è¦ ```json æ ‡è®°ã€‚
    """

    payload = {
        "model": VISION_MODEL_NAME, 
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{base64_image}"
                        }
                    }
                ]
            }
        ],
        "max_tokens": 1024
    }

    try:
        response = requests.post(
            "https://api.siliconflow.cn/v1/chat/completions",
            headers=headers,
            json=payload,
            timeout=45
        )
        if response.status_code == 200:
            result = response.json()
            content = result['choices'][0]['message']['content']
            clean_content = content.replace("```json", "").replace("```", "").strip()
            return json.loads(clean_content), None
        else:
            return None, f"API Error {response.status_code}: {response.text}"
    except Exception as e:
        return None, f"è¯·æ±‚å¼‚å¸¸: {str(e)}"

# --- ä¸»ç¨‹åº ---
def main():
    # 1. é…ç½®åŠ è½½
    st.sidebar.title("âš™ï¸ ä¸ªäººè´¢åŠ¡è®¾ç½®")
    
    sf_api_key = st.secrets.get("SILICONFLOW_API_KEY", "")
    github_token = st.secrets.get("GITHUB_TOKEN", "")
    github_repo = st.secrets.get("GITHUB_REPO", "")

    dm = DataManager(github_token, github_repo)
    
    if dm.use_github:
        st.sidebar.success(f"â˜ï¸ æ•°æ®å­˜å‚¨: GitHub ({github_repo})")
    else:
        st.sidebar.warning("ğŸ“‚ æ•°æ®å­˜å‚¨: æœ¬åœ°æ¨¡å¼")

    payday = st.sidebar.number_input("æ¯æœˆå‘è–ªæ—¥", 1, 31, 10)
    current_cash = st.sidebar.number_input("å½“å‰ç°é‡‘/ä½™é¢", value=3000.0)

    # 2. åŠ è½½æ•°æ®
    if 'ledger_data' not in st.session_state:
        df, sha = dm.load_data()
        st.session_state.ledger_data = df
        st.session_state.github_sha = sha

    # 3. è´¢åŠ¡æ¦‚è§ˆ
    st.title("ğŸ’° æç®€è´¦æœ¬")
    
    today = date.today()
    if today.day >= payday:
        next_pay_date = date(today.year + (1 if today.month == 12 else 0), 1 if today.month == 12 else today.month + 1, payday)
    else:
        next_pay_date = date(today.year, today.month, payday)
    
    days_left = (next_pay_date - today).days
    
    col1, col2, col3 = st.columns(3)
    col1.metric("å½“å‰ä½™é¢", f"Â¥{current_cash:,.2f}")
    col2.metric("è·ç¦»å‘å·¥èµ„", f"{days_left} å¤©")
    
    if days_left > 0:
        daily_budget = current_cash / days_left
        gap = daily_budget - DEFAULT_TARGET_SPEND
        col3.metric("æ¯æ—¥å¯ç”¨", f"Â¥{daily_budget:.1f}", 
                    f"{gap:+.1f} (vs Â¥{DEFAULT_TARGET_SPEND})",
                    delta_color="normal" if gap >= 0 else "inverse")
    else:
        col3.metric("æ¯æ—¥å¯ç”¨", "N/A", "ä»Šæ—¥å‘è–ªï¼")

    st.divider()

    # 4. è®°è´¦åŠŸèƒ½åŒº - ç»Ÿä¸€å…¥å£
    tab_auto, tab_manual = st.tabs(["ğŸ“¤ æ™ºèƒ½å¯¼å…¥ (æ–‡ä»¶/å›¾ç‰‡)", "âœï¸ æ‰‹åŠ¨è®°è´¦"])

    with tab_auto:
        st.markdown("""
        <small>æ”¯æŒæ ¼å¼ï¼š
        1. **å›¾ç‰‡** (jpg/png) -> ä½¿ç”¨ Qwen-VL è§†è§‰æ¨¡å‹è¯†åˆ«
        2. **æ–‡ä»¶** (csv/xlsx/xls/pdf) -> ä½¿ç”¨ DeepSeek-V3.2 æ–‡æœ¬æ¨¡å‹æ™ºèƒ½åˆ†æ (æ”¯æŒæ‰€æœ‰é“¶è¡Œ/æ”¯ä»˜è½¯ä»¶æ ¼å¼)
        </small>
        """, unsafe_allow_html=True)
        
        # å…è®¸ä¸Šä¼ å¤šä¸ªæ–‡ä»¶
        uploaded_files = st.file_uploader(
            "ç‚¹å‡»ä¸Šä¼  (æ”¯æŒå¤šé€‰)", 
            type=['png', 'jpg', 'jpeg', 'csv', 'xlsx', 'xls', 'pdf'], 
            key="unified_upload",
            accept_multiple_files=True
        )
        
        if uploaded_files:
            img_files = [f for f in uploaded_files if f.name.split('.')[-1].lower() in ['png', 'jpg', 'jpeg']]
            data_files = [f for f in uploaded_files if f.name.split('.')[-1].lower() in ['csv', 'xlsx', 'xls', 'pdf']]

            col_a, col_b = st.columns(2)
            
            # --- æ‰¹é‡å¤„ç†æ•°æ®æ–‡ä»¶ (AI æ–‡æœ¬è§£æ) ---
            if data_files:
                with col_a:
                    st.info(f"æ£€æµ‹åˆ° {len(data_files)} ä¸ªæ•°æ®æ–‡ä»¶")
                    if st.button(f"AI æ™ºèƒ½è§£æå¯¼å…¥", key="btn_import_batch"):
                        if not sf_api_key:
                            st.error("è¯·å…ˆé…ç½® SILICONFLOW_API_KEY")
                        else:
                            total_added = 0
                            total_skipped = 0
                            
                            with st.spinner("æ­£åœ¨æå–æ–‡æœ¬å¹¶å‘¼å« DeepSeek è¿›è¡Œåˆ†æ (å¯èƒ½éœ€è¦å‡ åç§’)..."):
                                batch_df = pd.DataFrame()
                                
                                for f in data_files:
                                    # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ä¼ å…¥ api_key
                                    df_new, err = BillParser.identify_and_parse(f, sf_api_key)
                                    if err:
                                        st.error(f"æ–‡ä»¶ {f.name} è§£æå¤±è´¥: {err}")
                                    elif df_new is not None and not df_new.empty:
                                        batch_df = pd.concat([batch_df, df_new], ignore_index=True)
                                
                                if not batch_df.empty:
                                    merged_df, added_count, skipped_count = BillParser.merge_and_deduplicate(
                                        st.session_state.ledger_data, batch_df
                                    )
                                    total_added += added_count
                                    total_skipped += skipped_count
                                    
                                    if total_added > 0:
                                        if dm.save_data(merged_df, st.session_state.get('github_sha')):
                                            st.session_state.ledger_data = merged_df
                                            st.session_state.github_sha = dm.load_data()[1]
                                            st.success(f"ğŸ‰ æˆåŠŸï¼DeepSeek å¸®ä½ æå–äº† {total_added} æ¡æ–°è®°å½•ã€‚")
                                            if total_skipped > 0:
                                                st.info(f"ğŸ›¡ï¸ è‡ªåŠ¨è·³è¿‡äº† {total_skipped} æ¡é‡å¤è®°å½•")
                                            st.rerun()
                                        else:
                                            st.error("ä¿å­˜å¤±è´¥")
                                    else:
                                        st.warning(f"åˆ†æå®Œæˆï¼Œä½†æ‰€æœ‰è®°å½•å‡å·²å­˜åœ¨ (è·³è¿‡ {total_skipped} æ¡)ã€‚")
                                else:
                                    st.warning("AI æ²¡æœ‰å‘ç°æœ‰æ•ˆçš„äº¤æ˜“æ•°æ®ï¼Œå¯èƒ½æ˜¯æ–‡ä»¶å†…å®¹ä¸ºç©ºæˆ–æ ¼å¼è¿‡äºç‰¹æ®Šã€‚")

            # --- æ‰¹é‡/å•å¼  å›¾ç‰‡å¤„ç† (OCR) ---
            if img_files:
                with col_b:
                    st.info(f"æ£€æµ‹åˆ° {len(img_files)} å¼ å›¾ç‰‡")
                    if 'ocr_queue' not in st.session_state:
                        st.session_state.ocr_queue = []
                        
                    if st.button(f"å¼€å§‹ AI è§†è§‰è¯†åˆ«", key="btn_ocr_batch"):
                        if not sf_api_key:
                            st.error("è¯·é…ç½® SILICONFLOW_API_KEY")
                        else:
                            with st.spinner("AI æ­£åœ¨é€å¼ è¯»å–..."):
                                for img_f in img_files:
                                    data, err = process_bill_image(img_f, sf_api_key)
                                    if not err and data:
                                        data['_filename'] = img_f.name
                                        st.session_state.ocr_queue.append(data)
                                    else:
                                        st.error(f"{img_f.name} è¯†åˆ«å¤±è´¥: {err}")
                            st.rerun()

        # --- OCR ç»“æœç¡®è®¤é˜Ÿåˆ— ---
        if 'ocr_queue' in st.session_state and len(st.session_state.ocr_queue) > 0:
            st.divider()
            st.subheader(f"ğŸ” å¾…ç¡®è®¤ OCR ç»“æœ (å‰©ä½™ {len(st.session_state.ocr_queue)} ä¸ª)")
            
            current_ocr = st.session_state.ocr_queue[0]
            
            with st.container(border=True):
                st.caption(f"æ¥æºæ–‡ä»¶: {current_ocr.get('_filename', 'Unknown')}")
                with st.form("ocr_confirm_queue"):
                    c1, c2 = st.columns(2)
                    o_date = c1.date_input("æ—¥æœŸ", pd.to_datetime(current_ocr.get('date', str(date.today()))))
                    o_type = c2.selectbox("ç±»å‹", ["æ”¯å‡º", "æ”¶å…¥"], index=1 if current_ocr.get('type') == 'æ”¶å…¥' else 0)
                    o_amt = c1.number_input("é‡‘é¢", float(current_ocr.get('amount', 0)))
                    o_cat = c2.text_input("åˆ†ç±»", current_ocr.get('category', 'é¤é¥®'))
                    o_desc = st.text_input("å¤‡æ³¨", current_ocr.get('merchant', ''))
                    
                    col_submit, col_skip = st.columns([1, 1])
                    if col_submit.form_submit_button("âœ… ç¡®è®¤æ·»åŠ "):
                        new_row = {"æ—¥æœŸ": str(o_date), "ç±»å‹": o_type, "é‡‘é¢": o_amt, "å¤‡æ³¨": o_desc, "åˆ†ç±»": o_cat}
                        st.session_state.ledger_data = pd.concat([st.session_state.ledger_data, pd.DataFrame([new_row])], ignore_index=True)
                        dm.save_data(st.session_state.ledger_data, st.session_state.get('github_sha'))
                        st.session_state.github_sha = dm.load_data()[1]
                        st.session_state.ocr_queue.pop(0)
                        st.rerun()
                        
                    if col_skip.form_submit_button("ğŸ—‘ï¸ è·³è¿‡æ­¤æ¡"):
                        st.session_state.ocr_queue.pop(0)
                        st.rerun()

    # --- Manual Tab ---
    with tab_manual:
        with st.form("manual_form"):
            c_m1, c_m2 = st.columns(2)
            m_date = c_m1.date_input("æ—¥æœŸ", date.today())
            m_type = c_m2.selectbox("ç±»å‹", ["æ”¯å‡º", "æ”¶å…¥"])
            m_amt = c_m1.number_input("é‡‘é¢", step=1.0)
            m_cat = c_m2.selectbox("åˆ†ç±»", ["é¤é¥®", "äº¤é€š", "è´­ç‰©", "å±…ä½", "å¨±ä¹", "å·¥èµ„", "å…¶ä»–"])
            m_desc = st.text_input("å¤‡æ³¨")
            
            if st.form_submit_button("ğŸ’¾ ä¿å­˜è®°å½•"):
                new_row = {"æ—¥æœŸ": str(m_date), "ç±»å‹": m_type, "é‡‘é¢": m_amt, "å¤‡æ³¨": m_desc, "åˆ†ç±»": m_cat}
                st.session_state.ledger_data = pd.concat([st.session_state.ledger_data, pd.DataFrame([new_row])], ignore_index=True)
                dm.save_data(st.session_state.ledger_data, st.session_state.get('github_sha'))
                st.session_state.github_sha = dm.load_data()[1]
                st.rerun()

    st.divider()

    # 5. å†å²è´¦å• & å¯è§†åŒ–
    if not st.session_state.ledger_data.empty:
        st.subheader("ğŸ“Š å†å²è´¦å•")
        edited_df = st.data_editor(
            st.session_state.ledger_data,
            num_rows="dynamic",
            use_container_width=True,
            key="history_editor"
        )
        if st.button("ğŸ”„ åŒæ­¥è¡¨æ ¼ä¿®æ”¹"):
            if dm.save_data(edited_df, st.session_state.get('github_sha')):
                st.session_state.ledger_data = edited_df
                st.session_state.github_sha = dm.load_data()[1]
                st.success("åŒæ­¥æˆåŠŸ")
                st.rerun()
        
        st.subheader("ğŸ“ˆ æ¶ˆè´¹é€è§†")
        chart_df = st.session_state.ledger_data.copy()
        chart_df['é‡‘é¢'] = pd.to_numeric(chart_df['é‡‘é¢'], errors='coerce').fillna(0)
        chart_df['æ—¥æœŸ'] = pd.to_datetime(chart_df['æ—¥æœŸ']).dt.date
        expense_df = chart_df[chart_df['ç±»å‹'] == 'æ”¯å‡º']
        
        if not expense_df.empty:
            t1, t2 = st.tabs(["ğŸ“Š åˆ†ç±»å æ¯”", "ğŸ“‰ æ¯æ—¥è¶‹åŠ¿"])
            with t1:
                st.bar_chart(expense_df.groupby('åˆ†ç±»')['é‡‘é¢'].sum().sort_values(ascending=False), color="#FF4B4B")
            with t2:
                st.line_chart(expense_df.groupby('æ—¥æœŸ')['é‡‘é¢'].sum())
    else:
        st.info("æš‚æ— æ•°æ®")

    # 6. AI åˆ†æ
    with st.expander("ğŸ¤– AI è´¢åŠ¡åˆ†æ"):
        if st.button("åˆ†ææˆ‘çš„å¼€é”€"):
            if sf_api_key and not st.session_state.ledger_data.empty:
                with st.spinner("AI æ­£åœ¨æ€è€ƒ..."):
                    summary = st.session_state.ledger_data.to_string()
                    payload = {
                        "model": TEXT_MODEL_NAME, 
                        "messages": [{"role": "user", "content": f"åˆ†æè¿™ä»½è´¦å•ï¼ŒæŒ‡å‡ºé—®é¢˜ï¼š\n{summary}"}]
                    }
                    try:
                        r = requests.post("https://api.siliconflow.cn/v1/chat/completions", 
                                        headers={"Authorization": f"Bearer {sf_api_key}"}, json=payload)
                        st.markdown(r.json()['choices'][0]['message']['content'])
                    except Exception as e:
                        st.error(f"AI æœåŠ¡å¼‚å¸¸: {e}")

if __name__ == "__main__":
    main()
