import streamlit as st
import pandas as pd
import datetime
from datetime import date
import requests
import json
import base64
from io import StringIO, BytesIO
import os

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="AI æ™ºèƒ½è´¦æœ¬", page_icon="ğŸ’°", layout="wide")

# --- å¸¸é‡é…ç½® ---
DEFAULT_TARGET_SPEND = 60.0  # æ¯æ—¥ä½“é¢æ”¯å‡ºæ ‡å‡†
GITHUB_API_URL = "https://api.github.com"
VISION_MODEL_NAME = "Qwen/Qwen3-VL-8B-Instruct" 
TEXT_MODEL_NAME = "deepseek-ai/DeepSeek-V3.2"

# --- å­˜å‚¨ç±» ---
class DataManager:
    """æ•°æ®ç®¡ç†ç±»ï¼Œæ”¯æŒ GitHub è¿œç¨‹å­˜å‚¨å’Œæœ¬åœ° CSV å­˜å‚¨"""
    def __init__(self, github_token=None, repo=None, filename="ledger.csv"):
        self.github_token = github_token
        if repo and repo.startswith("http"):
            self.repo = repo.rstrip("/").split("github.com/")[-1]
        else:
            self.repo = repo
        self.filename = filename
        self.use_github = bool(github_token and self.repo)

    def load_data(self):
        if self.use_github:
            return self._load_from_github()
        else:
            return self._load_from_local()

    def save_data(self, df, sha=None):
        if self.use_github:
            return self._save_to_github(df, sha)
        else:
            return self._save_to_local(df)

    def _load_from_local(self):
        if os.path.exists(self.filename):
            try:
                return pd.read_csv(self.filename), None
            except:
                return pd.DataFrame(columns=["æ—¥æœŸ", "ç±»å‹", "é‡‘é¢", "å¤‡æ³¨", "åˆ†ç±»"]), None
        return pd.DataFrame(columns=["æ—¥æœŸ", "ç±»å‹", "é‡‘é¢", "å¤‡æ³¨", "åˆ†ç±»"]), None

    def _save_to_local(self, df):
        df.to_csv(self.filename, index=False)
        return True

    def _load_from_github(self):
        headers = {
            "Authorization": f"token {self.github_token}",
            "Accept": "application/vnd.github.v3+json"
        }
        url = f"{GITHUB_API_URL}/repos/{self.repo}/contents/{self.filename}"
        response = requests.get(url, headers=headers)
        
        if response.status_code == 200:
            content = response.json()
            csv_str = base64.b64decode(content['content']).decode('utf-8')
            try:
                return pd.read_csv(StringIO(csv_str)), content['sha']
            except pd.errors.EmptyDataError:
                return pd.DataFrame(columns=["æ—¥æœŸ", "ç±»å‹", "é‡‘é¢", "å¤‡æ³¨", "åˆ†ç±»"]), content['sha']
        elif response.status_code == 404:
            return pd.DataFrame(columns=["æ—¥æœŸ", "ç±»å‹", "é‡‘é¢", "å¤‡æ³¨", "åˆ†ç±»"]), None
        else:
            st.error(f"GitHub è¯»å–é”™è¯¯: {response.status_code}")
            return pd.DataFrame(columns=["æ—¥æœŸ", "ç±»å‹", "é‡‘é¢", "å¤‡æ³¨", "åˆ†ç±»"]), None

    def _save_to_github(self, df, sha):
        headers = {
            "Authorization": f"token {self.github_token}",
            "Accept": "application/vnd.github.v3+json"
        }
        csv_str = df.to_csv(index=False)
        content_bytes = base64.b64encode(csv_str.encode('utf-8')).decode('utf-8')
        
        url = f"{GITHUB_API_URL}/repos/{self.repo}/contents/{self.filename}"
        data = {
            "message": f"Update ledger {datetime.datetime.now()}",
            "content": content_bytes
        }
        if sha:
            data["sha"] = sha
        response = requests.put(url, headers=headers, data=json.dumps(data))
        return response.status_code in [200, 201]

# --- è´¦å•è§£æä¸å»é‡ç±» ---
class BillParser:
    @staticmethod
    def identify_and_parse(file):
        """æ™ºèƒ½è¯†åˆ«æ–‡ä»¶ç±»å‹å¹¶è§£æ"""
        filename = file.name.lower()
        
        if filename.endswith('.csv'):
            return BillParser._parse_csv(file)
        elif filename.endswith(('.xls', '.xlsx')):
            return BillParser._parse_excel(file)
        else:
            return None, "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œè¯·ä¸Šä¼  CSV æˆ– Excel"

    @staticmethod
    def _parse_csv(file):
        """è§£æ CSV (å¾®ä¿¡/æ”¯ä»˜å®)"""
        try:
            content = file.getvalue().decode('utf-8')
        except UnicodeDecodeError:
            file.seek(0)
            content = file.getvalue().decode('gbk', errors='ignore')

        # ç®€å•çš„ç‰¹å¾æ£€æµ‹
        if "å¾®ä¿¡æ”¯ä»˜è´¦å•æ˜ç»†" in content or "äº¤æ˜“æ—¶é—´" in content:
            return BillParser._parse_wechat_content(content)
        elif "æ”¯ä»˜å®äº¤æ˜“è®°å½•æ˜ç»†" in content or "Partner Transaction ID" in content or "äº¤æ˜“åˆ›å»ºæ—¶é—´" in content:
            # æ”¯ä»˜å®æ ¼å¼è¾ƒå¤šï¼Œå°è¯•é€šç”¨è§£æ
            return BillParser._parse_alipay_content(content)
        elif "æ‹›å•†é“¶è¡Œ" in content:
            # æå°‘è§æ‹›è¡Œå¯¼å‡ºCSVï¼Œä½†é˜²ä¸‡ä¸€
            return None, "è¯·ä¸Šä¼ æ‹›å•†é“¶è¡Œçš„ Excel (xls/xlsx) æ ¼å¼æ–‡ä»¶"
        else:
            # å°è¯•ç›²è§£
            return BillParser._parse_alipay_content(content)

    @staticmethod
    def _parse_excel(file):
        """è§£æ Excel (æ‹›å•†é“¶è¡Œç­‰)"""
        try:
            df = pd.read_excel(file)
        except Exception as e:
            return None, f"Excel è¯»å–å¤±è´¥: {e}"

        # æ‹›å•†é“¶è¡Œç‰¹å¾æ£€æµ‹
        # æ‹›è¡Œè¡¨å¤´é€šå¸¸åŒ…å«: äº¤æ˜“æ—¥æœŸ, äº¤æ˜“æ—¶é—´, æ”¯å‡º, æ”¶å…¥, ä½™é¢, äº¤æ˜“ç±»å‹, äº¤æ˜“å¤‡æ³¨
        # æˆ–è€…: è®°è´¦æ—¥æœŸ, è´§å¸, äº¤æ˜“é‡‘é¢, è”æœºä½™é¢, äº¤æ˜“æ‘˜è¦
        cols = [str(c) for c in df.columns]
        col_str = " ".join(cols)
        
        if "äº¤æ˜“æ—¥æœŸ" in col_str and ("æ”¯å‡º" in col_str or "äº¤æ˜“é‡‘é¢" in col_str):
            return BillParser._parse_cmb(df)
        
        return None, "æœªè¯†åˆ«çš„ Excel è´¦å•æ ¼å¼ï¼Œç›®å‰ä»…ä¼˜åŒ–æ”¯æŒæ‹›å•†é“¶è¡Œã€‚"

    @staticmethod
    def _parse_wechat_content(content):
        lines = content.split('\n')
        start_row = 0
        for i, line in enumerate(lines):
            if "äº¤æ˜“æ—¶é—´" in line:
                start_row = i
                break
        
        try:
            df = pd.read_csv(StringIO(content), header=start_row)
        except:
            return None, "å¾®ä¿¡è´¦å•è§£æå¤±è´¥"

        df.columns = [c.strip() for c in df.columns]
        df = df[df['å½“å‰çŠ¶æ€'] == 'æ”¯ä»˜æˆåŠŸ']
        
        results = []
        for _, row in df.iterrows():
            row_type = row['æ”¶/æ”¯']
            if row_type == "/" or row_type == "ä¸è®¡æ”¶æ”¯": continue
            
            final_type = "æ”¯å‡º" if row_type == "æ”¯å‡º" else "æ”¶å…¥"
            amt = float(str(row['é‡‘é¢(å…ƒ)']).replace('Â¥', '').replace(',', ''))
            
            try:
                d_str = pd.to_datetime(row['äº¤æ˜“æ—¶é—´']).strftime('%Y-%m-%d')
            except:
                continue

            results.append({
                "æ—¥æœŸ": d_str,
                "ç±»å‹": final_type,
                "é‡‘é¢": amt,
                "å¤‡æ³¨": f"{row['äº¤æ˜“å¯¹æ–¹']} - {row['å•†å“']}",
                "åˆ†ç±»": "å¾®ä¿¡å¯¼å…¥"
            })
        return pd.DataFrame(results), None

    @staticmethod
    def _parse_alipay_content(content):
        # æ”¯ä»˜å®å¤„ç†é€»è¾‘
        lines = content.split('\n')
        start_row = 0
        for i, line in enumerate(lines):
            if "äº¤æ˜“æ—¶é—´" in line and "äº¤æ˜“å¯¹æ–¹" in line:
                start_row = i
                break
        
        try:
            df = pd.read_csv(StringIO(content), header=start_row)
        except:
            return None, "æ”¯ä»˜å®è´¦å•è§£æå¤±è´¥"

        df.columns = [c.strip() for c in df.columns]
        if 'äº¤æ˜“çŠ¶æ€' in df.columns:
            df = df[df['äº¤æ˜“çŠ¶æ€'].isin(['äº¤æ˜“æˆåŠŸ', 'æ”¯ä»˜æˆåŠŸ', 'å·²æ”¯å‡º'])]

        results = []
        for _, row in df.iterrows():
            if 'é‡‘é¢' not in row or pd.isna(row['é‡‘é¢']): continue
            row_type = str(row.get('æ”¶/æ”¯', '')).strip()
            if row_type == "ä¸è®¡æ”¶æ”¯" or row_type == "": continue
            
            final_type = "æ”¯å‡º" if row_type == "æ”¯å‡º" else "æ”¶å…¥"
            amt = float(str(row['é‡‘é¢']))
            
            try:
                d_str = pd.to_datetime(row['äº¤æ˜“æ—¶é—´']).strftime('%Y-%m-%d')
            except:
                continue

            results.append({
                "æ—¥æœŸ": d_str,
                "ç±»å‹": final_type,
                "é‡‘é¢": amt,
                "å¤‡æ³¨": f"{row.get('äº¤æ˜“å¯¹æ–¹','')} {row.get('å•†å“è¯´æ˜','')}".strip(),
                "åˆ†ç±»": row.get('äº¤æ˜“åˆ†ç±»', 'æ”¯ä»˜å®å¯¼å…¥')
            })
        return pd.DataFrame(results), None

    @staticmethod
    def _parse_cmb(df):
        """æ‹›å•†é“¶è¡Œ Excel è§£æé€»è¾‘"""
        # å¯»æ‰¾è¡¨å¤´è¡Œ
        header_row_idx = 0
        for i in range(len(df)):
            row_vals = [str(v) for v in df.iloc[i].values]
            if "äº¤æ˜“æ—¥æœŸ" in row_vals or "è®°è´¦æ—¥æœŸ" in row_vals:
                header_row_idx = i
                break
        
        # é‡æ–°è¯»å–ï¼ŒæŒ‡å®šheader
        df.columns = df.iloc[header_row_idx]
        df = df.iloc[header_row_idx+1:]
        df.columns = [str(c).strip() for c in df.columns]
        
        results = []
        for _, row in df.iterrows():
            # æ‹›è¡Œæ ¼å¼å¯èƒ½æœ‰å¤šç§ï¼Œå¸¸è§ä¸€ç§ï¼šäº¤æ˜“æ—¥æœŸ, æ”¯å‡º, æ”¶å…¥, äº¤æ˜“å¤‡æ³¨
            date_val = row.get('äº¤æ˜“æ—¥æœŸ') or row.get('è®°è´¦æ—¥æœŸ')
            if pd.isna(date_val): continue
            
            # æ ¼å¼åŒ–æ—¥æœŸ
            try:
                # æ‹›è¡Œæ—¥æœŸå¯èƒ½æ˜¯ 20230101 æˆ– 2023-01-01
                d_str = pd.to_datetime(str(date_val)).strftime('%Y-%m-%d')
            except:
                continue

            # é‡‘é¢å¤„ç†
            expense = row.get('æ”¯å‡º', 0)
            income = row.get('æ”¶å…¥', 0)
            # æœ‰äº›ç‰ˆæœ¬æ˜¯â€œäº¤æ˜“é‡‘é¢â€å¸¦è´Ÿå·
            trans_amt = row.get('äº¤æ˜“é‡‘é¢', 0)

            final_amt = 0.0
            final_type = "æ”¯å‡º"
            
            if trans_amt != 0:
                trans_amt = float(str(trans_amt).replace(',', ''))
                final_amt = abs(trans_amt)
                final_type = "æ”¯å‡º" if trans_amt < 0 else "æ”¶å…¥"
            elif expense and float(str(expense).replace(',', '')) > 0:
                final_amt = float(str(expense).replace(',', ''))
                final_type = "æ”¯å‡º"
            elif income and float(str(income).replace(',', '')) > 0:
                final_amt = float(str(income).replace(',', ''))
                final_type = "æ”¶å…¥"
            else:
                continue # é‡‘é¢ä¸º0è·³è¿‡

            memo = str(row.get('äº¤æ˜“å¤‡æ³¨') or row.get('äº¤æ˜“æ‘˜è¦') or "")
            
            results.append({
                "æ—¥æœŸ": d_str,
                "ç±»å‹": final_type,
                "é‡‘é¢": final_amt,
                "å¤‡æ³¨": memo.strip(),
                "åˆ†ç±»": "æ‹›è¡Œå¯¼å…¥"
            })
            
        return pd.DataFrame(results), None

    @staticmethod
    def merge_and_deduplicate(old_df, new_df):
        """
        åˆå¹¶å¹¶å»é‡
        ç­–ç•¥ï¼šå¦‚æœ Date + Amount + Type ç›¸åŒï¼Œè§†ä¸ºé‡å¤ã€‚
        é’ˆå¯¹é“¶è¡Œè´¦å•çš„ç‰¹æ®Šå¤„ç†ï¼šå¦‚æœå¤‡æ³¨åŒ…å« 'æ”¯ä»˜å®'/'å¾®ä¿¡' ä¸”é‡‘é¢é‡å¤ï¼Œæ›´è¦è·³è¿‡ã€‚
        """
        if new_df is None or new_df.empty:
            return old_df, 0, 0

        added_rows = []
        skipped_count = 0
        
        # å»ºç«‹ç´¢å¼•ä»¥åŠ é€ŸæŸ¥æ‰¾ (æ—¥æœŸ+é‡‘é¢+ç±»å‹)
        # ä¸ºé¿å…æµ®ç‚¹æ•°ç²¾åº¦é—®é¢˜ï¼Œé‡‘é¢ä¿ç•™2ä½å°æ•°çš„å­—ç¬¦ä¸²ä½œä¸ºKey
        existing_keys = set()
        for _, row in old_df.iterrows():
            key = f"{row['æ—¥æœŸ']}_{float(row['é‡‘é¢']):.2f}_{row['ç±»å‹']}"
            existing_keys.add(key)

        for _, row in new_df.iterrows():
            amt = float(row['é‡‘é¢'])
            key = f"{row['æ—¥æœŸ']}_{amt:.2f}_{row['ç±»å‹']}"
            
            if key in existing_keys:
                # å‘ç°æ½œåœ¨é‡å¤
                # å¦‚æœæ˜¯é“¶è¡Œè´¦å•ï¼Œä¸”åŒ…å«ç¬¬ä¸‰æ–¹æ”¯ä»˜å…³é”®å­—ï¼Œè¿™æ˜¯å…¸å‹çš„â€œé‡åˆè´¦å•â€ï¼Œå¿…é¡»è·³è¿‡
                memo = str(row['å¤‡æ³¨'])
                if "æ‹›è¡Œ" in str(row.get('åˆ†ç±»', '')):
                    if any(k in memo for k in ["æ”¯ä»˜å®", "å¾®ä¿¡", "è´¢ä»˜é€š", "Tenpay", "Alipay"]):
                        skipped_count += 1
                        continue
                
                # å³ä½¿æ²¡æœ‰å…³é”®å­—ï¼Œåªè¦æ—¥æœŸé‡‘é¢å®Œå…¨ä¸€è‡´ï¼Œä¹Ÿè§†ä¸ºé‡å¤ï¼ˆç”¨æˆ·ä¸å¸Œæœ›é‡å¤è®°å½•ï¼‰
                skipped_count += 1
                continue
            else:
                added_rows.append(row)
                existing_keys.add(key) # é¿å…æ–°æ–‡ä»¶å†…éƒ¨è‡ªæˆ‘é‡å¤

        if not added_rows:
            return old_df, 0, skipped_count
            
        return pd.concat([old_df, pd.DataFrame(added_rows)], ignore_index=True), len(added_rows), skipped_count

# --- AI å¤„ç†å‡½æ•° ---
def process_bill_image(image_file, api_key):
    if not api_key:
        return None, "æœªé…ç½® API Key"

    image_bytes = image_file.getvalue()
    base64_image = base64.b64encode(image_bytes).decode('utf-8')

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    prompt = """
    è¯·è¯†åˆ«è¿™å¼ è´¦å•å›¾ç‰‡ã€‚æå–ä»¥ä¸‹å­—æ®µå¹¶ä»¥JSONæ ¼å¼è¿”å›ï¼š
    1. date (æ ¼å¼YYYY-MM-DD)
    2. amount (æ•°å­—ç±»å‹ï¼Œä¸è¦å¸¦è´§å¸ç¬¦å·)
    3. merchant (å•†æˆ·åæˆ–äº¤æ˜“è¯´æ˜)
    4. category (ä»ä»¥ä¸‹é€‰æ‹©æœ€æ¥è¿‘çš„: é¤é¥®, äº¤é€š, è´­ç‰©, å±…ä½, å¨±ä¹, å·¥èµ„, å…¶ä»–)
    5. type (æ”¯å‡º æˆ– æ”¶å…¥)
    
    ç›´æ¥è¿”å›JSONï¼Œä¸éœ€è¦ ```json æ ‡è®°ã€‚
    """

    payload = {
        "model": VISION_MODEL_NAME, 
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{base64_image}"
                        }
                    }
                ]
            }
        ],
        "max_tokens": 512
    }

    try:
        response = requests.post(
            "[https://api.siliconflow.cn/v1/chat/completions](https://api.siliconflow.cn/v1/chat/completions)",
            headers=headers,
            json=payload,
            timeout=45
        )
        if response.status_code == 200:
            result = response.json()
            content = result['choices'][0]['message']['content']
            clean_content = content.replace("```json", "").replace("```", "").strip()
            return json.loads(clean_content), None
        else:
            return None, f"API Error {response.status_code}: {response.text}"
    except Exception as e:
        return None, f"è¯·æ±‚å¼‚å¸¸: {str(e)}"

# --- ä¸»ç¨‹åº ---
def main():
    # 1. é…ç½®åŠ è½½
    st.sidebar.title("âš™ï¸ ä¸ªäººè´¢åŠ¡è®¾ç½®")
    
    sf_api_key = st.secrets.get("SILICONFLOW_API_KEY", "")
    github_token = st.secrets.get("GITHUB_TOKEN", "")
    github_repo = st.secrets.get("GITHUB_REPO", "")

    dm = DataManager(github_token, github_repo)
    
    if dm.use_github:
        st.sidebar.success(f"â˜ï¸ æ•°æ®å­˜å‚¨: GitHub ({github_repo})")
    else:
        st.sidebar.warning("ğŸ“‚ æ•°æ®å­˜å‚¨: æœ¬åœ°æ¨¡å¼")

    payday = st.sidebar.number_input("æ¯æœˆå‘è–ªæ—¥", 1, 31, 10)
    current_cash = st.sidebar.number_input("å½“å‰ç°é‡‘/ä½™é¢", value=3000.0)

    # 2. åŠ è½½æ•°æ®
    if 'ledger_data' not in st.session_state:
        df, sha = dm.load_data()
        st.session_state.ledger_data = df
        st.session_state.github_sha = sha

    # 3. è´¢åŠ¡æ¦‚è§ˆ
    st.title("ğŸ’° æç®€è´¦æœ¬")
    
    today = date.today()
    if today.day >= payday:
        next_pay_date = date(today.year + (1 if today.month == 12 else 0), 1 if today.month == 12 else today.month + 1, payday)
    else:
        next_pay_date = date(today.year, today.month, payday)
    
    days_left = (next_pay_date - today).days
    
    col1, col2, col3 = st.columns(3)
    col1.metric("å½“å‰ä½™é¢", f"Â¥{current_cash:,.2f}")
    col2.metric("è·ç¦»å‘å·¥èµ„", f"{days_left} å¤©")
    
    if days_left > 0:
        daily_budget = current_cash / days_left
        gap = daily_budget - DEFAULT_TARGET_SPEND
        col3.metric("æ¯æ—¥å¯ç”¨", f"Â¥{daily_budget:.1f}", 
                    f"{gap:+.1f} (vs Â¥{DEFAULT_TARGET_SPEND})",
                    delta_color="normal" if gap >= 0 else "inverse")
    else:
        col3.metric("æ¯æ—¥å¯ç”¨", "N/A", "ä»Šæ—¥å‘è–ªï¼")

    st.divider()

    # 4. è®°è´¦åŠŸèƒ½åŒº - ç»Ÿä¸€å…¥å£
    tab_auto, tab_manual = st.tabs(["ğŸ“¤ æ™ºèƒ½å¯¼å…¥ (å›¾ç‰‡/æ–‡ä»¶)", "âœï¸ æ‰‹åŠ¨è®°è´¦"])

    with tab_auto:
        st.markdown("""
        <small>æ”¯æŒæ ¼å¼ï¼š
        1. **å›¾ç‰‡** (jpg/png) -> AI è‡ªåŠ¨è¯†åˆ«
        2. **æ–‡ä»¶** (csv/xlsx/xls) -> å¾®ä¿¡/æ”¯ä»˜å®/æ‹›å•†é“¶è¡Œè´¦å•å¯¼å…¥ (è‡ªåŠ¨å»é‡)
        </small>
        """, unsafe_allow_html=True)
        
        uploaded_file = st.file_uploader("ç‚¹å‡»ä¸Šä¼ è´¦å•æˆ–æˆªå›¾", type=['png', 'jpg', 'jpeg', 'csv', 'xlsx', 'xls'], key="unified_upload")
        
        if uploaded_file:
            file_type = uploaded_file.name.split('.')[-1].lower()
            
            # --- åˆ†æ”¯ A: å›¾ç‰‡å¤„ç† (OCR) ---
            if file_type in ['png', 'jpg', 'jpeg']:
                if st.button("å¼€å§‹ AI è¯†åˆ«", key="btn_ocr"):
                    if not sf_api_key:
                        st.error("è¯·é…ç½® SILICONFLOW_API_KEY")
                    else:
                        with st.spinner("AI æ­£åœ¨è¯»å–è´¦å•..."):
                            data, err = process_bill_image(uploaded_file, sf_api_key)
                            if err:
                                st.error(err)
                            else:
                                st.session_state.temp_ocr_data = data
                
                # OCR ç»“æœç¡®è®¤æ¡†
                if 'temp_ocr_data' in st.session_state:
                    res = st.session_state.temp_ocr_data
                    with st.form("ocr_confirm"):
                        st.write("ğŸ” è¯†åˆ«ç»“æœç¡®è®¤ï¼š")
                        o_date = st.date_input("æ—¥æœŸ", pd.to_datetime(res.get('date', str(date.today()))))
                        o_type = st.selectbox("ç±»å‹", ["æ”¯å‡º", "æ”¶å…¥"], index=1 if res.get('type') == 'æ”¶å…¥' else 0)
                        o_amt = st.number_input("é‡‘é¢", float(res.get('amount', 0)))
                        o_cat = st.text_input("åˆ†ç±»", res.get('category', 'é¤é¥®'))
                        o_desc = st.text_input("å¤‡æ³¨", res.get('merchant', ''))
                        
                        if st.form_submit_button("âœ… ç¡®è®¤æ·»åŠ "):
                            new_row = {"æ—¥æœŸ": str(o_date), "ç±»å‹": o_type, "é‡‘é¢": o_amt, "å¤‡æ³¨": o_desc, "åˆ†ç±»": o_cat}
                            st.session_state.ledger_data = pd.concat([st.session_state.ledger_data, pd.DataFrame([new_row])], ignore_index=True)
                            dm.save_data(st.session_state.ledger_data, st.session_state.get('github_sha'))
                            st.session_state.github_sha = dm.load_data()[1]
                            del st.session_state.temp_ocr_data
                            st.rerun()

            # --- åˆ†æ”¯ B: æ–‡ä»¶å¯¼å…¥ (CSV/Excel) ---
            elif file_type in ['csv', 'xlsx', 'xls']:
                if st.button("è§£æå¹¶å¯¼å…¥æ–‡ä»¶", key="btn_import"):
                    with st.spinner("æ­£åœ¨è§£æ..."):
                        df_new, err = BillParser.identify_and_parse(uploaded_file)
                        
                        if err:
                            st.error(err)
                        elif df_new is not None and not df_new.empty:
                            # æ‰§è¡Œåˆå¹¶ä¸å»é‡
                            merged_df, added_count, skipped_count = BillParser.merge_and_deduplicate(
                                st.session_state.ledger_data, df_new
                            )
                            
                            if added_count > 0:
                                if dm.save_data(merged_df, st.session_state.get('github_sha')):
                                    st.session_state.ledger_data = merged_df
                                    st.session_state.github_sha = dm.load_data()[1]
                                    st.success(f"ğŸ‰ æˆåŠŸå¯¼å…¥ {added_count} æ¡è®°å½•ï¼")
                                    if skipped_count > 0:
                                        st.info(f"ğŸ›¡ï¸ è‡ªåŠ¨è·³è¿‡äº† {skipped_count} æ¡é‡å¤è®°å½• (åŒ…å«å¾®ä¿¡/æ”¯ä»˜å®ä¸æ‹›è¡Œé‡åˆéƒ¨åˆ†)")
                                    st.rerun()
                                else:
                                    st.error("ä¿å­˜å¤±è´¥")
                            else:
                                st.warning(f"æœªæ·»åŠ ä»»ä½•è®°å½•ã€‚æ£€æµ‹åˆ° {skipped_count} æ¡é‡å¤æ•°æ®ã€‚")

    # --- Manual Tab ---
    with tab_manual:
        with st.form("manual_form"):
            c_m1, c_m2 = st.columns(2)
            m_date = c_m1.date_input("æ—¥æœŸ", date.today())
            m_type = c_m2.selectbox("ç±»å‹", ["æ”¯å‡º", "æ”¶å…¥"])
            m_amt = c_m1.number_input("é‡‘é¢", step=1.0)
            m_cat = c_m2.selectbox("åˆ†ç±»", ["é¤é¥®", "äº¤é€š", "è´­ç‰©", "å±…ä½", "å¨±ä¹", "å·¥èµ„", "å…¶ä»–"])
            m_desc = st.text_input("å¤‡æ³¨")
            
            if st.form_submit_button("ğŸ’¾ ä¿å­˜è®°å½•"):
                new_row = {"æ—¥æœŸ": str(m_date), "ç±»å‹": m_type, "é‡‘é¢": m_amt, "å¤‡æ³¨": m_desc, "åˆ†ç±»": m_cat}
                st.session_state.ledger_data = pd.concat([st.session_state.ledger_data, pd.DataFrame([new_row])], ignore_index=True)
                dm.save_data(st.session_state.ledger_data, st.session_state.get('github_sha'))
                st.session_state.github_sha = dm.load_data()[1]
                st.rerun()

    st.divider()

    # 5. å†å²è´¦å• & å¯è§†åŒ–
    if not st.session_state.ledger_data.empty:
        st.subheader("ğŸ“Š å†å²è´¦å•")
        edited_df = st.data_editor(
            st.session_state.ledger_data,
            num_rows="dynamic",
            use_container_width=True,
            key="history_editor"
        )
        if st.button("ğŸ”„ åŒæ­¥è¡¨æ ¼ä¿®æ”¹"):
            if dm.save_data(edited_df, st.session_state.get('github_sha')):
                st.session_state.ledger_data = edited_df
                st.session_state.github_sha = dm.load_data()[1]
                st.success("åŒæ­¥æˆåŠŸ")
                st.rerun()
        
        st.subheader("ğŸ“ˆ æ¶ˆè´¹é€è§†")
        chart_df = st.session_state.ledger_data.copy()
        chart_df['é‡‘é¢'] = pd.to_numeric(chart_df['é‡‘é¢'], errors='coerce').fillna(0)
        chart_df['æ—¥æœŸ'] = pd.to_datetime(chart_df['æ—¥æœŸ']).dt.date
        expense_df = chart_df[chart_df['ç±»å‹'] == 'æ”¯å‡º']
        
        if not expense_df.empty:
            t1, t2 = st.tabs(["ğŸ“Š åˆ†ç±»å æ¯”", "ğŸ“‰ æ¯æ—¥è¶‹åŠ¿"])
            with t1:
                st.bar_chart(expense_df.groupby('åˆ†ç±»')['é‡‘é¢'].sum().sort_values(ascending=False), color="#FF4B4B")
            with t2:
                st.line_chart(expense_df.groupby('æ—¥æœŸ')['é‡‘é¢'].sum())
    else:
        st.info("æš‚æ— æ•°æ®")

    # 6. AI åˆ†æ
    with st.expander("ğŸ¤– AI è´¢åŠ¡åˆ†æ"):
        if st.button("åˆ†ææˆ‘çš„å¼€é”€"):
            if sf_api_key and not st.session_state.ledger_data.empty:
                with st.spinner("AI æ­£åœ¨æ€è€ƒ..."):
                    summary = st.session_state.ledger_data.to_string()
                    payload = {
                        "model": TEXT_MODEL_NAME, 
                        "messages": [{"role": "user", "content": f"åˆ†æè¿™ä»½è´¦å•ï¼ŒæŒ‡å‡ºé—®é¢˜ï¼š\n{summary}"}]
                    }
                    try:
                        r = requests.post("[https://api.siliconflow.cn/v1/chat/completions](https://api.siliconflow.cn/v1/chat/completions)", 
                                        headers={"Authorization": f"Bearer {sf_api_key}"}, json=payload)
                        st.markdown(r.json()['choices'][0]['message']['content'])
                    except Exception as e:
                        st.error(f"AI æœåŠ¡å¼‚å¸¸: {e}")

if __name__ == "__main__":
    main()
