import streamlit as st
import pandas as pd
import datetime
from datetime import date
import requests
import json
import base64
from io import StringIO, BytesIO
import os
import fitz  # PyMuPDF
import re
from openai import OpenAI
import concurrent.futures
import time
import plotly.express as px
import plotly.graph_objects as go

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="AI æ™ºèƒ½è´¦æœ¬ Pro (10å·è´¦æœŸç‰ˆ)", page_icon="ğŸ’°", layout="wide")

# --- å¸¸é‡é…ç½® ---
GITHUB_API_URL = "https://api.github.com"
VISION_MODEL_NAME = "Qwen/Qwen3-VL-8B-Instruct" 
TEXT_MODEL_NAME = "deepseek-ai/DeepSeek-V3.2"
CHUNK_SIZE = 12000 
BILL_CYCLE_DAY = 10  # è´¦å•æ—¥ï¼šæ¯æœˆ10å·

# --- æ ‡å‡†åˆ†ç±»å®šä¹‰ ---
# æ ¼å¼ï¼š "æ ‡å‡†åˆ†ç±»": ["å…³é”®è¯1", "å…³é”®è¯2", ...]
CATEGORY_MAPPING = {
    "é¤é¥®ç¾é£Ÿ": ["éº¦å½“åŠ³", "è‚¯å¾·åŸº", "é¥¿äº†ä¹ˆ", "ç¾å›¢", "æ˜Ÿå·´å…‹", "ç‘å¹¸", "é¥­", "é¢", "åƒ", "é¥®", "çƒ§çƒ¤", "ç«é”…", "é£Ÿå“", "èœ", "é…’", "èŒ¶", "å…»ç”Ÿå°é£ŸåŠ"],
    "äº¤é€šå‡ºè¡Œ": ["æ»´æ»´", "æ‰“è½¦", "åœ°é“", "å…¬äº¤", "äº¤é€š", "åŠ æ²¹", "åœè½¦", "é“è·¯", "è½¦", "æœºç¥¨", "ä¸€å¡é€š"],
    "è´­ç‰©æ¶ˆè´¹": ["è¶…å¸‚", "ä¾¿åˆ©åº—", "äº¬ä¸œ", "æ·˜å®", "å¤©çŒ«", "æ‹¼å¤šå¤š", "å•†æˆ·æ¶ˆè´¹", "æ‰«äºŒç»´ç ä»˜æ¬¾", "7-11", "å…¨å®¶"],
    "ç”Ÿæ´»æœåŠ¡": ["è¯è´¹", "ç”µè´¹", "æ°´è´¹", "ç‡ƒæ°”", "å®½å¸¦", "ç†å‘", "æ´—", "å……å€¼ç¼´è´¹"],
    "å¨±ä¹ä¼‘é—²": ["ç”µå½±", "æ¸¸æˆ", "ä¼šå‘˜", "è§†é¢‘", "KTV", "ç½‘å§", "ç©", "æ¸©æ³‰", "é¾™æ‚¦é…’åº—"],
    "å·¥èµ„æ”¶å…¥": ["å·¥èµ„", "è–ª", "å¥–é‡‘", "è¡¥åŠ©", "æŠ¥é”€", "è½§å·®"],
    "è½¬è´¦çº¢åŒ…": ["çº¢åŒ…", "è½¬è´¦", "é€€æ¬¾"],
    "å…¶ä»–": []  # å…œåº•
}

# --- æ ¸å¿ƒå·¥å…·ï¼šOpenAI Client ---
def get_llm_client(api_key):
    return OpenAI(api_key=api_key, base_url="https://api.siliconflow.cn/v1")

# --- è¾…åŠ©é€»è¾‘ï¼šè®¡ç®—è´¦æœŸèŒƒå›´ ---
def get_fiscal_range(current_date, cycle_day=BILL_CYCLE_DAY):
    """
    æ ¹æ®ç»™å®šçš„æ—¥æœŸå’Œè´¦å•æ—¥ï¼Œè®¡ç®—æ‰€å±çš„è´¦æœŸèŒƒå›´ã€‚
    é€»è¾‘ï¼šå¦‚æœä»Šå¤© >= 10å·ï¼Œåˆ™è´¦æœŸæ˜¯ æœ¬æœˆ10å· åˆ° ä¸‹æœˆ9å·
          å¦‚æœä»Šå¤© < 10å·ï¼Œåˆ™è´¦æœŸæ˜¯ ä¸Šæœˆ10å· åˆ° æœ¬æœˆ9å·
    """
    if isinstance(current_date, str):
        current_date = datetime.datetime.strptime(current_date, "%Y-%m-%d").date()
    elif isinstance(current_date, datetime.datetime):
        current_date = current_date.date()

    if current_date.day >= cycle_day:
        start_date = date(current_date.year, current_date.month, cycle_day)
        # ä¸‹ä¸ªæœˆ
        if current_date.month == 12:
            end_date = date(current_date.year + 1, 1, cycle_day) - datetime.timedelta(days=1)
        else:
            end_date = date(current_date.year, current_date.month + 1, cycle_day) - datetime.timedelta(days=1)
    else:
        # ä¸Šä¸ªæœˆ
        if current_date.month == 1:
            start_date = date(current_date.year - 1, 12, cycle_day)
        else:
            start_date = date(current_date.year, current_date.month - 1, cycle_day)
        end_date = date(current_date.year, current_date.month, cycle_day) - datetime.timedelta(days=1)
    
    return start_date, end_date

# --- è¾…åŠ©é€»è¾‘ï¼šè‡ªåŠ¨åˆ†ç±» ---
def auto_categorize(row):
    """åŸºäºå¤‡æ³¨å’ŒåŸå§‹åˆ†ç±»ï¼Œè‡ªåŠ¨å½’ç±»åˆ°æ ‡å‡†åˆ†ç±»"""
    # å¦‚æœå·²ç»æ˜¯æ ‡å‡†åˆ†ç±»ï¼Œç›´æ¥è¿”å›
    if row['åˆ†ç±»'] in CATEGORY_MAPPING.keys():
        return row['åˆ†ç±»']

    # ç»„åˆæœç´¢æ–‡æœ¬ï¼šå¤‡æ³¨ + åŸå§‹åˆ†ç±»
    text = f"{str(row['å¤‡æ³¨'])} {str(row['åˆ†ç±»'])}".lower()
    
    # ä¼˜å…ˆåŒ¹é…å…·ä½“å…³é”®è¯
    for category, keywords in CATEGORY_MAPPING.items():
        for kw in keywords:
            if kw.lower() in text:
                return category
    
    # é»˜è®¤é€»è¾‘
    if row['ç±»å‹'] == 'æ”¶å…¥':
        return "å…¶ä»–æ”¶å…¥"
    
    return "å…¶ä»–" # æ— æ³•è¯†åˆ«å½’ä¸ºå…¶ä»–

# --- å·¥å…·å‡½æ•°ï¼šJSON æå–ä¸ä¿®å¤ ---
def repair_truncated_json(json_str):
    json_str = json_str.strip()
    if json_str.endswith("]"): return json_str
    repair_attempts = ["]", "}]", "\"}]", "0}]", "null}]"]
    if json_str.endswith(","): json_str = json_str[:-1]
    for suffix in repair_attempts:
        try:
            temp_str = json_str + suffix
            json.loads(temp_str)
            return temp_str
        except: continue
    return json_str

def extract_json_from_text(text):
    if not text: return None, "ç©ºå“åº”"
    try:
        text = text.strip()
        code_block_pattern = r"``" + r"`(?:json)?(.*?)``" + r"`"
        match_code = re.search(code_block_pattern, text, re.DOTALL)
        if match_code: text = match_code.group(1).strip()
        else:
            text = re.sub(r'```json\s*', '', text)
            text = re.sub(r'```\s*', '', text)
            text = text.strip()
        
        text = repair_truncated_json(text)
        match_array = re.search(r'\[.*\]', text, re.DOTALL)
        if match_array: text_to_parse = match_array.group()
        else: text_to_parse = text
            
        result = json.loads(text_to_parse)
        if isinstance(result, (list, dict)):
            return result if isinstance(result, list) else [result], None
    except: pass
    return None, "JSONæå–å¤±è´¥"

# --- æ•°æ®ç®¡ç†ç±» ---
class DataManager:
    def __init__(self, github_token=None, repo=None, filename="ledger.csv"):
        self.github_token = github_token
        if repo and repo.startswith("http"):
            self.repo = repo.rstrip("/").split("github.com/")[-1]
        else:
            self.repo = repo
        self.filename = filename
        self.use_github = bool(github_token and self.repo)

    def load_data(self, force_refresh=False):
        if self.use_github:
            if force_refresh: self._fetch_github_content.clear()
            df, sha = self._load_from_github()
        else:
            df, sha = self._load_from_local()
        df = self._clean_df_types(df)
        return df, sha

    def save_data(self, df, sha=None):
        save_df = df.copy()
        if 'æ—¥æœŸ' in save_df.columns:
            save_df['æ—¥æœŸ'] = save_df['æ—¥æœŸ'].astype(str)
        if self.use_github:
            success, new_sha = self._save_to_github(save_df, sha)
            return success, new_sha
        else:
            return self._save_to_local(save_df), None

    @staticmethod
    def merge_data(old_df, new_df):
        if new_df is None or new_df.empty: return old_df, 0
        
        # 1. åº”ç”¨è‡ªåŠ¨åˆ†ç±»æ¸…æ´—
        new_df['åˆ†ç±»'] = new_df.apply(auto_categorize, axis=1)

        def get_fp(d): return d['æ—¥æœŸ'].astype(str) + d['é‡‘é¢'].astype(str) + d['å¤‡æ³¨'].str[:5]
        if old_df.empty: return new_df, len(new_df)
        old_fp = set(get_fp(old_df))
        new_df['_fp'] = get_fp(new_df)
        to_add = new_df[~new_df['_fp'].isin(old_fp)].drop(columns=['_fp'])
        if to_add.empty: return old_df, 0
        merged = pd.concat([old_df, to_add], ignore_index=True)
        merged = DataManager._clean_df_types(merged)
        merged = merged.sort_values('æ—¥æœŸ', ascending=False).reset_index(drop=True)
        return merged, len(to_add)

    @staticmethod
    def _clean_df_types(df):
        expected_cols = ["æ—¥æœŸ", "ç±»å‹", "é‡‘é¢", "å¤‡æ³¨", "åˆ†ç±»"]
        for col in expected_cols:
            if col not in df.columns: df[col] = ""
        df['é‡‘é¢'] = pd.to_numeric(df['é‡‘é¢'], errors='coerce').fillna(0.0)
        df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'], errors='coerce')
        df['æ—¥æœŸ'] = df['æ—¥æœŸ'].fillna(pd.Timestamp(date.today()))
        df['æ—¥æœŸ'] = df['æ—¥æœŸ'].dt.date
        df['ç±»å‹'] = df['ç±»å‹'].astype(str).replace('nan', 'æ”¯å‡º')
        # å¦‚æœè¯»å–æ—¶åˆ†ç±»ä¸ºç©ºæˆ–ä¸æ ‡å‡†ï¼Œä¹Ÿå¯ä»¥åœ¨è¿™é‡Œå†æ´—ä¸€æ¬¡ï¼Œä½†ä¸€èˆ¬åœ¨mergeæ—¶åš
        df['åˆ†ç±»'] = df['åˆ†ç±»'].astype(str).replace('nan', 'å…¶ä»–')
        df['å¤‡æ³¨'] = df['å¤‡æ³¨'].astype(str).replace('nan', '')
        return df

    def _load_from_local(self):
        if os.path.exists(self.filename):
            try: return pd.read_csv(self.filename), None
            except: pass
        return self._create_empty_df(), None

    def _save_to_local(self, df):
        df.to_csv(self.filename, index=False)
        return True

    @st.cache_data(ttl=300, show_spinner=False)
    def _fetch_github_content(_self):
        headers = {"Authorization": f"token {_self.github_token}", "Accept": "application/vnd.github.v3+json"}
        url = f"{GITHUB_API_URL}/repos/{_self.repo}/contents/{_self.filename}"
        try:
            response = requests.get(url, headers=headers, timeout=30)
            if response.status_code == 200: return response.json(), None
            elif response.status_code == 404: return None, 404
            else: return None, response.status_code
        except Exception as e: return None, str(e)

    def _load_from_github(self):
        content, error = self._fetch_github_content()
        if content:
            try:
                csv_str = base64.b64decode(content['content']).decode('utf-8')
                df = pd.read_csv(StringIO(csv_str))
                return df, content['sha']
            except: return self._create_empty_df(), content['sha']
        return self._create_empty_df(), None

    def _save_to_github(self, df, sha):
        headers = {"Authorization": f"token {self.github_token}", "Accept": "application/vnd.github.v3+json"}
        url = f"{GITHUB_API_URL}/repos/{self.repo}/contents/{self.filename}"
        csv_str = df.to_csv(index=False)
        content_bytes = base64.b64encode(csv_str.encode('utf-8')).decode('utf-8')
        data = {"message": f"Update ledger", "content": content_bytes}
        if sha: data["sha"] = sha
        try:
            resp = requests.put(url, headers=headers, data=json.dumps(data), timeout=30)
            if resp.status_code in [200, 201]:
                self._fetch_github_content.clear()
                return True, resp.json()['content']['sha']
            elif resp.status_code in [409, 422]:
                self._fetch_github_content.clear()
                latest_content, _ = self._fetch_github_content()
                if latest_content:
                    data["sha"] = latest_content['sha']
                    retry = requests.put(url, headers=headers, data=json.dumps(data), timeout=30)
                    if retry.status_code in [200, 201]:
                        self._fetch_github_content.clear()
                        return True, retry.json()['content']['sha']
            return False, None
        except: return False, None

    @staticmethod
    def _create_empty_df():
        return pd.DataFrame(columns=["æ—¥æœŸ", "ç±»å‹", "é‡‘é¢", "å¤‡æ³¨", "åˆ†ç±»"])

# --- AI è§£æå™¨ ---
class BillParser:
    @staticmethod
    def chunk_text_by_lines(text, max_chars=CHUNK_SIZE):
        if len(text) <= max_chars: return [text]
        lines = text.split('\n')
        chunks, current_chunk, current_len = [], [], 0
        for line in lines:
            line_len = len(line) + 1
            if current_len + line_len > max_chars:
                if current_chunk: chunks.append("\n".join(current_chunk))
                current_chunk = [line]; current_len = line_len
            else:
                current_chunk.append(line); current_len += line_len
        if current_chunk: chunks.append("\n".join(current_chunk))
        return chunks

    @staticmethod
    def _call_llm_for_text(text_chunk, api_key):
        client = get_llm_client(api_key)
        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„è´¢åŠ¡ä¸“å®¶ã€‚
        ä»»åŠ¡ï¼šä»æ–‡æœ¬æå–äº¤æ˜“ã€‚
        æ ‡å‡†åˆ†ç±»ï¼š{list(CATEGORY_MAPPING.keys())}ã€‚
        è¦æ±‚ï¼š
        1. ä»…æå–å«æ—¥æœŸã€é‡‘é¢çš„è¡Œã€‚
        2. æ ¹æ®å¤‡æ³¨æˆ–å•†æˆ·åï¼Œ**å¿…é¡»**å°†åˆ†ç±»æ˜ å°„åˆ°ä¸Šè¿°æ ‡å‡†åˆ†ç±»ä¹‹ä¸€ã€‚
        3. è¿”å›çº¯JSONæ•°ç»„: [{{"date":"YYYY-MM-DD","type":"æ”¯å‡º/æ”¶å…¥","amount":æ•°å­—,"merchant":"å¤‡æ³¨","category":"æ ‡å‡†åˆ†ç±»"}}]
        æ–‡æœ¬ï¼š{text_chunk}
        """
        try:
            resp = client.chat.completions.create(
                model=TEXT_MODEL_NAME, messages=[{"role": "user", "content": prompt}], max_tokens=4096, temperature=0.0
            )
            return resp.choices[0].message.content, None
        except Exception as e: return None, str(e)

    @staticmethod
    def identify_and_parse(filename, file_bytes, api_key):
        try:
            content_text = ""
            if filename.endswith('.csv'):
                try: content_text = file_bytes.decode('utf-8')
                except: content_text = file_bytes.decode('gbk', errors='ignore')
            elif filename.endswith(('.xls', '.xlsx')):
                xls = pd.read_excel(BytesIO(file_bytes), sheet_name=None)
                content_text = "\n".join([f"{s}\n{d.to_csv(index=False)}" for s, d in xls.items()])
            elif filename.endswith('.pdf'):
                with fitz.open(stream=file_bytes, filetype="pdf") as doc:
                    content_text = "\n".join([p.get_text() for p in doc])
            
            if not content_text.strip(): return None, "ç©ºæ–‡ä»¶", {}

            chunks = BillParser.chunk_text_by_lines(content_text, CHUNK_SIZE)
            all_data = []
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
                futures = {executor.submit(BillParser._call_llm_for_text, chunk, api_key): chunk for chunk in chunks}
                for future in concurrent.futures.as_completed(futures):
                    res, err = future.result()
                    if not err:
                        data, _ = extract_json_from_text(res)
                        if data: all_data.extend(data)
            
            if not all_data: return None, "æœªæå–åˆ°æ•°æ®", {}
            
            df = pd.DataFrame(all_data)
            cols = {"date": "æ—¥æœŸ", "type": "ç±»å‹", "amount": "é‡‘é¢", "merchant": "å¤‡æ³¨", "category": "åˆ†ç±»"}
            df = df.rename(columns=cols)
            for c in cols.values(): 
                if c not in df.columns: df[c] = ""
            
            df['é‡‘é¢'] = pd.to_numeric(df['é‡‘é¢'], errors='coerce').fillna(0)
            df['æ—¥æœŸ'] = df['æ—¥æœŸ'].astype(str).apply(lambda x: x.split(' ')[0])
            return df, None, {}
        except Exception as e: return None, str(e), {}

    @staticmethod
    def process_image(filename, image_bytes, api_key):
        try:
            b64_img = base64.b64encode(image_bytes).decode('utf-8')
            client = get_llm_client(api_key)
            resp = client.chat.completions.create(
                model=VISION_MODEL_NAME,
                messages=[{
                    "role": "user", 
                    "content": [
                        {"type": "text", "text": f"æå–è´¦å•ã€‚è¯·å½’ç±»ä¸ºï¼š{list(CATEGORY_MAPPING.keys())}ã€‚è¿”å›JSONæ•°ç»„ã€‚"},
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64_img}"}}
                    ]
                }],
                max_tokens=2048
            )
            data, _ = extract_json_from_text(resp.choices[0].message.content)
            if not data: return None, "æ— æ•°æ®", {}
            if isinstance(data, dict): data = [data]
            df = pd.DataFrame(data)
            cols = {"date": "æ—¥æœŸ", "type": "ç±»å‹", "amount": "é‡‘é¢", "merchant": "å¤‡æ³¨", "category": "åˆ†ç±»"}
            df = df.rename(columns=cols)
            for c in cols.values(): 
                if c not in df.columns: df[c] = ""
            return df, None, {}
        except Exception as e: return None, str(e), {}

# --- ä¸»ç¨‹åº ---
def main():
    if 'debug_mode' not in st.session_state: st.session_state.debug_mode = False
    
    st.sidebar.title("âš™ï¸ è®¾ç½®")
    api_key = st.secrets.get("SILICONFLOW_API_KEY") or st.sidebar.text_input("API Key", type="password")
    gh_token = st.secrets.get("GITHUB_TOKEN")
    gh_repo = st.secrets.get("GITHUB_REPO")
    
    dm = DataManager(gh_token, gh_repo)
    
    if dm.use_github:
        if st.sidebar.button("â˜ï¸ å¼ºåˆ¶åŒæ­¥äº‘ç«¯"):
            with st.spinner("åŒæ­¥ä¸­..."):
                df, sha = dm.load_data(force_refresh=True)
                st.session_state.ledger_data = df
                st.session_state.github_sha = sha
                st.success("åŒæ­¥å®Œæˆ")
                st.rerun()
    
    if 'ledger_data' not in st.session_state:
        df, sha = dm.load_data()
        st.session_state.ledger_data = df
        st.session_state.github_sha = sha

    # --- æ ‡é¢˜ä¸è´¦æœŸé€‰æ‹© ---
    st.title("ğŸ’° AI æ™ºèƒ½è´¦æœ¬ Pro")
    
    # é»˜è®¤è´¦æœŸï¼šä»Šå¤©æ‰€å±çš„è´¦æœŸ
    default_start, default_end = get_fiscal_range(date.today())
    
    col_d1, col_d2 = st.columns([2, 1])
    with col_d1:
        st.caption(f"å½“å‰ç»Ÿè®¡å‘¨æœŸ (æ¯æœˆ{BILL_CYCLE_DAY}å·åˆ‡åˆ†)")
        date_range = st.date_input(
            "é€‰æ‹©ç»Ÿè®¡æ—¶é—´æ®µ",
            value=(default_start, default_end),
            format="YYYY-MM-DD"
        )

    # --- æ ¸å¿ƒæŒ‡æ ‡è®¡ç®— ---
    df = st.session_state.ledger_data.copy()
    
    # æŒ‡æ ‡åˆå§‹åŒ–
    current_income = 0.0
    current_expense = 0.0
    net_asset = 0.0
    
    if not df.empty and len(date_range) == 2:
        df['é‡‘é¢'] = pd.to_numeric(df['é‡‘é¢'], errors='coerce').fillna(0)
        df['dt'] = pd.to_datetime(df['æ—¥æœŸ'], errors='coerce').dt.date
        
        # å…¨é‡å‡€èµ„äº§ (ä¸å—æ—¥æœŸç­›é€‰å½±å“)
        net_asset = df[df['ç±»å‹']=='æ”¶å…¥']['é‡‘é¢'].sum() - df[df['ç±»å‹']=='æ”¯å‡º']['é‡‘é¢'].sum()
        
        # ç­›é€‰å½“æœŸæ•°æ®
        start_d, end_d = date_range[0], date_range[1]
        mask_period = (df['dt'] >= start_d) & (df['dt'] <= end_d)
        df_period = df[mask_period]
        
        current_income = df_period[df_period['ç±»å‹']=='æ”¶å…¥']['é‡‘é¢'].sum()
        current_expense = df_period[df_period['ç±»å‹']=='æ”¯å‡º']['é‡‘é¢'].sum()
        
    else:
        df_period = pd.DataFrame()

    # --- é¡¶éƒ¨çœ‹æ¿ ---
    c1, c2, c3, c4 = st.columns(4)
    c1.metric("ğŸ’° å†å²æ€»å‡€å€¼", f"Â¥{net_asset:,.2f}", help="å†å²æ‰€æœ‰æ”¶å…¥ - å†å²æ‰€æœ‰æ”¯å‡º")
    c2.metric("ğŸ“… æœ¬æœŸæ”¯å‡º", f"Â¥{current_expense:,.2f}", delta=f"-{current_expense/max(1, (date_range[1]-date_range[0]).days):.1f}/å¤©", delta_color="inverse")
    c3.metric("ğŸ“… æœ¬æœŸæ”¶å…¥", f"Â¥{current_income:,.2f}")
    c4.metric("ğŸ“Š æœ¬æœŸç»“ä½™", f"Â¥{current_income - current_expense:,.2f}", delta_color="normal")
    
    st.divider()

    t_import, t_add, t_history, t_stats = st.tabs(["ğŸ“¥ æ™ºèƒ½å¯¼å…¥", "âœï¸ æ‰‹åŠ¨è®°è´¦", "ğŸ“‹ å†å²æ˜ç»†", "ğŸ“Š å¯è§†åŒ–æŠ¥è¡¨"])

    with t_import:
        st.info("ğŸ’¡ å¯¼å…¥æ—¶ä¼šè‡ªåŠ¨æ ¹æ®å¤‡æ³¨å…³é”®è¯ï¼ˆå¦‚'éº¦å½“åŠ³'->'é¤é¥®ç¾é£Ÿ'ï¼‰è¿›è¡Œæ ‡å‡†åŒ–å½’ç±»ã€‚")
        files = st.file_uploader("ä¸Šä¼ æ–‡ä»¶ (PDF/CSV/Excel/å›¾ç‰‡)", accept_multiple_files=True)
        if files and st.button("ğŸš€ å¼€å§‹è¯†åˆ«", type="primary"):
            if not api_key: st.error("è¯·é…ç½® API Key"); st.stop()
            
            # ... (ä¿æŒåŸæœ‰çš„å¤šçº¿ç¨‹å¤„ç†é€»è¾‘ä¸å˜ï¼Œè¿™é‡Œç®€åŒ–æ˜¾ç¤º) ...
            # è¿™é‡Œçš„ identify_and_parse å†…éƒ¨å·²ç»è°ƒç”¨äº† auto_categorize é€»è¾‘ï¼ˆé€šè¿‡ prompt æˆ–è€… åå¤„ç†ï¼‰
            # ä¸ºäº†ä¿é™©ï¼Œæˆ‘ä»¬åœ¨ merge_data æ—¶å†æ¬¡åº”ç”¨ auto_categorize
            
            new_df = pd.DataFrame()
            # æ¨¡æ‹Ÿå¤„ç†è¿‡ç¨‹ (å¤ç”¨ä¹‹å‰çš„é€»è¾‘)
            tasks_doc, tasks_img = [], []
            for f in files:
                ext = f.name.split('.')[-1].lower()
                f.seek(0); b = f.read()
                if ext in ['png', 'jpg']: tasks_img.append({"name":f.name, "bytes":b})
                else: tasks_doc.append({"name":f.name, "bytes":b})
            
            with st.status("æ­£åœ¨AIè¯†åˆ«...") as status:
                with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
                    futures = {}
                    for t in tasks_doc: futures[executor.submit(BillParser.identify_and_parse, t['name'], t['bytes'], api_key)] = t['name']
                    for t in tasks_img: futures[executor.submit(BillParser.process_image, t['name'], t['bytes'], api_key)] = t['name']
                    
                    for future in concurrent.futures.as_completed(futures):
                        try:
                            res, err, _ = future.result()
                            if res is not None:
                                new_df = pd.concat([new_df, res], ignore_index=True)
                        except: pass
                status.update(label="å®Œæˆ", state="complete")

            if not new_df.empty:
                merged, added = DataManager.merge_data(st.session_state.ledger_data, new_df)
                if added > 0:
                    ok, sha = dm.save_data(merged, st.session_state.get('github_sha'))
                    if ok:
                        st.session_state.ledger_data = merged
                        st.session_state.github_sha = sha
                        st.success(f"å¯¼å…¥ {added} æ¡")
                    else: st.error("ä¿å­˜å¤±è´¥")
                else: st.warning("æ— æ–°æ•°æ®")

    with t_add:
        with st.form("manual"):
            c1, c2, c3 = st.columns(3)
            d = c1.date_input("æ—¥æœŸ", date.today())
            t = c2.selectbox("ç±»å‹", ["æ”¯å‡º", "æ”¶å…¥"])
            a = c3.number_input("é‡‘é¢", min_value=0.01)
            c4, c5 = st.columns([1,2])
            cat = c4.selectbox("åˆ†ç±»", list(CATEGORY_MAPPING.keys()) + ["å…¶ä»–"])
            rem = c5.text_input("å¤‡æ³¨")
            if st.form_submit_button("ä¿å­˜", width="stretch"):
                row = pd.DataFrame([{"æ—¥æœŸ":str(d),"ç±»å‹":t,"é‡‘é¢":a,"åˆ†ç±»":cat,"å¤‡æ³¨":rem}])
                merged, added = DataManager.merge_data(st.session_state.ledger_data, row)
                ok, sha = dm.save_data(merged, st.session_state.get('github_sha'))
                if ok: 
                    st.session_state.ledger_data = merged
                    st.session_state.github_sha = sha
                    st.success("æˆåŠŸ")

    with t_history:
        if st.session_state.ledger_data.empty: st.info("æ— æ•°æ®")
        else:
            edited = st.data_editor(st.session_state.ledger_data, use_container_width=True, num_rows="dynamic",
                                    column_config={"åˆ†ç±»": st.column_config.SelectboxColumn(options=list(CATEGORY_MAPPING.keys()) + ["å…¶ä»–"])})
            if st.button("ä¿å­˜è¡¨æ ¼"):
                ok, sha = dm.save_data(edited, st.session_state.get('github_sha'))
                if ok:
                    st.session_state.ledger_data = edited
                    st.session_state.github_sha = sha
                    st.success("å·²æ›´æ–°")

    with t_stats:
        if df_period.empty:
            st.info("æœ¬æœŸæš‚æ— æ•°æ®ï¼Œè¯·è°ƒæ•´æ—¶é—´æ®µæˆ–å¯¼å…¥æ•°æ®ã€‚")
        else:
            df_exp = df_period[df_period['ç±»å‹'] == 'æ”¯å‡º']
            
            col_chart1, col_chart2 = st.columns(2)
            
            with col_chart1:
                st.subheader("ğŸ“Š æ”¯å‡ºç»“æ„")
                if not df_exp.empty:
                    fig_pie = px.pie(df_exp, values='é‡‘é¢', names='åˆ†ç±»', hole=0.4, color_discrete_sequence=px.colors.qualitative.Pastel)
                    fig_pie.update_layout(margin=dict(t=0, b=0, l=0, r=0))
                    st.plotly_chart(fig_pie, use_container_width=True)
                else:
                    st.caption("æ— æ”¯å‡ºæ•°æ®")

            with col_chart2:
                st.subheader("ğŸ“‰ æ¯æ—¥æ”¯å‡º")
                if not df_exp.empty:
                    daily = df_exp.groupby("æ—¥æœŸ")['é‡‘é¢'].sum().reset_index()
                    fig_bar = px.bar(daily, x='æ—¥æœŸ', y='é‡‘é¢', color='é‡‘é¢', color_continuous_scale="Blues")
                    fig_bar.update_layout(xaxis_title="", yaxis_title="")
                    st.plotly_chart(fig_bar, use_container_width=True)
                else:
                    st.caption("æ— æ”¯å‡ºæ•°æ®")

            st.divider()
            st.subheader("ğŸ“ˆ èµ„äº§å‡€å€¼è¶‹åŠ¿ (å…¨å‘¨æœŸ)")
            # å‡€å€¼è¶‹åŠ¿ä½¿ç”¨å…¨é‡æ•°æ®ï¼Œå› ä¸ºçœ‹å‡€å€¼é€šå¸¸éœ€è¦çœ‹é•¿æœŸçš„
            if not df.empty:
                df_sorted = df.sort_values('dt')
                df_sorted['net'] = df_sorted.apply(lambda x: x['é‡‘é¢'] if x['ç±»å‹']=='æ”¶å…¥' else -x['é‡‘é¢'], axis=1)
                daily_net = df_sorted.groupby('dt')['net'].sum().reset_index()
                daily_net['asset'] = daily_net['net'].cumsum()
                
                fig_area = px.area(daily_net, x='dt', y='asset', line_shape='spline')
                fig_area.update_layout(xaxis_title="", yaxis_title="å‡€èµ„äº§", showlegend=False)
                fig_area.update_traces(line_color="#2E86C1", fill_color="rgba(46, 134, 193, 0.2)")
                st.plotly_chart(fig_area, use_container_width=True)

if __name__ == "__main__":
    main()
