import streamlit as st
import pandas as pd
import datetime
from datetime import date
import requests
import json
import base64
from io import StringIO, BytesIO
import os
import fitz  # PyMuPDF
import re
from openai import OpenAI, APITimeoutError
import concurrent.futures
import time

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="AI æ™ºèƒ½è´¦æœ¬ Pro", page_icon="ğŸ’°", layout="wide")

# --- å¸¸é‡é…ç½® ---
DEFAULT_TARGET_SPEND = 60.0  # æ¯æ—¥ä½“é¢æ”¯å‡ºæ ‡å‡†
GITHUB_API_URL = "https://api.github.com"
VISION_MODEL_NAME = "Qwen/Qwen3-VL-8B-Instruct" 
TEXT_MODEL_NAME = "deepseek-ai/DeepSeek-V3.2"

# --- æ ¸å¿ƒå·¥å…·ï¼šOpenAI Client (æ— ç¼“å­˜ï¼Œé˜²æ­¢çº¿ç¨‹å®‰å…¨é—®é¢˜) ---
def get_llm_client(api_key):
    return OpenAI(api_key=api_key, base_url="https://api.siliconflow.cn/v1")

# --- å·¥å…·å‡½æ•°ï¼šå¢å¼ºç‰ˆ JSON æå–ï¼ˆå«æˆªæ–­ä¿®å¤ï¼‰ ---
def extract_json_from_text(text, auto_repair=False):
    """å¢å¼ºç‰ˆJSONæå–ï¼Œæ”¯æŒæ›´å¤šå¼‚å¸¸æ ¼å¼ï¼Œè¿”å› (data, error_msg)"""
    if not text: 
        return None, "ç©ºå“åº”"
    
    # ä¿å­˜åŸå§‹æ–‡æœ¬ç”¨äºè°ƒè¯•
    original_preview = text[:200].replace('\n', '\\n')
    
    try:
        # 1. ç§»é™¤æ‰€æœ‰Markdownä»£ç å—æ ‡è®°
        text = text.strip()
        # å®‰å…¨æ„å»ºæ­£åˆ™è¡¨è¾¾å¼ï¼Œé¿å…ä¸‰ä¸ªåå¼•å·ç ´å Markdown æ¸²æŸ“
        # åŒ¹é… ```json ... ``` æˆ– ``` ... ```
        code_block_pattern = r"``" + r"`(?:json)?(.*?)``" + r"`"
        match_code = re.search(code_block_pattern, text, re.DOTALL)
        
        if match_code:
            text = match_code.group(1).strip()
        else:
            # ç®€å•çš„ç§»é™¤å…œåº•
            text = re.sub(r'```json\s*', '', text)
            text = re.sub(r'```\s*', '', text)
            text = text.strip()
        
        # 2. å¦‚æœå†…å®¹æ˜¯[]ï¼Œç›´æ¥è¿”å›
        if text == '[]':
            return [], None

        # 3. å°è¯•å®šä½æ•°ç»„è¾¹ç•Œ (å¤„ç† AI å›å¤ä¸­åŒ…å«å‰åæ–‡çš„æƒ…å†µ)
        match_array = re.search(r'\[.*\]', text, re.DOTALL)
        if match_array:
            text_to_parse = match_array.group()
        else:
            text_to_parse = text
            
        # 4. å°è¯•ç›´æ¥è§£æ
        result = json.loads(text_to_parse)
        if isinstance(result, (list, dict)):
            return result if isinstance(result, list) else [result], None
            
    except Exception as e:
        # 5. å°è¯•ä¿®å¤å¸¸è§é—®é¢˜ï¼šåˆ é™¤æ³¨é‡Š
        try:
            # ç§»é™¤ // æ³¨é‡Š
            text_no_comments = re.sub(r'//.*?\n', '\n', text)
            # ç§»é™¤ /* */ æ³¨é‡Š
            text_no_comments = re.sub(r'/\*.*?\*/', '', text_no_comments, flags=re.DOTALL)
            
            # å†æ¬¡å°è¯•å®šä½æ•°ç»„
            match_array = re.search(r'\[.*\]', text_no_comments, re.DOTALL)
            if match_array:
                text_no_comments = match_array.group()
            
            result = json.loads(text_no_comments)
            return result if isinstance(result, list) else [result], None
        except:
            pass

        # 6. **JSONæˆªæ–­è‡ªåŠ¨ä¿®å¤**
        if auto_repair:
            try:
                # æ£€æµ‹æ˜¯å¦ä¸ºæˆªæ–­çš„æ•°ç»„
                if text_to_parse.strip().endswith(','):
                    # å¯èƒ½æ˜¯æ•°ç»„å…ƒç´ æœªå®Œæ•´
                    last_bracket = text_to_parse.rfind('{')
                    if last_bracket != -1:
                        # ç§»é™¤æœ€åä¸€ä¸ªä¸å®Œæ•´çš„å¯¹è±¡
                        text_to_parse = text_to_parse[:last_bracket].strip()
                        if text_to_parse.endswith(','):
                            text_to_parse = text_to_parse[:-1]
                        # å…³é—­æ•°ç»„
                        text_to_parse += ']'
                        result = json.loads(text_to_parse)
                        return result, None
                elif text_to_parse.strip().endswith('}'):
                    # å¯èƒ½æ˜¯æœ€åä¸€ä¸ªå¯¹è±¡å®Œæ•´ä½†ç¼ºå°‘é—­åˆæ‹¬å·
                    text_to_parse += ']'
                    result = json.loads(text_to_parse)
                    return result, None
                elif not text_to_parse.strip().endswith(']'):
                    # å…¶ä»–æƒ…å†µï¼Œå°è¯•ç›´æ¥è¡¥å…¨
                    text_to_parse = text_to_parse.strip() + ']'
                    result = json.loads(text_to_parse)
                    return result, None
            except:
                pass
    
    return None, f"è§£æå¤±è´¥ï¼Œéæ ‡å‡†JSONæ ¼å¼ã€‚é¢„è§ˆ: {original_preview}..."

# --- æ•°æ®ç®¡ç†ç±» (å¢å¼ºç‰ˆ) ---
class DataManager:
    def __init__(self, github_token=None, repo=None, filename="ledger.csv"):
        self.github_token = github_token
        if repo and repo.startswith("http"):
            self.repo = repo.rstrip("/").split("github.com/")[-1]
        else:
            self.repo = repo
        self.filename = filename
        self.use_github = bool(github_token and self.repo)

    def load_data(self, force_refresh=False):
        """åŠ è½½æ•°æ®ï¼Œæ”¯æŒå¼ºåˆ¶åˆ·æ–°"""
        if self.use_github:
            if force_refresh:
                # æ¸…é™¤ Streamlit ç¼“å­˜
                self._fetch_github_content.clear()
            df, sha = self._load_from_github()
        else:
            df, sha = self._load_from_local()
        
        # ç»Ÿä¸€è¿›è¡Œç±»å‹æ¸…æ´—ï¼Œé˜²æ­¢ data_editor æŠ¥é”™
        df = self._clean_df_types(df)
        return df, sha

    def save_data(self, df, sha=None):
        """ä¿å­˜æ•°æ®ï¼Œå¸¦è‡ªåŠ¨é‡è¯•æœºåˆ¶"""
        # ä¿å­˜å‰ç¡®ä¿æ ¼å¼åŒ–ä¸ºå­—ç¬¦ä¸²ï¼Œæ–¹ä¾¿ CSV å­˜å‚¨
        save_df = df.copy()
        if 'æ—¥æœŸ' in save_df.columns:
            save_df['æ—¥æœŸ'] = save_df['æ—¥æœŸ'].astype(str)
            
        if self.use_github:
            success, new_sha = self._save_to_github(save_df, sha)
            return success, new_sha
        else:
            return self._save_to_local(save_df), None

    @staticmethod
    def _clean_df_types(df):
        """æ¸…æ´—æ•°æ®ç±»å‹ï¼Œç¡®ä¿å…¼å®¹ st.data_editor"""
        # 1. è¡¥å…¨åˆ—
        expected_cols = ["æ—¥æœŸ", "ç±»å‹", "é‡‘é¢", "å¤‡æ³¨", "åˆ†ç±»"]
        for col in expected_cols:
            if col not in df.columns:
                df[col] = ""
        
        # 2. å¼ºåˆ¶è½¬æ¢é‡‘é¢ä¸º float (å¤„ç†ç©ºå­—ç¬¦ä¸²ã€éæ•°å­—å­—ç¬¦)
        df['é‡‘é¢'] = pd.to_numeric(df['é‡‘é¢'], errors='coerce').fillna(0.0)
        
        # 3. å¼ºåˆ¶è½¬æ¢æ—¥æœŸ (ä¿®å¤ StreamlitAPIException)
        # å…ˆè½¬æ¢ä¸º datetime64[ns]ï¼Œå¤„ç†æ— æ•ˆå€¼ä¸º NaT
        df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'], errors='coerce')
        # å¡«å…… NaT ä¸ºä»Šå¤©
        df['æ—¥æœŸ'] = df['æ—¥æœŸ'].fillna(pd.Timestamp(date.today()))
        # æœ€åè½¬æ¢ä¸º python date å¯¹è±¡
        df['æ—¥æœŸ'] = df['æ—¥æœŸ'].dt.date

        # 4. å­—ç¬¦ä¸²åˆ—å¤„ç† NaNs
        df['ç±»å‹'] = df['ç±»å‹'].astype(str).replace('nan', 'æ”¯å‡º')
        df['åˆ†ç±»'] = df['åˆ†ç±»'].astype(str).replace('nan', 'å…¶ä»–')
        df['å¤‡æ³¨'] = df['å¤‡æ³¨'].astype(str).replace('nan', '')

        return df

    def _load_from_local(self):
        if os.path.exists(self.filename):
            try:
                return pd.read_csv(self.filename), None
            except:
                pass
        return self._create_empty_df(), None

    def _save_to_local(self, df):
        df.to_csv(self.filename, index=False)
        return True

    # ä½¿ç”¨ st.cache_data å‡å°‘ GitHub API è°ƒç”¨
    @st.cache_data(ttl=300, show_spinner=False)
    def _fetch_github_content(_self):
        headers = {
            "Authorization": f"token {_self.github_token}",
            "Accept": "application/vnd.github.v3+json"
        }
        url = f"{GITHUB_API_URL}/repos/{_self.repo}/contents/{_self.filename}"
        try:
            response = requests.get(url, headers=headers, timeout=30)
            if response.status_code == 200:
                return response.json(), None
            elif response.status_code == 404:
                return None, 404
            else:
                return None, response.status_code
        except Exception as e:
            return None, str(e)

    def _load_from_github(self):
        content, error = self._fetch_github_content()
        if content:
            try:
                csv_str = base64.b64decode(content['content']).decode('utf-8')
                df = pd.read_csv(StringIO(csv_str))
                return df, content['sha']
            except:
                return self._create_empty_df(), content['sha']
        return self._create_empty_df(), None

    def _save_to_github(self, df, sha):
        """
        æ ¸å¿ƒä¿å­˜é€»è¾‘ï¼ŒåŒ…å« 409/422 å†²çªè‡ªåŠ¨ä¿®å¤
        """
        headers = {
            "Authorization": f"token {self.github_token}",
            "Accept": "application/vnd.github.v3+json"
        }
        url = f"{GITHUB_API_URL}/repos/{self.repo}/contents/{self.filename}"
        
        csv_str = df.to_csv(index=False)
        content_bytes = base64.b64encode(csv_str.encode('utf-8')).decode('utf-8')
        
        data = {
            "message": f"Update ledger {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "content": content_bytes
        }
        if sha:
            data["sha"] = sha

        def do_put(payload):
            return requests.put(url, headers=headers, data=json.dumps(payload), timeout=30)

        try:
            # ç¬¬ä¸€æ¬¡å°è¯•
            resp = do_put(data)
            
            # å¦‚æœæˆåŠŸ
            if resp.status_code in [200, 201]:
                self._fetch_github_content.clear() # æ¸…é™¤è¯»ç¼“å­˜
                return True, resp.json()['content']['sha']
            
            # å¦‚æœå¤±è´¥æ˜¯å› ä¸º SHA ä¸åŒ¹é… (409 Conflict æˆ– 422 Unprocessable Entity)
            elif resp.status_code in [409, 422]:
                if st.session_state.get('debug_mode'):
                    st.warning(f"âš ï¸ GitHub SHA å†²çª ({resp.status_code})ï¼Œæ­£åœ¨å°è¯•è‡ªåŠ¨ä¿®å¤...")
                
                # 1. å¼ºåˆ¶è·å–æœ€æ–° SHA
                self._fetch_github_content.clear()
                latest_content, _ = self._fetch_github_content()
                
                if latest_content and 'sha' in latest_content:
                    # 2. æ›´æ–° payload ä¸­çš„ sha
                    data["sha"] = latest_content['sha']
                    # 3. é‡è¯•ä¿å­˜
                    retry_resp = do_put(data)
                    if retry_resp.status_code in [200, 201]:
                        self._fetch_github_content.clear()
                        if st.session_state.get('debug_mode'):
                            st.success("âœ… è‡ªåŠ¨ä¿®å¤æˆåŠŸï¼")
                        return True, retry_resp.json()['content']['sha']
                    else:
                        st.error(f"âŒ è‡ªåŠ¨ä¿®å¤å¤±è´¥: {retry_resp.status_code} - {retry_resp.text}")
                        return False, None
                else:
                    st.error("âŒ æ— æ³•è·å–æœ€æ–° SHAï¼Œä¿å­˜å¤±è´¥ã€‚")
                    return False, None
            else:
                st.error(f"GitHub ä¿å­˜å¤±è´¥: {resp.status_code} - {resp.text}")
                return False, None

        except Exception as e:
            st.error(f"ç½‘ç»œå¼‚å¸¸: {e}")
            return False, None

    @staticmethod
    def _create_empty_df():
        return pd.DataFrame(columns=["æ—¥æœŸ", "ç±»å‹", "é‡‘é¢", "å¤‡æ³¨", "åˆ†ç±»"])

# --- AI è§£æå™¨ ---
class BillParser:
    @staticmethod
    def identify_and_parse(filename, file_bytes, api_key):
        """å¤„ç†æ–‡ä»¶å†…å®¹ï¼Œæ”¯æŒå¤§æ–‡ä»¶æ™ºèƒ½åˆ†ç‰‡å¹¶å‘å¤„ç†"""
        t_start = time.time()
        debug_log = {"file": filename, "steps": [], "chunks": []}
        
        try:
            # 1. è¯»å–å†…å®¹
            t0 = time.time()
            content_text = ""
            source_type = "æœªçŸ¥"
            
            file_stream = BytesIO(file_bytes)
            
            if filename.endswith('.csv'):
                source_type = "CSV"
                try:
                    content_text = file_bytes.decode('utf-8')
                except:
                    try:
                        content_text = file_bytes.decode('gbk')
                    except:
                        content_text = file_bytes.decode('latin-1', errors='ignore')
            elif filename.endswith(('.xls', '.xlsx')):
                source_type = "Excel"
                xls = pd.read_excel(file_stream, sheet_name=None)
                parts = []
                for sname, sdf in xls.items():
                    parts.append(f"Sheet: {sname}\n{sdf.to_csv(index=False)}")
                content_text = "\n".join(parts)
            elif filename.endswith('.pdf'):
                source_type = "PDF"
                with fitz.open(stream=file_bytes, filetype="pdf") as doc:
                    content_text = "\n".join([p.get_text() for p in doc])
            
            debug_log["steps"].append(f"è¯»å–è€—æ—¶: {time.time()-t0:.4f}s")
            debug_log["text_len"] = len(content_text)
            
            if not content_text.strip():
                return None, "å†…å®¹ä¸ºç©º", debug_log

            # 2. æ™ºèƒ½åˆ†ç‰‡ä¸å¹¶å‘å¤„ç†
            t1 = time.time()
            all_results = []
            
            # åˆ†ç‰‡é˜ˆå€¼ï¼š10k å­—ç¬¦
            CHUNK_SIZE = 8000  # æ¯ç‰‡çº¦ 8k å­—ç¬¦ï¼Œç•™æœ‰ä½™é‡
            if len(content_text) > 10000:
                debug_log["steps"].append(f"æ£€æµ‹åˆ°é•¿æ–‡æœ¬ ({len(content_text)} å­—ç¬¦)ï¼Œå¯åŠ¨æ™ºèƒ½åˆ†ç‰‡...")
                
                # æŒ‰è¡Œåˆ†å‰²ï¼Œä¿æŒå®Œæ•´æ€§
                lines = content_text.split('\n')
                chunks = []
                current_chunk = []
                current_len = 0
                
                for line in lines:
                    line_len = len(line) + 1  # +1 for \n
                    if current_len + line_len > CHUNK_SIZE and current_chunk:
                        # å½“å‰å—å·²æ»¡ï¼Œä¿å­˜å¹¶å¼€å§‹æ–°å—
                        chunks.append('\n'.join(current_chunk))
                        current_chunk = [line]
                        current_len = line_len
                    else:
                        current_chunk.append(line)
                        current_len += line_len
                
                if current_chunk:
                    chunks.append('\n'.join(current_chunk))
                
                debug_log["chunk_count"] = len(chunks)
                debug_log["steps"].append(f"åˆ†ç‰‡å®Œæˆ: {len(chunks)} ä¸ªç‰‡æ®µ")
                
                # å¹¶å‘å¤„ç†æ¯ä¸ªåˆ†ç‰‡
                with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:  # max_workers=3 é¿å… API é™æµ
                    future_to_chunk = {}
                    for idx, chunk in enumerate(chunks):
                        # ä¸ºæ¯ä¸ªåˆ†ç‰‡æ·»åŠ ä¸Šä¸‹æ–‡æç¤º
                        chunk_prompt = f"""
                        ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„è´¢åŠ¡æ•°æ®æå–ä¸“å®¶ã€‚
                        ä»»åŠ¡ï¼šä»ä»¥ä¸‹æ–‡æœ¬ç‰‡æ®µä¸­æå–äº¤æ˜“è®°å½•ï¼ˆè¿™æ˜¯ç¬¬ {idx+1}/{len(chunks)} ä¸ªç‰‡æ®µï¼‰ã€‚
                        åŸåˆ™ï¼šå®ç¼ºæ¯‹å‡ï¼Œç¦æ­¢æé€ ã€‚
                        
                        è¾“å…¥æ–‡æœ¬ç±»å‹ï¼š{source_type}
                        å½“å‰å¹´ä»½å‚è€ƒï¼š{datetime.datetime.now().year}
                        
                        **å¼ºåˆ¶è¦æ±‚**ï¼šå¿…é¡»è¿”å›**çº¯JSONæ•°ç»„**ï¼Œä¸è¦ä»»ä½•è§£é‡Šã€‚
                        æ ¼å¼ï¼š[{{"date":"YYYY-MM-DD","type":"æ”¯å‡º/æ”¶å…¥","amount":æ•°å­—,"merchant":"å•†æˆ·/å¤‡æ³¨","category":"åˆ†ç±»"}}]
                        å¦‚æœæœ¬ç‰‡æ®µæ— æ•°æ®ï¼Œè¿”å›ï¼š[]
                        
                        æ–‡æœ¬å†…å®¹ï¼š
                        {chunk}
                        """
                        
                        future = executor.submit(
                            BillParser._process_chunk,
                            chunk_prompt,
                            api_key,
                            idx
                        )
                        future_to_chunk[future] = idx
                    
                    # æ”¶é›†ç»“æœ
                    for future in concurrent.futures.as_completed(future_to_chunk):
                        chunk_idx = future_to_chunk[future]
                        try:
                            chunk_result, chunk_debug = future.result(timeout=60)
                            debug_log["chunks"].append(chunk_debug)
                            
                            if chunk_result is not None and not chunk_result.empty:
                                all_results.append(chunk_result)
                        except Exception as e:
                            chunk_debug = {
                                "index": chunk_idx,
                                "status": "error",
                                "error": str(e)
                            }
                            debug_log["chunks"].append(chunk_debug)
                            st.warning(f"ç¬¬ {chunk_idx+1} ä¸ªåˆ†ç‰‡å¤„ç†å¤±è´¥: {e}")
                
            else:
                # çŸ­æ–‡æœ¬ï¼Œç›´æ¥å¤„ç†
                prompt = f"""
                ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„è´¢åŠ¡æ•°æ®æå–ä¸“å®¶ã€‚
                ä»»åŠ¡ï¼šä»æ–‡æœ¬ä¸­æå–äº¤æ˜“è®°å½•ã€‚
                åŸåˆ™ï¼šå®ç¼ºæ¯‹å‡ï¼Œç¦æ­¢æé€ ã€‚
                
                è¾“å…¥æ–‡æœ¬ç±»å‹ï¼š{source_type}
                å½“å‰å¹´ä»½å‚è€ƒï¼š{datetime.datetime.now().year}
                
                **å¼ºåˆ¶è¦æ±‚**ï¼šå¿…é¡»è¿”å›**çº¯JSONæ•°ç»„**ï¼Œä¸è¦ä»»ä½•è§£é‡Šã€‚
                æ ¼å¼ï¼š[{{"date":"YYYY-MM-DD","type":"æ”¯å‡º/æ”¶å…¥","amount":æ•°å­—,"merchant":"å•†æˆ·/å¤‡æ³¨","category":"åˆ†ç±»"}}]
                å¦‚æœæ— æ•°æ®ï¼Œè¿”å›ï¼š[]
                
                æ–‡æœ¬å†…å®¹ï¼š
                {content_text}
                """
                
                result, chunk_debug = BillParser._process_chunk(prompt, api_key, 0)
                debug_log["chunks"].append(chunk_debug)
                if result is not None:
                    all_results.append(result)
            
            debug_log["steps"].append(f"AIå¤„ç†æ€»è€—æ—¶: {time.time()-t1:.4f}s")
            
            # 3. åˆå¹¶æ‰€æœ‰åˆ†ç‰‡ç»“æœ
            if not all_results:
                return None, "æ‰€æœ‰åˆ†ç‰‡å‡æœªæå–åˆ°æœ‰æ•ˆæ•°æ®", debug_log
            
            # åˆå¹¶æ‰€æœ‰ DataFrame
            combined_df = pd.concat(all_results, ignore_index=True)
            
        else:
            # çŸ­æ–‡æœ¬è·¯å¾„
            combined_df, err, debug_log = BillParser._process_short_text(content_text, source_type, api_key, debug_log)
            if err:
                return None, err, debug_log

        # 4. å»é‡å’Œæ¸…æ´—
        combined_df['é‡‘é¢'] = pd.to_numeric(combined_df['é‡‘é¢'], errors='coerce').fillna(0)
        combined_df['æ—¥æœŸ'] = combined_df['æ—¥æœŸ'].astype(str).apply(lambda x: x.split(' ')[0])
        
        debug_log["total_records"] = len(combined_df)
        debug_log["total_time"] = time.time() - t_start
        
        return combined_df, None, debug_log

    except Exception as e:
        return None, str(e), debug_log

    @staticmethod
    def _process_short_text(content_text, source_type, api_key, debug_log):
        """å¤„ç†çŸ­æ–‡æœ¬ï¼ˆå…¼å®¹æ—§é€»è¾‘ï¼‰"""
        t1 = time.time()
        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„è´¢åŠ¡æ•°æ®æå–ä¸“å®¶ã€‚
        ä»»åŠ¡ï¼šä»æ–‡æœ¬ä¸­æå–äº¤æ˜“è®°å½•ã€‚
        åŸåˆ™ï¼šå®ç¼ºæ¯‹å‡ï¼Œç¦æ­¢æé€ ã€‚
        
        è¾“å…¥æ–‡æœ¬ç±»å‹ï¼š{source_type}
        å½“å‰å¹´ä»½å‚è€ƒï¼š{datetime.datetime.now().year}
        
        **å¼ºåˆ¶è¦æ±‚**ï¼šå¿…é¡»è¿”å›**çº¯JSONæ•°ç»„**ï¼Œä¸è¦ä»»ä½•è§£é‡Šã€‚
        æ ¼å¼ï¼š[{{"date":"YYYY-MM-DD","type":"æ”¯å‡º/æ”¶å…¥","amount":æ•°å­—,"merchant":"å•†æˆ·/å¤‡æ³¨","category":"åˆ†ç±»"}}]
        å¦‚æœæ— æ•°æ®ï¼Œè¿”å›ï¼š[]
        
        æ–‡æœ¬å†…å®¹ï¼š
        {content_text}
        """
        
        client = get_llm_client(api_key)
        resp = client.chat.completions.create(
            model=TEXT_MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=4096,
            temperature=0.0
        )
        debug_log["steps"].append(f"AIå“åº”è€—æ—¶: {time.time()-t1:.4f}s")
        
        raw_json = resp.choices[0].message.content
        debug_log["ai_response_full"] = raw_json
        
        data, parse_err = extract_json_from_text(raw_json, auto_repair=True)
        
        if parse_err: 
            return None, f"JSONæå–å¤±è´¥: {parse_err}", debug_log
            
        if not data: 
            return None, "æœªæå–åˆ°æœ‰æ•ˆæ•°æ®", debug_log
            
        df = pd.DataFrame(data)
        # æ ‡å‡†åŒ–
        cols = {"date": "æ—¥æœŸ", "type": "ç±»å‹", "amount": "é‡‘é¢", "merchant": "å¤‡æ³¨", "category": "åˆ†ç±»"}
        df = df.rename(columns=cols)
        for c in cols.values():
            if c not in df.columns: df[c] = ""
        
        # æ¸…æ´—
        df['é‡‘é¢'] = pd.to_numeric(df['é‡‘é¢'], errors='coerce').fillna(0)
        df['æ—¥æœŸ'] = df['æ—¥æœŸ'].astype(str).apply(lambda x: x.split(' ')[0])
        
        debug_log["total_time"] = time.time() - time.time()
        return df, None, debug_log

    @staticmethod
    def _process_chunk(prompt, api_key, chunk_idx):
        """å¤„ç†å•ä¸ªåˆ†ç‰‡ï¼ˆç‹¬ç«‹å‡½æ•°ä¾¿äºå¹¶å‘ï¼‰"""
        chunk_debug = {"index": chunk_idx, "status": "processing"}
        
        client = get_llm_client(api_key)
        resp = client.chat.completions.create(
            model=TEXT_MODEL_NAME,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=4096,
            temperature=0.0,
            timeout=60  # å¢åŠ è¶…æ—¶æ—¶é—´
        )
        
        raw_json = resp.choices[0].message.content
        chunk_debug["ai_response_full"] = raw_json[:500]  # ä¿ç•™å‰500å­—ç¬¦ç”¨äºè°ƒè¯•
        
        # æå–å’Œä¿®å¤ JSON
        data, parse_err = extract_json_from_text(raw_json, auto_repair=True)
        
        if parse_err:
            chunk_debug["status"] = "parse_error"
            chunk_debug["error"] = parse_err
            return None, chunk_debug
        
        if not data:
            chunk_debug["status"] = "no_data"
            return None, chunk_debug
        
        df = pd.DataFrame(data)
        cols = {"date": "æ—¥æœŸ", "type": "ç±»å‹", "amount": "é‡‘é¢", "merchant": "å¤‡æ³¨", "category": "åˆ†ç±»"}
        df = df.rename(columns=cols)
        for c in cols.values():
            if c not in df.columns:
                df[c] = ""
        
        chunk_debug["status"] = "success"
        return df, chunk_debug

    @staticmethod
    def process_image(filename, image_bytes, api_key):
        """å¤„ç†å›¾ç‰‡ (çº¯å‡½æ•°)"""
        t_start = time.time()
        debug_log = {"file": filename, "steps": []}
        
        try:
            b64_img = base64.b64encode(image_bytes).decode('utf-8')
            client = get_llm_client(api_key)
            
            t1 = time.time()
            resp = client.chat.completions.create(
                model=VISION_MODEL_NAME,
                messages=[{
                    "role": "user", 
                    "content": [
                        {"type": "text", "text": "æå–è´¦å•æ˜ç»†ã€‚å¿…é¡»è¿”å›çº¯JSONæ•°ç»„ï¼š[{date, type, amount, merchant, category}]ï¼Œæ— æ•°æ®è¿”å›[]"},
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64_img}"}}
                    ]
                }],
                max_tokens=2048
            )
            debug_log["steps"].append(f"è§†è§‰æ¨¡å‹è€—æ—¶: {time.time()-t1:.4f}s")
            
            raw_json = resp.choices[0].message.content
            debug_log["ai_response_full"] = raw_json
            
            data, parse_err = extract_json_from_text(raw_json, auto_repair=True)
            
            if parse_err:
                 return None, f"è¯†åˆ«ç»“æœè§£æå¤±è´¥: {parse_err}", debug_log

            if isinstance(data, dict): data = [data]
            
            if not data: return None, "æœªè¯†åˆ«åˆ°æ•°æ®", debug_log
            
            df = pd.DataFrame(data)
            # ç®€å•æ˜ å°„
            cols = {"date": "æ—¥æœŸ", "type": "ç±»å‹", "amount": "é‡‘é¢", "merchant": "å¤‡æ³¨", "category": "åˆ†ç±»"}
            df = df.rename(columns=cols)
            for c in cols.values(): 
                if c not in df.columns: df[c] = ""
            
            return df, None, debug_log
            
        except Exception as e:
            return None, str(e), debug_log

    @staticmethod
    def merge_data(old_df, new_df):
        """åˆå¹¶å»é‡"""
        if new_df is None or new_df.empty: return old_df, 0
        
        # ç®€å•æŒ‡çº¹
        def get_fp(d): 
            return d['æ—¥æœŸ'].astype(str) + d['é‡‘é¢'].astype(str) + d['å¤‡æ³¨'].str[:5]
            
        if old_df.empty: 
            return new_df, len(new_df)
            
        old_fp = set(get_fp(old_df))
        new_df['_fp'] = get_fp(new_df)
        
        to_add = new_df[~new_df['_fp'].isin(old_fp)].drop(columns=['_fp'])
        
        if to_add.empty: return old_df, 0
        
        merged = pd.concat([old_df, to_add], ignore_index=True)
        # ç¡®ä¿åˆå¹¶åæ¸…æ´—ç±»å‹ï¼Œé˜²æ­¢åç»­æŠ¥é”™
        merged = DataManager._clean_df_types(merged)
        merged = merged.sort_values('æ—¥æœŸ', ascending=False).reset_index(drop=True)
        return merged, len(to_add)

# --- ä¸»ç¨‹åº ---
def main():
    # 1. åˆå§‹åŒ–ä¸ä¾§è¾¹æ 
    if 'debug_mode' not in st.session_state: st.session_state.debug_mode = False
    
    st.sidebar.title("âš™ï¸ è®¾ç½®")
    st.session_state.debug_mode = st.sidebar.checkbox("ğŸ å¼€å¯æ·±åº¦è°ƒè¯•", value=st.session_state.debug_mode)
    
    api_key = st.secrets.get("SILICONFLOW_API_KEY") or st.sidebar.text_input("API Key", type="password")
    gh_token = st.secrets.get("GITHUB_TOKEN")
    gh_repo = st.secrets.get("GITHUB_REPO")
    
    dm = DataManager(gh_token, gh_repo)
    
    if dm.use_github:
        st.sidebar.success(f"å·²è¿æ¥: {dm.repo}")
        if st.sidebar.button("â˜ï¸ å¼ºåˆ¶åŒæ­¥äº‘ç«¯"):
            with st.spinner("æ­£åœ¨æ‹‰å–æœ€æ–°æ•°æ®..."):
                df, sha = dm.load_data(force_refresh=True)
                st.session_state.ledger_data = df
                st.session_state.github_sha = sha
                st.success("åŒæ­¥å®Œæˆï¼")
                st.rerun()
    else:
        st.sidebar.warning("æœ¬åœ°æ¨¡å¼ (æ•°æ®ä¸æŒä¹…åŒ–)")

    payday = st.sidebar.number_input("æ¯æœˆå‘è–ªæ—¥", 1, 31, 10)
    current_asset = st.sidebar.number_input("å½“å‰èµ„äº§", value=3000.0)

    # 2. æ•°æ®åŠ è½½
    if 'ledger_data' not in st.session_state:
        df, sha = dm.load_data()
        st.session_state.ledger_data = df
        st.session_state.github_sha = sha

    # 3. é¡¶éƒ¨æ¦‚è§ˆ
    st.title("ğŸ’° AI æ™ºèƒ½è´¦æœ¬ Pro")
    
    today = date.today()
    # ç®€å•çš„è´¦æœŸè®¡ç®—
    target_month = today.month if today.day < payday else (today.month % 12) + 1
    target_year = today.year + (1 if (today.month==12 and today.day >= payday) else 0)
    target_date = date(target_year, target_month, payday)
    days_left = (target_date - today).days

    df = st.session_state.ledger_data.copy()
    month_spend = 0.0
    if not df.empty:
        # ç¡®ä¿ç±»å‹å®‰å…¨
        if 'æ—¥æœŸ' not in df.columns:
             df['æ—¥æœŸ'] = []
        df['dt'] = pd.to_datetime(df['æ—¥æœŸ'], errors='coerce')
        # æœ¬æœˆæ”¯å‡º (è‡ªç„¶æœˆ)
        mask = (df['dt'].dt.month == today.month) & (df['dt'].dt.year == today.year) & (df['ç±»å‹']=='æ”¯å‡º')
        month_spend = df.loc[mask, 'é‡‘é¢'].sum()

    c1, c2, c3, c4 = st.columns(4)
    c1.metric("èµ„äº§ä½™é¢", f"Â¥{current_asset:,.2f}")
    c2.metric("æœ¬æœˆå·²æ”¯", f"Â¥{month_spend:,.2f}")
    c3.metric("è·å‘è–ªæ—¥", f"{days_left} å¤©")
    
    daily_budget = current_asset / max(1, days_left)
    c4.metric("æ¯æ—¥å¯ç”¨", f"Â¥{daily_budget:.0f}", 
              delta=f"{daily_budget - DEFAULT_TARGET_SPEND:.0f}", delta_color="normal")

    st.divider()

    # 4. ä¸»è¦åŠŸèƒ½åŒº
    t_import, t_add, t_history, t_stats = st.tabs(["ğŸ“¥ æ™ºèƒ½å¯¼å…¥", "âœï¸ æ‰‹åŠ¨è®°è´¦", "ğŸ“‹ å†å²æ˜ç»†", "ğŸ“Š ç»Ÿè®¡æŠ¥è¡¨"])

    # --- æ™ºèƒ½å¯¼å…¥ Tab ---
    with t_import:
        files = st.file_uploader("æ”¯æŒ PDF/CSV/Excel/å›¾ç‰‡", accept_multiple_files=True)
        if files and st.button("ğŸš€ æ‰¹é‡å¼€å§‹è¯†åˆ«", type="primary"):
            if not api_key:
                st.error("ç¼ºå°‘ API Key")
                st.stop()
            
            # 1. é¢„è¯»å–æ‰€æœ‰æ–‡ä»¶ (é¿å…åœ¨çº¿ç¨‹ä¸­ä¼  Streamlit å¯¹è±¡)
            tasks_doc = []
            tasks_img = []
            
            with st.status("æ­£åœ¨é¢„å¤„ç†æ–‡ä»¶...") as status:
                for f in files:
                    ext = f.name.split('.')[-1].lower()
                    f.seek(0) # å…³é”®ï¼šé‡ç½®æŒ‡é’ˆ
                    bytes_data = f.read()
                    
                    item = {"name": f.name, "bytes": bytes_data}
                    if ext in ['png', 'jpg', 'jpeg']:
                        tasks_img.append(item)
                    else:
                        tasks_doc.append(item)
                status.update(label="æ–‡ä»¶è¯»å–å®Œæˆï¼Œå‡†å¤‡æäº¤ AI", state="complete")

            # 2. å¹¶å‘å¤„ç†
            new_df = pd.DataFrame()
            debug_logs = []
            
            progress = st.progress(0)
            total_tasks = len(tasks_doc) + len(tasks_img)
            completed = 0

            with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
                futures = {}
                
                # æäº¤æ–‡æ¡£ä»»åŠ¡
                for t in tasks_doc:
                    f = executor.submit(BillParser.identify_and_parse, t['name'], t['bytes'], api_key)
                    futures[f] = t['name']
                
                # æäº¤å›¾ç‰‡ä»»åŠ¡
                for t in tasks_img:
                    f = executor.submit(BillParser.process_image, t['name'], t['bytes'], api_key)
                    futures[f] = t['name']
                
                # ç­‰å¾…ç»“æœ
                for future in concurrent.futures.as_completed(futures):
                    fname = futures[future]
                    try:
                        res, err, dbg = future.result()
                        debug_logs.append(dbg)
                        
                        if res is not None and not res.empty:
                            new_df = pd.concat([new_df, res], ignore_index=True)
                            st.toast(f"âœ… {fname} æˆåŠŸ")
                        else:
                            st.error(f"âŒ {fname}: {err}")
                            
                    except Exception as e:
                        st.error(f"âŒ {fname} å¼‚å¸¸: {e}")
                    
                    completed += 1
                    progress.progress(completed / total_tasks)

            # 3. æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
            if st.session_state.debug_mode:
                with st.expander("ğŸ”¬ æ·±åº¦è°ƒè¯•ä¿¡æ¯ (åŒ…å«åˆ†ç‰‡è¯¦æƒ…)", expanded=True):
                    # æ˜¾ç¤ºæ•´ä½“æ—¥å¿—
                    st.json(debug_logs)
                    
                    # æ˜¾ç¤ºæ¯ä¸ªåˆ†ç‰‡çš„è¯¦ç»†æƒ…å†µ
                    for dbg in debug_logs:
                        if "chunks" in dbg:
                            for chunk_info in dbg["chunks"]:
                                with st.container():
                                    col1, col2 = st.columns([1, 4])
                                    status_emoji = "âœ…" if chunk_info.get("status") == "success" else "âŒ"
                                    col1.markdown(f"**åˆ†ç‰‡ {chunk_info.get('index', 0) + 1}** {status_emoji}")
                                    
                                    if chunk_info.get("status") == "success":
                                        col2.caption(f"æå–è®°å½•: {chunk_info.get('records', 0)} æ¡")
                                        # å¯å±•å¼€æŸ¥çœ‹åŸå§‹å“åº”
                                        if "debug" in chunk_info and "ai_response_full" in chunk_info["debug"]:
                                            with col2.expander("æŸ¥çœ‹åŸå§‹å“åº”"):
                                                st.code(chunk_info["debug"]["ai_response_full"], language="json")
                                    else:
                                        col2.error(f"{chunk_info.get('error', 'æœªçŸ¥é”™è¯¯')}")

            # 4. ä¿å­˜
            if not new_df.empty:
                merged_df, added = DataManager.merge_data(st.session_state.ledger_data, new_df)
                if added > 0:
                    with st.spinner("æ­£åœ¨ä¿å­˜è‡³äº‘ç«¯..."):
                        ok, new_sha = dm.save_data(merged_df, st.session_state.get('github_sha'))
                        if ok:
                            st.session_state.ledger_data = merged_df
                            st.session_state.github_sha = new_sha
                            st.success(f"ğŸ‰ æˆåŠŸå­˜å…¥ {added} æ¡æ–°è®°å½•ï¼")
                        else:
                            st.error("ä¿å­˜å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–é…ç½®")
                else:
                    st.warning("æ‰€æœ‰è®°å½•å·²å­˜åœ¨ï¼Œæ— æ–°å¢ã€‚")

    # --- æ‰‹åŠ¨è®°è´¦ Tab ---
    with t_add:
        with st.form("manual_add"):
            c1, c2, c3 = st.columns(3)
            d = c1.date_input("æ—¥æœŸ", date.today())
            t = c2.selectbox("ç±»å‹", ["æ”¯å‡º", "æ”¶å…¥"])
            a = c3.number_input("é‡‘é¢", min_value=0.01)
            c4, c5 = st.columns([1, 2])
            cat = c4.selectbox("åˆ†ç±»", ["é¤é¥®", "äº¤é€š", "è´­ç‰©", "å±…ä½", "å¨±ä¹", "åŒ»ç–—", "å·¥èµ„", "å…¶ä»–"])
            rem = c5.text_input("å¤‡æ³¨")
            
            if st.form_submit_button("ğŸ’¾ ä¿å­˜", width="stretch"):
                row = pd.DataFrame([{"æ—¥æœŸ": str(d), "ç±»å‹": t, "é‡‘é¢": a, "åˆ†ç±»": cat, "å¤‡æ³¨": rem}])
                merged, added = DataManager.merge_data(st.session_state.ledger_data, row)
                ok, new_sha = dm.save_data(merged, st.session_state.get('github_sha'))
                if ok:
                    st.session_state.ledger_data = merged
                    st.session_state.github_sha = new_sha
                    st.success("ä¿å­˜æˆåŠŸ")
                    st.rerun()

    # --- å†å²æ˜ç»† Tab ---
    with t_history:
        st.subheader("ğŸ“‹ è´¦å•æ˜ç»† (æ”¯æŒç¼–è¾‘)")
        if st.session_state.ledger_data.empty:
            st.info("æš‚æ— æ•°æ®")
        else:
            # ç¡®ä¿åœ¨å±•ç¤ºå‰æ•°æ®æ˜¯å¹²å‡€çš„
            st.session_state.ledger_data = DataManager._clean_df_types(st.session_state.ledger_data)
            
            edited_df = st.data_editor(
                st.session_state.ledger_data,
                use_container_width=True,
                num_rows="dynamic",
                key="history_editor",
                column_config={
                    "é‡‘é¢": st.column_config.NumberColumn(format="Â¥%.2f", required=True),
                    "æ—¥æœŸ": st.column_config.DateColumn(format="YYYY-MM-DD", required=True),
                    "ç±»å‹": st.column_config.SelectboxColumn(options=["æ”¯å‡º", "æ”¶å…¥"], required=True),
                    "åˆ†ç±»": st.column_config.SelectboxColumn(options=["é¤é¥®", "äº¤é€š", "è´­ç‰©", "å±…ä½", "å¨±ä¹", "åŒ»ç–—", "å·¥èµ„", "å…¶ä»–"]),
                    "å¤‡æ³¨": st.column_config.TextColumn()
                }
            )
            
            if st.button("ğŸ’¾ ä¿å­˜è¡¨æ ¼å˜æ›´"):
                if not edited_df.equals(st.session_state.ledger_data):
                    with st.spinner("åŒæ­¥ä¸­..."):
                        ok, new_sha = dm.save_data(edited_df, st.session_state.get('github_sha'))
                        if ok:
                            st.session_state.ledger_data = edited_df
                            st.session_state.github_sha = new_sha
                            st.success("âœ… æ›´æ–°æˆåŠŸ")
                else:
                    st.info("æ•°æ®æœªå˜æ›´")

    # --- ç»Ÿè®¡æŠ¥è¡¨ Tab ---
    with t_stats:
        if st.session_state.ledger_data.empty:
            st.info("æš‚æ— æ•°æ®ï¼Œè¯·å…ˆè®°è´¦")
        else:
            df = st.session_state.ledger_data.copy()
            # ç±»å‹è½¬æ¢ç”¨äºç»˜å›¾
            df['é‡‘é¢'] = pd.to_numeric(df['é‡‘é¢'], errors='coerce').fillna(0)
            
            # ç­›é€‰
            df_exp = df[df['ç±»å‹'] == 'æ”¯å‡º']
            
            c_s1, c_s2 = st.columns(2)
            
            with c_s1:
                st.subheader("ğŸ“Š åˆ†ç±»æ”¯å‡ºå æ¯”")
                if not df_exp.empty:
                    chart_data = df_exp.groupby("åˆ†ç±»")['é‡‘é¢'].sum().reset_index()
                    st.bar_chart(chart_data, x="åˆ†ç±»", y="é‡‘é¢", color="åˆ†ç±»")
                else:
                    st.caption("æ— æ”¯å‡ºæ•°æ®")

            with c_s2:
                st.subheader("ğŸ“‰ æ¯æ—¥æ”¯å‡ºè¶‹åŠ¿")
                if not df_exp.empty:
                    daily_data = df_exp.groupby("æ—¥æœŸ")['é‡‘é¢'].sum().reset_index()
                    st.line_chart(daily_data, x="æ—¥æœŸ", y="é‡‘é¢")
                else:
                    st.caption("æ— æ”¯å‡ºæ•°æ®")

            # AI åˆ†ææ¨¡å—
            st.divider()
            st.subheader("ğŸ¤– AI è´¢åŠ¡é¡¾é—®")
            if st.button("ç”Ÿæˆæœ¬æœˆåˆ†ææŠ¥å‘Š"):
                if not api_key:
                    st.error("è¯·é…ç½® API Key")
                else:
                    with st.spinner("AI æ­£åœ¨åˆ†ææ‚¨çš„è´¢åŠ¡çŠ¶å†µ..."):
                        # ä»…å‘é€æœ€è¿‘ 100 æ¡æ”¯å‡ºæ•°æ®ï¼Œé¿å…è¶…é•¿
                        summary_csv = df_exp.sort_values('æ—¥æœŸ', ascending=False).head(100).to_csv(index=False)
                        client = get_llm_client(api_key)
                        try:
                            res = client.chat.completions.create(
                                model=TEXT_MODEL_NAME,
                                messages=[
                                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªçŠ€åˆ©çš„ç†è´¢å¸ˆã€‚è¯·æ ¹æ®ç”¨æˆ·æœ€è¿‘çš„æ”¯å‡ºï¼ˆCSVæ ¼å¼ï¼‰ï¼Œç»™å‡ºï¼š1. æ¶ˆè´¹ç»“æ„è¯„ä»· 2. å¼‚å¸¸å¤§é¢æ”¯å‡ºé¢„è­¦ 3. å…·ä½“çš„çœé’±å»ºè®®ã€‚é£æ ¼å¹½é»˜çŠ€åˆ©ã€‚"},
                                    {"role": "user", "content": summary_csv}
                                ],
                                max_tokens=2000
                            )
                            st.markdown(res.choices[0].message.content)
                        except Exception as e:
                            st.error(f"AI åˆ†æå¤±è´¥: {e}")

if __name__ == "__main__":
    main()
