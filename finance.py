import streamlit as st
import pandas as pd
import datetime
from datetime import date
import requests
import json
import base64
from io import StringIO, BytesIO
import os
import fitz  # PyMuPDF
import re
from openai import OpenAI, APITimeoutError
import concurrent.futures
import time

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="AI æ™ºèƒ½è´¦æœ¬ Pro", page_icon="ğŸ’°", layout="wide")

# --- å¸¸é‡é…ç½® ---
DEFAULT_TARGET_SPEND = 60.0  # æ¯æ—¥ä½“é¢æ”¯å‡ºæ ‡å‡†
GITHUB_API_URL = "https://api.github.com"
VISION_MODEL_NAME = "Qwen/Qwen3-VL-8B-Instruct" 
TEXT_MODEL_NAME = "deepseek-ai/DeepSeek-V3.2"

# --- æ ¸å¿ƒå·¥å…·ï¼šOpenAI Client (æ— ç¼“å­˜ï¼Œé˜²æ­¢çº¿ç¨‹å®‰å…¨é—®é¢˜) ---
def get_llm_client(api_key):
    return OpenAI(api_key=api_key, base_url="https://api.siliconflow.cn/v1")

# --- å·¥å…·å‡½æ•°ï¼šå¢å¼ºç‰ˆ JSON æå–ä¸ä¿®å¤ ---
def extract_json_from_text(text):
    """å¢å¼ºç‰ˆJSONæå–ï¼Œæ”¯æŒæˆªæ–­ä¿®å¤ã€æ³¨é‡Šæ¸…æ´—"""
    if not text: 
        return None, "ç©ºå“åº”"
    
    # ä¿å­˜åŸå§‹æ–‡æœ¬ç”¨äºè°ƒè¯• (å–å‰200å­—ç¬¦é¢„è§ˆ)
    original_preview = text[:200].replace('\n', '\\n')
    
    try:
        text = text.strip()
        
        # 1. ç§»é™¤ Markdown æ ‡è®°
        match_code = re.search(r"``" + r"`(?:json)?(.*?)``" + r"`", text, re.DOTALL)
        if match_code:
            text = match_code.group(1).strip()
        else:
            text = re.sub(r'```json\s*', '', text)
            text = re.sub(r'```\s*', '', text)
            text = text.strip()
        
        # 2. å¿«é€Ÿåˆ¤æ–­ç©ºæ•°ç»„
        if text == '[]':
            return [], None

        # 3. å°è¯•å®šä½æ•°ç»„è¾¹ç•Œ
        # å¯»æ‰¾ç¬¬ä¸€ä¸ª [ å’Œ æœ€åä¸€ä¸ª ]
        start_idx = text.find('[')
        end_idx = text.rfind(']')
        
        if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
            text_to_parse = text[start_idx:end_idx+1]
        elif start_idx != -1:
            # åªæœ‰å¼€å¤´æ²¡æœ‰ç»“å°¾ï¼Œå¯èƒ½æ˜¯è¢«æˆªæ–­äº†
            text_to_parse = text[start_idx:]
        else:
            text_to_parse = text

        # 4. å°è¯•ç›´æ¥è§£æ
        try:
            result = json.loads(text_to_parse)
            if isinstance(result, (list, dict)):
                return result if isinstance(result, list) else [result], None
        except json.JSONDecodeError:
            # 5. è§£æå¤±è´¥ï¼Œå°è¯•ä¿®å¤æˆªæ–­é—®é¢˜
            # å¸¸è§æƒ…å†µï¼šç»“å°¾å°‘äº† ] æˆ– }
            try:
                # å°è¯•è¡¥å…¨ç»“å°¾
                fixed_text = text_to_parse.strip()
                if not fixed_text.endswith(']'):
                    if fixed_text.endswith('}'):
                        fixed_text += ']'
                    elif fixed_text.endswith(','):
                        fixed_text = fixed_text[:-1] + '}]' # å‡è®¾æ–­åœ¨å¯¹è±¡é—´
                    else:
                        # æš´åŠ›å°è¯•ï¼šæ‰¾åˆ°æœ€åä¸€ä¸ª }ï¼Œæˆªæ–­åè¡¥ ]
                        last_brace = fixed_text.rfind('}')
                        if last_brace != -1:
                            fixed_text = fixed_text[:last_brace+1] + ']'
                
                result = json.loads(fixed_text)
                return result if isinstance(result, list) else [result], None
            except:
                pass

            # 6. å°è¯•ç§»é™¤æ³¨é‡Š (//...)
            try:
                text_no_comments = re.sub(r'//.*?\n', '\n', text_to_parse)
                text_no_comments = re.sub(r'/\*.*?\*/', '', text_no_comments, flags=re.DOTALL)
                result = json.loads(text_no_comments)
                return result if isinstance(result, list) else [result], None
            except:
                pass
            
            # å¦‚æœè¿˜æ˜¯å¤±è´¥ï¼ŒæŠ›å‡ºåŸå§‹å¼‚å¸¸ä»¥ä¾¿æŸ¥çœ‹
            return None, f"æ— æ³•ä¿®å¤çš„JSONæ ¼å¼ã€‚å°è¯•è§£æç‰‡æ®µ: {text_to_parse[:100]}..."
            
    except Exception as e:
        return None, f"è§£æå¼‚å¸¸: {str(e)}"

# --- è¾…åŠ©å‡½æ•°ï¼šå¤§æ–‡æœ¬åˆ‡ç‰‡ ---
def split_text_into_chunks(text, max_chars=12000):
    """å°†é•¿æ–‡æœ¬æŒ‰è¡Œåˆ‡åˆ†ä¸ºå¤šä¸ªç‰‡æ®µï¼Œé¿å… LLM ä¸Šä¸‹æ–‡æº¢å‡ºæˆ–è¾“å‡ºæˆªæ–­"""
    lines = text.split('\n')
    chunks = []
    current_chunk = []
    current_len = 0
    
    # ä¿ç•™è¡¨å¤´ï¼ˆå‡è®¾å‰5è¡Œæ˜¯è¡¨å¤´ï¼‰
    header = "\n".join(lines[:5]) if len(lines) > 5 else ""
    
    for line in lines:
        if current_len + len(line) > max_chars:
            chunk_content = "\n".join(current_chunk)
            # å¦‚æœä¸æ˜¯ç¬¬ä¸€å—ï¼ŒåŠ ä¸Šè¡¨å¤´ä¸Šä¸‹æ–‡
            if len(chunks) > 0:
                chunk_content = header + "\n...[æ¥ä¸Šæ–‡]...\n" + chunk_content
            chunks.append(chunk_content)
            current_chunk = []
            current_len = 0
        current_chunk.append(line)
        current_len += len(line) + 1
        
    if current_chunk:
        chunk_content = "\n".join(current_chunk)
        if len(chunks) > 0:
            chunk_content = header + "\n...[æ¥ä¸Šæ–‡]...\n" + chunk_content
        chunks.append(chunk_content)
        
    return chunks

# --- æ•°æ®ç®¡ç†ç±» ---
class DataManager:
    def __init__(self, github_token=None, repo=None, filename="ledger.csv"):
        self.github_token = github_token
        if repo and repo.startswith("http"):
            self.repo = repo.rstrip("/").split("github.com/")[-1]
        else:
            self.repo = repo
        self.filename = filename
        self.use_github = bool(github_token and self.repo)

    def load_data(self, force_refresh=False):
        if self.use_github:
            if force_refresh:
                self._fetch_github_content.clear()
            df, sha = self._load_from_github()
        else:
            df, sha = self._load_from_local()
        return self._clean_df_types(df), sha

    def save_data(self, df, sha=None):
        save_df = df.copy()
        if 'æ—¥æœŸ' in save_df.columns:
            save_df['æ—¥æœŸ'] = save_df['æ—¥æœŸ'].astype(str)
            
        if self.use_github:
            return self._save_to_github(save_df, sha)
        else:
            return self._save_to_local(save_df), None

    @staticmethod
    def _clean_df_types(df):
        cols = ["æ—¥æœŸ", "ç±»å‹", "é‡‘é¢", "å¤‡æ³¨", "åˆ†ç±»"]
        for c in cols:
            if c not in df.columns: df[c] = ""
        
        df['é‡‘é¢'] = pd.to_numeric(df['é‡‘é¢'], errors='coerce').fillna(0.0)
        df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'], errors='coerce').dt.date
        df['æ—¥æœŸ'] = df['æ—¥æœŸ'].fillna(date.today())
        
        for c in ['ç±»å‹', 'åˆ†ç±»', 'å¤‡æ³¨']:
            df[c] = df[c].astype(str).replace('nan', '')
            
        return df

    def _load_from_local(self):
        if os.path.exists(self.filename):
            try:
                return pd.read_csv(self.filename), None
            except: pass
        return self._create_empty_df(), None

    def _save_to_local(self, df):
        df.to_csv(self.filename, index=False)
        return True

    @st.cache_data(ttl=300, show_spinner=False)
    def _fetch_github_content(_self):
        headers = {"Authorization": f"token {_self.github_token}", "Accept": "application/vnd.github.v3+json"}
        url = f"{GITHUB_API_URL}/repos/{_self.repo}/contents/{_self.filename}"
        try:
            r = requests.get(url, headers=headers, timeout=30)
            if r.status_code == 200: return r.json(), None
            elif r.status_code == 404: return None, 404
            else: return None, r.status_code
        except Exception as e: return None, str(e)

    def _load_from_github(self):
        c, e = self._fetch_github_content()
        if c:
            try:
                csv = base64.b64decode(c['content']).decode('utf-8')
                return pd.read_csv(StringIO(csv)), c['sha']
            except: pass
        return self._create_empty_df(), c.get('sha') if c else None

    def _save_to_github(self, df, sha):
        headers = {"Authorization": f"token {self.github_token}", "Accept": "application/vnd.github.v3+json"}
        url = f"{GITHUB_API_URL}/repos/{self.repo}/contents/{self.filename}"
        csv_str = df.to_csv(index=False)
        data = {
            "message": f"Update {datetime.datetime.now()}",
            "content": base64.b64encode(csv_str.encode('utf-8')).decode('utf-8')
        }
        if sha: data["sha"] = sha

        def put(d): return requests.put(url, headers=headers, data=json.dumps(d), timeout=30)

        try:
            r = put(data)
            if r.status_code in [200, 201]:
                self._fetch_github_content.clear()
                return True, r.json()['content']['sha']
            elif r.status_code in [409, 422]: # SHA å†²çªä¿®å¤
                self._fetch_github_content.clear()
                latest, _ = self._fetch_github_content()
                if latest:
                    data["sha"] = latest['sha']
                    r2 = put(data)
                    if r2.status_code in [200, 201]:
                        self._fetch_github_content.clear()
                        return True, r2.json()['content']['sha']
            return False, None
        except: return False, None

    @staticmethod
    def _create_empty_df():
        return pd.DataFrame(columns=["æ—¥æœŸ", "ç±»å‹", "é‡‘é¢", "å¤‡æ³¨", "åˆ†ç±»"])

# --- AI è§£æå™¨ ---
class BillParser:
    @staticmethod
    def identify_and_parse(filename, file_bytes, api_key):
        """ä¸»å…¥å£ï¼šå¤„ç†å•ä¸ªæ–‡ä»¶ï¼Œæ”¯æŒå¤§æ–‡ä»¶åˆ‡ç‰‡"""
        t_start = time.time()
        debug_log = {"file": filename, "steps": [], "chunks": 0}
        
        try:
            # 1. æå–çº¯æ–‡æœ¬
            t0 = time.time()
            text = ""
            file_stream = BytesIO(file_bytes)
            
            if filename.endswith('.csv'):
                try: text = file_bytes.decode('utf-8')
                except: text = file_bytes.decode('gbk', 'ignore')
            elif filename.endswith(('.xls', '.xlsx')):
                xls = pd.read_excel(file_stream, sheet_name=None)
                text = "\n".join([df.to_csv(index=False) for df in xls.values()])
            elif filename.endswith('.pdf'):
                with fitz.open(stream=file_bytes, filetype="pdf") as doc:
                    text = "\n".join([p.get_text() for p in doc])
            
            if not text.strip(): return None, "ç©ºæ–‡ä»¶", debug_log
            debug_log["steps"].append(f"è¯»å–è€—æ—¶: {time.time()-t0:.4f}s")
            
            # 2. æ™ºèƒ½åˆ‡ç‰‡ (å¤„ç†è¶…é•¿è´¦å•çš„å…³é”®)
            # å¦‚æœæ–‡æœ¬ > 15000 å­—ç¬¦ï¼Œå¤§æ¦‚ç‡è¶…è¿‡ 4k output tokenï¼Œéœ€è¦åˆ‡ç‰‡
            chunks = split_text_into_chunks(text, max_chars=15000)
            debug_log["chunks"] = len(chunks)
            
            # 3. å¹¶å‘å¤„ç†æ‰€æœ‰åˆ‡ç‰‡
            all_df = pd.DataFrame()
            
            # ä½¿ç”¨å†…éƒ¨å‡½æ•°å¤„ç†å•ä¸ªåˆ‡ç‰‡
            def process_chunk(chunk_idx, chunk_text):
                t_c = time.time()
                prompt = f"""
                ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„è´¢åŠ¡ä¸“å®¶ã€‚è¯·ä»æ–‡æœ¬ä¸­æå–äº¤æ˜“è®°å½•ã€‚
                å½“å‰æ˜¯ç¬¬ {chunk_idx+1} éƒ¨åˆ†æ–‡æœ¬ã€‚
                åŸåˆ™ï¼šå®ç¼ºæ¯‹å‡ï¼Œç¦æ­¢æé€ ã€‚åªæå–æœ‰æ•ˆäº¤æ˜“è¡Œã€‚
                
                å½“å‰å¹´ä»½å‚è€ƒï¼š{datetime.datetime.now().year}
                
                **å¼ºåˆ¶è¦æ±‚**ï¼šä»…è¿”å›çº¯JSONæ•°ç»„ã€‚
                æ ¼å¼ï¼š[{{"date":"YYYY-MM-DD","type":"æ”¯å‡º/æ”¶å…¥","amount":æ•°å­—,"merchant":"å•†æˆ·/å¤‡æ³¨","category":"åˆ†ç±»"}}]
                
                æ–‡æœ¬å†…å®¹ï¼š
                {chunk_text}
                """
                
                try:
                    client = get_llm_client(api_key)
                    resp = client.chat.completions.create(
                        model=TEXT_MODEL_NAME,
                        messages=[{"role": "user", "content": prompt}],
                        max_tokens=8192, # å°½å¯èƒ½å¤§
                        temperature=0.0
                    )
                    raw = resp.choices[0].message.content
                    data, err = extract_json_from_text(raw)
                    return data, err, raw, time.time()-t_c
                except Exception as e:
                    return None, str(e), "", 0

            # æ‰§è¡Œå¹¶å‘
            with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
                futures = {executor.submit(process_chunk, i, c): i for i, c in enumerate(chunks)}
                
                for future in concurrent.futures.as_completed(futures):
                    i = futures[future]
                    data, err, raw, cost = future.result()
                    
                    # è®°å½•é¦–ä¸ªåˆ‡ç‰‡çš„è°ƒè¯•ä¿¡æ¯ï¼Œé¿å…æ—¥å¿—çˆ†ç‚¸
                    if i == 0:
                        debug_log["ai_response_sample"] = raw[:500] + "..."
                        if err: debug_log["first_chunk_error"] = err
                    
                    if data:
                        all_df = pd.concat([all_df, pd.DataFrame(data)], ignore_index=True)
            
            if all_df.empty:
                return None, "æœªæå–åˆ°ä»»ä½•æ•°æ® (å¯èƒ½æ ¼å¼ä¸æ”¯æŒæˆ–Tokenè¶…é™)", debug_log
                
            # 4. ç»Ÿä¸€æ¸…æ´—
            cols = {"date": "æ—¥æœŸ", "type": "ç±»å‹", "amount": "é‡‘é¢", "merchant": "å¤‡æ³¨", "category": "åˆ†ç±»"}
            all_df = all_df.rename(columns=cols)
            for c in cols.values(): 
                if c not in all_df.columns: all_df[c] = ""
            
            all_df['é‡‘é¢'] = pd.to_numeric(all_df['é‡‘é¢'], errors='coerce').fillna(0)
            
            debug_log["total_time"] = time.time() - t_start
            return all_df, None, debug_log

        except Exception as e:
            return None, str(e), debug_log

    @staticmethod
    def process_image(filename, image_bytes, api_key):
        t_start = time.time()
        debug_log = {"file": filename}
        try:
            b64 = base64.b64encode(image_bytes).decode('utf-8')
            client = get_llm_client(api_key)
            resp = client.chat.completions.create(
                model=VISION_MODEL_NAME,
                messages=[{
                    "role": "user",
                    "content": [
                        {"type": "text", "text": "æå–è´¦å•ã€‚è¿”å›çº¯JSONæ•°ç»„ï¼š[{date, type, amount, merchant, category}]"},
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64}"}}
                    ]
                }],
                max_tokens=4096
            )
            raw = resp.choices[0].message.content
            debug_log["ai_response"] = raw
            data, err = extract_json_from_text(raw)
            
            if err: return None, err, debug_log
            if not data: return None, "æ— æ•°æ®", debug_log
            
            df = pd.DataFrame(data if isinstance(data, list) else [data])
            cols = {"date": "æ—¥æœŸ", "type": "ç±»å‹", "amount": "é‡‘é¢", "merchant": "å¤‡æ³¨", "category": "åˆ†ç±»"}
            df = df.rename(columns=cols)
            for c in cols.values(): 
                if c not in df.columns: df[c] = ""
            return df, None, debug_log
        except Exception as e:
            return None, str(e), debug_log

    @staticmethod
    def merge_data(old_df, new_df):
        if new_df is None or new_df.empty: return old_df, 0
        def fp(d): return d['æ—¥æœŸ'].astype(str) + d['é‡‘é¢'].astype(str) + d['å¤‡æ³¨'].str[:6]
        if old_df.empty: return new_df, len(new_df)
        
        new_df = DataManager._clean_df_types(new_df) # ç¡®ä¿ç±»å‹ä¸€è‡´
        old_fp = set(fp(old_df))
        new_df['_fp'] = fp(new_df)
        to_add = new_df[~new_df['_fp'].isin(old_fp)].drop(columns=['_fp'])
        
        if to_add.empty: return old_df, 0
        merged = pd.concat([old_df, to_add], ignore_index=True)
        merged = merged.sort_values('æ—¥æœŸ', ascending=False).reset_index(drop=True)
        return merged, len(to_add)

# --- Main ---
def main():
    if 'debug_mode' not in st.session_state: st.session_state.debug_mode = False
    
    st.sidebar.title("âš™ï¸ è®¾ç½®")
    st.session_state.debug_mode = st.sidebar.checkbox("ğŸ è°ƒè¯•æ¨¡å¼", value=st.session_state.debug_mode)
    api_key = st.secrets.get("SILICONFLOW_API_KEY") or st.sidebar.text_input("API Key", type="password")
    
    dm = DataManager(st.secrets.get("GITHUB_TOKEN"), st.secrets.get("GITHUB_REPO"))
    
    if dm.use_github:
        st.sidebar.success(f"äº‘ç«¯: {dm.repo}")
        if st.sidebar.button("â˜ï¸ å¼ºåˆ¶åŒæ­¥"):
            df, sha = dm.load_data(True)
            st.session_state.ledger_data = df
            st.session_state.github_sha = sha
            st.rerun()
    
    payday = st.sidebar.number_input("å‘è–ªæ—¥", 1, 31, 10)
    assets = st.sidebar.number_input("èµ„äº§", value=3000.0)

    if 'ledger_data' not in st.session_state:
        df, sha = dm.load_data()
        st.session_state.ledger_data = df
        st.session_state.github_sha = sha

    st.title("ğŸ’° AI æ™ºèƒ½è´¦æœ¬ Pro")
    
    # æ¦‚è§ˆé€»è¾‘
    today = date.today()
    df = st.session_state.ledger_data.copy()
    m_spend = 0.0
    if not df.empty:
        df['dt'] = pd.to_datetime(df['æ—¥æœŸ'], errors='coerce')
        mask = (df['dt'].dt.month == today.month) & (df['dt'].dt.year == today.year) & (df['ç±»å‹'] == 'æ”¯å‡º')
        m_spend = df.loc[mask, 'é‡‘é¢'].sum()
        
    next_pay = date(today.year, today.month, payday)
    if today.day >= payday:
        next_pay = (next_pay + pd.DateOffset(months=1)).date()
    days_left = (next_pay - today).days
    
    c1, c2, c3, c4 = st.columns(4)
    c1.metric("èµ„äº§", f"Â¥{assets:,.0f}")
    c2.metric("æœ¬æœˆæ”¯å‡º", f"Â¥{m_spend:,.0f}")
    c3.metric("è·å‘è–ª", f"{days_left}å¤©")
    budget = (assets / max(1, days_left))
    c4.metric("æ—¥å‡å¯ç”¨", f"Â¥{budget:.0f}", delta=f"{budget-DEFAULT_TARGET_SPEND:.0f}")

    st.divider()
    
    t_imp, t_add, t_his, t_stat = st.tabs(["ğŸ“¥ å¯¼å…¥", "âœï¸ è®°è´¦", "ğŸ“‹ æ˜ç»†", "ğŸ“Š ç»Ÿè®¡"])
    
    with t_imp:
        files = st.file_uploader("ä¼ æ–‡ä»¶ (PDF/å›¾ç‰‡/Excel)", accept_multiple_files=True)
        if files and st.button("ğŸš€ å¼€å§‹è§£æ", type="primary"):
            if not api_key: st.stop()
            
            tasks_doc, tasks_img = [], []
            for f in files:
                f.seek(0)
                b = f.read()
                ext = f.name.split('.')[-1].lower()
                if ext in ['png','jpg','jpeg']: tasks_img.append((f.name, b))
                else: tasks_doc.append((f.name, b))
            
            new_df = pd.DataFrame()
            dbg_logs = []
            
            prog = st.progress(0)
            tot = len(tasks_doc) + len(tasks_img)
            done = 0
            
            with concurrent.futures.ThreadPoolExecutor(max_workers=5) as exc:
                fs = {}
                for n, b in tasks_doc: fs[exc.submit(BillParser.identify_and_parse, n, b, api_key)] = n
                for n, b in tasks_img: fs[exc.submit(BillParser.process_image, n, b, api_key)] = n
                
                for f in concurrent.futures.as_completed(fs):
                    name = fs[f]
                    try:
                        res, err, dbg = f.result()
                        dbg_logs.append(dbg)
                        if res is not None:
                            new_df = pd.concat([new_df, res], ignore_index=True)
                            st.toast(f"âœ… {name} å®Œæˆ")
                        else:
                            st.error(f"âŒ {name}: {err}")
                    except Exception as e: st.error(str(e))
                    done += 1
                    prog.progress(done/tot)
            
            if st.session_state.debug_mode:
                st.json(dbg_logs)
                
            if not new_df.empty:
                m_df, cnt = DataManager.merge_data(st.session_state.ledger_data, new_df)
                if cnt > 0:
                    ok, sha = dm.save_data(m_df, st.session_state.get('github_sha'))
                    if ok:
                        st.session_state.ledger_data = m_df
                        st.session_state.github_sha = sha
                        st.balloons()
                        st.success(f"å¯¼å…¥ {cnt} æ¡")
                else: st.warning("æ— æ–°æ•°æ®")

    with t_add:
        with st.form("add"):
            c1, c2, c3 = st.columns(3)
            d = c1.date_input("æ—¥æœŸ")
            t = c2.selectbox("ç±»å‹", ["æ”¯å‡º", "æ”¶å…¥"])
            a = c3.number_input("é‡‘é¢", min_value=0.01)
            cat = st.selectbox("åˆ†ç±»", ["é¤é¥®", "äº¤é€š", "è´­ç‰©", "å±…ä½", "å¨±ä¹", "åŒ»ç–—", "å·¥èµ„", "å…¶ä»–"])
            rem = st.text_input("å¤‡æ³¨")
            if st.form_submit_button("ä¿å­˜", use_container_width=True):
                r = pd.DataFrame([{"æ—¥æœŸ": str(d), "ç±»å‹": t, "é‡‘é¢": a, "åˆ†ç±»": cat, "å¤‡æ³¨": rem}])
                m_df, cnt = DataManager.merge_data(st.session_state.ledger_data, r)
                ok, sha = dm.save_data(m_df, st.session_state.get('github_sha'))
                if ok:
                    st.session_state.ledger_data = m_df
                    st.session_state.github_sha = sha
                    st.success("å·²ä¿å­˜")
                    st.rerun()

    with t_his:
        st.session_state.ledger_data = DataManager._clean_df_types(st.session_state.ledger_data)
        edf = st.data_editor(st.session_state.ledger_data, use_container_width=True, num_rows="dynamic",
                             column_config={"é‡‘é¢": st.column_config.NumberColumn(format="Â¥%.2f"),
                                            "æ—¥æœŸ": st.column_config.DateColumn(format="YYYY-MM-DD"),
                                            "ç±»å‹": st.column_config.SelectboxColumn(options=["æ”¯å‡º", "æ”¶å…¥"])})
        if st.button("åŒæ­¥ä¿®æ”¹"):
            if not edf.equals(st.session_state.ledger_data):
                ok, sha = dm.save_data(edf, st.session_state.get('github_sha'))
                if ok:
                    st.session_state.ledger_data = edf
                    st.session_state.github_sha = sha
                    st.success("å·²åŒæ­¥")

    with t_stat:
        if not df.empty:
            df['é‡‘é¢'] = pd.to_numeric(df['é‡‘é¢'], errors='coerce').fillna(0)
            exp = df[df['ç±»å‹']=='æ”¯å‡º']
            c1, c2 = st.columns(2)
            with c1:
                if not exp.empty: st.bar_chart(exp.groupby("åˆ†ç±»")['é‡‘é¢'].sum())
            with c2:
                if not exp.empty: st.line_chart(exp.groupby("æ—¥æœŸ")['é‡‘é¢'].sum())
            
            if st.button("ç”ŸæˆAIæœˆæŠ¥") and api_key:
                with st.spinner("AI åˆ†æä¸­..."):
                    csv = exp.sort_values('æ—¥æœŸ', ascending=False).head(100).to_csv(index=False)
                    try:
                        client = get_llm_client(api_key)
                        r = client.chat.completions.create(model=TEXT_MODEL_NAME, messages=[
                            {"role":"system","content":"ç®€è¾£ç‚¹è¯„æ¶ˆè´¹ä¹ æƒ¯ï¼Œç»™å‡ºçœé’±å»ºè®®ã€‚"},
                            {"role":"user","content":csv}], max_tokens=1000)
                        st.markdown(r.choices[0].message.content)
                    except Exception as e: st.error(str(e))

if __name__ == "__main__":
    main()
