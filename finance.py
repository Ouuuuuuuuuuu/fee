import streamlit as st
import pandas as pd
import datetime
from datetime import date
import requests
import json
import base64
from io import StringIO, BytesIO
import os
import pdfplumber
import re

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="AI æ™ºèƒ½è´¦æœ¬", page_icon="ğŸ’°", layout="wide")

# --- å¸¸é‡é…ç½® ---
DEFAULT_TARGET_SPEND = 60.0  # æ¯æ—¥ä½“é¢æ”¯å‡ºæ ‡å‡†
GITHUB_API_URL = "https://api.github.com"
VISION_MODEL_NAME = "Qwen/Qwen3-VL-8B-Instruct" 
TEXT_MODEL_NAME = "deepseek-ai/DeepSeek-V3.2"

# --- å­˜å‚¨ç±» ---
class DataManager:
    """æ•°æ®ç®¡ç†ç±»ï¼Œæ”¯æŒ GitHub è¿œç¨‹å­˜å‚¨å’Œæœ¬åœ° CSV å­˜å‚¨"""
    def __init__(self, github_token=None, repo=None, filename="ledger.csv"):
        self.github_token = github_token
        if repo and repo.startswith("http"):
            self.repo = repo.rstrip("/").split("github.com/")[-1]
        else:
            self.repo = repo
        self.filename = filename
        self.use_github = bool(github_token and self.repo)

    def load_data(self):
        if self.use_github:
            return self._load_from_github()
        else:
            return self._load_from_local()

    def save_data(self, df, sha=None):
        if self.use_github:
            return self._save_to_github(df, sha)
        else:
            return self._save_to_local(df)

    def _load_from_local(self):
        if os.path.exists(self.filename):
            try:
                return pd.read_csv(self.filename), None
            except:
                return pd.DataFrame(columns=["æ—¥æœŸ", "ç±»å‹", "é‡‘é¢", "å¤‡æ³¨", "åˆ†ç±»"]), None
        return pd.DataFrame(columns=["æ—¥æœŸ", "ç±»å‹", "é‡‘é¢", "å¤‡æ³¨", "åˆ†ç±»"]), None

    def _save_to_local(self, df):
        df.to_csv(self.filename, index=False)
        return True

    def _load_from_github(self):
        headers = {
            "Authorization": f"token {self.github_token}",
            "Accept": "application/vnd.github.v3+json"
        }
        url = f"{GITHUB_API_URL}/repos/{self.repo}/contents/{self.filename}"
        response = requests.get(url, headers=headers)
        
        if response.status_code == 200:
            content = response.json()
            csv_str = base64.b64decode(content['content']).decode('utf-8')
            try:
                return pd.read_csv(StringIO(csv_str)), content['sha']
            except pd.errors.EmptyDataError:
                return pd.DataFrame(columns=["æ—¥æœŸ", "ç±»å‹", "é‡‘é¢", "å¤‡æ³¨", "åˆ†ç±»"]), content['sha']
        elif response.status_code == 404:
            return pd.DataFrame(columns=["æ—¥æœŸ", "ç±»å‹", "é‡‘é¢", "å¤‡æ³¨", "åˆ†ç±»"]), None
        else:
            st.error(f"GitHub è¯»å–é”™è¯¯: {response.status_code}")
            return pd.DataFrame(columns=["æ—¥æœŸ", "ç±»å‹", "é‡‘é¢", "å¤‡æ³¨", "åˆ†ç±»"]), None

    def _save_to_github(self, df, sha):
        headers = {
            "Authorization": f"token {self.github_token}",
            "Accept": "application/vnd.github.v3+json"
        }
        csv_str = df.to_csv(index=False)
        content_bytes = base64.b64encode(csv_str.encode('utf-8')).decode('utf-8')
        
        url = f"{GITHUB_API_URL}/repos/{self.repo}/contents/{self.filename}"
        data = {
            "message": f"Update ledger {datetime.datetime.now()}",
            "content": content_bytes
        }
        if sha:
            data["sha"] = sha
        response = requests.put(url, headers=headers, data=json.dumps(data))
        return response.status_code in [200, 201]

# --- è´¦å•è§£æä¸å»é‡ç±» ---
class BillParser:
    @staticmethod
    def identify_and_parse(file):
        """æ™ºèƒ½è¯†åˆ«æ–‡ä»¶ç±»å‹å¹¶è§£æ"""
        filename = file.name.lower()
        
        if filename.endswith('.csv'):
            return BillParser._parse_csv(file)
        elif filename.endswith(('.xls', '.xlsx')):
            return BillParser._parse_excel(file)
        elif filename.endswith('.pdf'):
            return BillParser._parse_pdf(file)
        else:
            return None, "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œè¯·ä¸Šä¼  CSV, Excel æˆ– PDF"

    @staticmethod
    def _parse_csv(file):
        """è§£æ CSV (é€‚é…å¾®ä¿¡/æ”¯ä»˜å®)"""
        try:
            content = file.getvalue().decode('utf-8')
        except UnicodeDecodeError:
            file.seek(0)
            content = file.getvalue().decode('gbk', errors='ignore')

        # --- ç­–ç•¥ï¼šå…ˆåˆ¤æ–­æ˜¯å“ªç§è´¦å•ï¼Œå†ç²¾å‡†å®šä½è¡¨å¤´ ---
        
        # 1. å¾®ä¿¡ç‰¹å¾
        if "å¾®ä¿¡æ”¯ä»˜è´¦å•æ˜ç»†" in content or "å•†æˆ·å•å·" in content:
            return BillParser._parse_wechat_content(content)
        
        # 2. æ”¯ä»˜å®ç‰¹å¾
        # æ”¯ä»˜å®é€šå¸¸åŒ…å« "æ”¯ä»˜å®äº¤æ˜“è®°å½•æ˜ç»†" æˆ–è€… åˆ—ååŒ…å« "å•†å“è¯´æ˜" å’Œ "å¯¹æ–¹è´¦å·"
        elif "æ”¯ä»˜å®" in content or "Partner Transaction ID" in content:
            return BillParser._parse_alipay_content(content)
             
        # é»˜è®¤å°è¯•æ”¯ä»˜å®è§£æï¼ˆå®¹é”™ï¼‰
        return BillParser._parse_alipay_content(content)

    @staticmethod
    def _parse_excel(file):
        """è§£æ Excel (æ‹›å•†é“¶è¡Œç­‰)"""
        try:
            df = pd.read_excel(file)
        except Exception as e:
            return None, f"Excel è¯»å–å¤±è´¥: {e}"

        cols = [str(c) for c in df.columns]
        col_str = " ".join(cols)
        
        if "äº¤æ˜“æ—¥æœŸ" in col_str and ("æ”¯å‡º" in col_str or "äº¤æ˜“é‡‘é¢" in col_str):
            return BillParser._parse_cmb_dataframe(df)
        
        return None, "æœªè¯†åˆ«çš„ Excel è´¦å•æ ¼å¼ã€‚"

    @staticmethod
    def _parse_pdf(file):
        """è§£æ PDF (æ‹›å•†é“¶è¡Œ)"""
        try:
            results = []
            with pdfplumber.open(file) as pdf:
                for page in pdf.pages:
                    # æå–è¡¨æ ¼
                    table = page.extract_table()
                    if not table:
                        continue
                    
                    # å¯»æ‰¾è¡¨å¤´è¡Œ (æ‹›è¡ŒPDFé€šå¸¸æœ‰ 'è®°è´¦æ—¥æœŸ' æˆ– 'Date')
                    header_idx = -1
                    for i, row in enumerate(table):
                        # æ¸…æ´— row ä¸­çš„ None
                        row_text = [str(cell).replace('\n', '') for cell in row if cell]
                        row_str = "".join(row_text)
                        if "è®°è´¦æ—¥æœŸ" in row_str or "Date" in row_str and "Currency" in row_str:
                            header_idx = i
                            break
                    
                    if header_idx == -1:
                        continue # æ²¡æ‰¾åˆ°è¡¨å¤´ï¼Œè·³è¿‡æ­¤é¡µ

                    # ç¡®å®šåˆ—ç´¢å¼• (åŸºäºæ‹›è¡Œæ ‡å‡†PDFæ ¼å¼)
                    # é€šå¸¸: è®°è´¦æ—¥æœŸ(0), è´§å¸(1), äº¤æ˜“é‡‘é¢(2), è”æœºä½™é¢(3), äº¤æ˜“æ‘˜è¦(4), å¯¹æ‰‹ä¿¡æ¯(5)
                    # æ³¨æ„ï¼šæœ‰æ—¶å€™å¯èƒ½æœ‰é¢å¤–ç©ºåˆ—ï¼Œéœ€è¦åŠ¨æ€åŒ¹é…
                    headers = [str(h).replace('\n', '').strip() for h in table[header_idx] if h]
                    
                    # å¼€å§‹è§£ææ•°æ®
                    for row in table[header_idx+1:]:
                        # è¿‡æ»¤æ— æ•ˆè¡Œ (ä¾‹å¦‚ä¸‹ä¸€é¡µçš„è¡¨å¤´æˆ–è€…æ˜¯ç©ºçš„)
                        if not row or len(row) < 3: continue
                        
                        # ç®€å•æ˜ å°„ï¼šå‡è®¾å‰å‡ åˆ—å›ºå®š
                        # æ¸…æ´—æ¢è¡Œç¬¦
                        clean_row = [str(cell).strip() if cell else "" for cell in row]
                        
                        # æ—¥æœŸåˆ— (é€šå¸¸ç¬¬1åˆ—)
                        date_str = clean_row[0].replace('\n', '')
                        if not re.match(r'\d{4}-\d{2}-\d{2}', date_str):
                            continue # ä¸æ˜¯æ—¥æœŸï¼Œè·³è¿‡

                        # é‡‘é¢åˆ— (é€šå¸¸ç¬¬3åˆ—)
                        amt_str = clean_row[2].replace(',', '').replace('\n', '')
                        try:
                            amt = float(amt_str)
                        except:
                            continue

                        final_type = "æ”¯å‡º" if amt < 0 else "æ”¶å…¥"
                        final_amt = abs(amt)

                        # å¤‡æ³¨ä¿¡æ¯ (æ‘˜è¦ + å¯¹æ‰‹ä¿¡æ¯)
                        # æ‘˜è¦é€šå¸¸ç¬¬5åˆ—ï¼Œå¯¹æ‰‹ä¿¡æ¯ç¬¬6åˆ— (ç´¢å¼•4, 5)
                        memo = ""
                        if len(clean_row) > 4:
                            memo += clean_row[4].replace('\n', ' ')
                        if len(clean_row) > 5:
                            memo += " " + clean_row[5].replace('\n', ' ')

                        results.append({
                            "æ—¥æœŸ": date_str,
                            "ç±»å‹": final_type,
                            "é‡‘é¢": final_amt,
                            "å¤‡æ³¨": memo.strip(),
                            "åˆ†ç±»": "æ‹›è¡ŒPDF"
                        })
            
            if not results:
                return None, "PDF è§£ææˆåŠŸä½†æœªæå–åˆ°æœ‰æ•ˆæ•°æ®ï¼Œè¯·ç¡®è®¤æ˜¯æ‹›å•†é“¶è¡Œæµæ°´ã€‚"
                
            return pd.DataFrame(results), None

        except Exception as e:
            return None, f"PDF è§£æå¼‚å¸¸: {str(e)}"

    @staticmethod
    def _parse_wechat_content(content):
        # å¾®ä¿¡é€»è¾‘ä¼˜åŒ–ï¼šå¯»æ‰¾ "äº¤æ˜“æ—¶é—´" æ‰€åœ¨è¡Œä½œä¸º Header
        lines = content.split('\n')
        start_row = 0
        found = False
        for i, line in enumerate(lines):
            # å¾®ä¿¡è¡¨å¤´ç‰¹å¾ï¼šåŒ…å« 'äº¤æ˜“æ—¶é—´' ä¸”åŒ…å« 'å½“å‰çŠ¶æ€'
            if "äº¤æ˜“æ—¶é—´" in line and "å½“å‰çŠ¶æ€" in line:
                start_row = i
                found = True
                break
        
        if not found:
            return None, "æœªæ‰¾åˆ°å¾®ä¿¡è´¦å•è¡¨å¤´"

        try:
            df = pd.read_csv(StringIO(content), header=start_row)
        except:
            return None, "å¾®ä¿¡CSVç»“æ„é”™è¯¯"

        df.columns = [c.strip() for c in df.columns]
        
        # ç­›é€‰æ”¯ä»˜æˆåŠŸçš„
        if 'å½“å‰çŠ¶æ€' in df.columns:
            df = df[df['å½“å‰çŠ¶æ€'] == 'æ”¯ä»˜æˆåŠŸ']
        
        results = []
        for _, row in df.iterrows():
            row_type = row.get('æ”¶/æ”¯', '')
            if row_type == "/" or row_type == "ä¸è®¡æ”¶æ”¯": continue
            
            final_type = "æ”¯å‡º" if row_type == "æ”¯å‡º" else "æ”¶å…¥"
            # å¤„ç†é‡‘é¢ï¼šå» Â¥ ç¬¦å·
            amt_str = str(row.get('é‡‘é¢(å…ƒ)', 0)).replace('Â¥', '').replace(',', '')
            try:
                amt = float(amt_str)
            except:
                continue
            
            # æ—¥æœŸå¤„ç†
            try:
                d_str = pd.to_datetime(row['äº¤æ˜“æ—¶é—´']).strftime('%Y-%m-%d')
            except:
                continue

            # ç»„åˆå¤‡æ³¨ï¼šå•†å“ + äº¤æ˜“å¯¹æ–¹
            item = str(row.get('å•†å“', '')).strip()
            partner = str(row.get('äº¤æ˜“å¯¹æ–¹', '')).strip()
            memo = f"{partner} - {item}" if partner else item

            results.append({
                "æ—¥æœŸ": d_str,
                "ç±»å‹": final_type,
                "é‡‘é¢": amt,
                "å¤‡æ³¨": memo.strip(),
                "åˆ†ç±»": "å¾®ä¿¡å¯¼å…¥"
            })
        return pd.DataFrame(results), None

    @staticmethod
    def _parse_alipay_content(content):
        # æ”¯ä»˜å®é€»è¾‘ä¼˜åŒ–
        lines = content.split('\n')
        start_row = 0
        found = False
        for i, line in enumerate(lines):
            # æ”¯ä»˜å®è¡¨å¤´ç‰¹å¾ï¼šåŒ…å« 'äº¤æ˜“æ—¶é—´' ä¸”åŒ…å« 'äº¤æ˜“åˆ†ç±»' (ç”¨æˆ·æä¾›çš„æ ·æœ¬ç‰¹å¾)
            # æˆ–è€…åŒ…å« 'äº¤æ˜“æ—¶é—´' å’Œ 'å•†å“è¯´æ˜'
            if "äº¤æ˜“æ—¶é—´" in line and ("äº¤æ˜“åˆ†ç±»" in line or "å•†å“è¯´æ˜" in line):
                start_row = i
                found = True
                break
        
        if not found:
            # å°è¯•æš´åŠ›å›é€€æŸ¥æ‰¾
            # æœ‰æ—¶å€™åˆ†éš”çº¿åœ¨è¡¨å¤´ä¸Šé¢
            for i, line in enumerate(lines):
                if "----------------" in line:
                    start_row = i + 1
                    found = True
                    break
        
        if not found:
             return None, "æœªæ‰¾åˆ°æ”¯ä»˜å®è´¦å•è¡¨å¤´"

        try:
            df = pd.read_csv(StringIO(content), header=start_row)
        except:
            return None, "æ”¯ä»˜å®CSVç»“æ„é”™è¯¯"

        df.columns = [c.strip() for c in df.columns]
        
        # çŠ¶æ€è¿‡æ»¤
        if 'äº¤æ˜“çŠ¶æ€' in df.columns:
            df = df[df['äº¤æ˜“çŠ¶æ€'].isin(['äº¤æ˜“æˆåŠŸ', 'æ”¯ä»˜æˆåŠŸ', 'å·²æ”¯å‡º', 'èµ„é‡‘è½¬ç§»'])]

        results = []
        for _, row in df.iterrows():
            # è¿‡æ»¤ç©ºé‡‘é¢
            if pd.isna(row.get('é‡‘é¢')): continue
            
            row_type = str(row.get('æ”¶/æ”¯', '')).strip()
            # ç”¨æˆ·æ ·æœ¬æ˜¾ç¤ºæœ‰ "ä¸è®¡æ”¶æ”¯"ï¼Œé€šå¸¸æˆ‘ä»¬ä¸è®°è¿™ç¬”ï¼ˆå› ä¸ºå¯èƒ½æ˜¯ç†è´¢/è½¬è´¦ï¼‰ï¼Œæˆ–è€…è®°ä¸ºæ”¯å‡ºï¼Ÿ
            # æŒ‰ç…§æƒ¯ä¾‹ï¼Œ"ä¸è®¡æ”¶æ”¯" å¾€å¾€æ˜¯ä¿¡ç”¨å¡è¿˜æ¬¾æˆ–ç†è´¢ï¼Œä¸ºäº†ä¸é‡è®°ï¼Œé€šå¸¸å¿½ç•¥ï¼Œé™¤éç”¨æˆ·å¼ºè¡Œè¦
            # è¿™é‡Œä¿æŒå¿½ç•¥é€»è¾‘
            if row_type == "ä¸è®¡æ”¶æ”¯" or row_type == "": continue
            
            final_type = "æ”¯å‡º" if row_type == "æ”¯å‡º" else "æ”¶å…¥"
            try:
                amt = float(str(row['é‡‘é¢']))
            except:
                continue
            
            try:
                d_str = pd.to_datetime(row['äº¤æ˜“æ—¶é—´']).strftime('%Y-%m-%d')
            except:
                continue
            
            partner = str(row.get('äº¤æ˜“å¯¹æ–¹', '')).strip()
            item_name = str(row.get('å•†å“è¯´æ˜', '')).strip()
            cat = str(row.get('äº¤æ˜“åˆ†ç±»', 'æ”¯ä»˜å®å¯¼å…¥')).strip()

            results.append({
                "æ—¥æœŸ": d_str,
                "ç±»å‹": final_type,
                "é‡‘é¢": amt,
                "å¤‡æ³¨": f"{partner} {item_name}".strip(),
                "åˆ†ç±»": cat
            })
        return pd.DataFrame(results), None

    @staticmethod
    def _parse_cmb_dataframe(df):
        """æ‹›è¡Œ Excel DataFrame è§£æ"""
        # å¯»æ‰¾ Header
        header_row_idx = 0
        for i in range(len(df)):
            row_vals = [str(v) for v in df.iloc[i].values]
            if "äº¤æ˜“æ—¥æœŸ" in row_vals or "è®°è´¦æ—¥æœŸ" in row_vals:
                header_row_idx = i
                break
        
        df.columns = df.iloc[header_row_idx]
        df = df.iloc[header_row_idx+1:]
        df.columns = [str(c).strip() for c in df.columns]
        
        results = []
        for _, row in df.iterrows():
            date_val = row.get('äº¤æ˜“æ—¥æœŸ') or row.get('è®°è´¦æ—¥æœŸ')
            if pd.isna(date_val): continue
            
            try:
                d_str = pd.to_datetime(str(date_val)).strftime('%Y-%m-%d')
            except:
                continue

            # é‡‘é¢å¤„ç†
            expense = row.get('æ”¯å‡º', 0)
            income = row.get('æ”¶å…¥', 0)
            trans_amt = row.get('äº¤æ˜“é‡‘é¢', 0)

            final_amt = 0.0
            final_type = "æ”¯å‡º"
            
            if trans_amt != 0 and not pd.isna(trans_amt):
                # æ‹›è¡Œå¯èƒ½æ˜¯ "-22.00" å­—ç¬¦ä¸²
                try:
                    t_val = float(str(trans_amt).replace(',', ''))
                    final_amt = abs(t_val)
                    final_type = "æ”¯å‡º" if t_val < 0 else "æ”¶å…¥"
                except:
                    pass
            elif expense and float(str(expense).replace(',', '')) > 0:
                final_amt = float(str(expense).replace(',', ''))
                final_type = "æ”¯å‡º"
            elif income and float(str(income).replace(',', '')) > 0:
                final_amt = float(str(income).replace(',', ''))
                final_type = "æ”¶å…¥"
            
            if final_amt == 0: continue

            memo = str(row.get('äº¤æ˜“å¤‡æ³¨') or row.get('äº¤æ˜“æ‘˜è¦') or "") + " " + str(row.get('å¯¹æ‰‹ä¿¡æ¯') or "")
            
            results.append({
                "æ—¥æœŸ": d_str,
                "ç±»å‹": final_type,
                "é‡‘é¢": final_amt,
                "å¤‡æ³¨": memo.strip(),
                "åˆ†ç±»": "æ‹›è¡Œå¯¼å…¥"
            })
            
        return pd.DataFrame(results), None

    @staticmethod
    def merge_and_deduplicate(old_df, new_df):
        """
        åˆå¹¶å¹¶å»é‡
        ç­–ç•¥ï¼šå¦‚æœ Date + Amount + Type ç›¸åŒï¼Œè§†ä¸ºé‡å¤ã€‚
        å¼ºåŒ–ï¼šæ‹›è¡Œè´¦å•å¦‚æœå¤‡æ³¨åŒ…å« 'æ”¯ä»˜å®'/'å¾®ä¿¡'/'è´¢ä»˜é€š' ä¸”é‡‘é¢åŒ¹é…ï¼Œè§†ä¸ºé‡å¤ï¼ˆå³ä½¿ Old æ•°æ®é‡Œæ²¡æœ‰å¤‡æ³¨ï¼‰ã€‚
        """
        if new_df is None or new_df.empty:
            return old_df, 0, 0

        added_rows = []
        skipped_count = 0
        
        # å»ºç«‹ç´¢å¼•ï¼š(æ—¥æœŸ, é‡‘é¢) -> å­˜åœ¨çš„è®°å½•åˆ—è¡¨
        # ä½¿ç”¨ set å­˜å‚¨ key åŠ é€Ÿåˆ¤æ–­
        # Key æ ¼å¼: "2023-01-01_100.50_æ”¯å‡º"
        existing_keys = set()
        for _, row in old_df.iterrows():
            try:
                amt = float(row['é‡‘é¢'])
                key = f"{row['æ—¥æœŸ']}_{amt:.2f}_{row['ç±»å‹']}"
                existing_keys.add(key)
            except:
                continue

        for _, row in new_df.iterrows():
            try:
                amt = float(row['é‡‘é¢'])
                key = f"{row['æ—¥æœŸ']}_{amt:.2f}_{row['ç±»å‹']}"
            except:
                continue
            
            is_duplicate = False
            
            # 1. ä¸¥æ ¼å…¨åŒ¹é…æ£€æŸ¥
            if key in existing_keys:
                is_duplicate = True
                
            # 2. æ‹›å•†é“¶è¡Œç‰¹æ®Šå»é‡é€»è¾‘ (é‡å è´¦å•)
            # å¦‚æœè¿™ç¬”æ˜¯æ‹›è¡Œçš„ï¼Œä¸”é‡‘é¢/æ—¥æœŸå·²ç»åœ¨è´¦æœ¬é‡Œäº†ï¼ˆå¤§æ¦‚ç‡æ˜¯æ”¯ä»˜å®/å¾®ä¿¡è®°è¿‡äº†ï¼‰ï¼Œä¸”æ‹›è¡Œå¤‡æ³¨é‡Œæ˜ç¡®å†™äº†å®ƒæ˜¯ç¬¬ä¸‰æ–¹æ”¯ä»˜
            memo = str(row['å¤‡æ³¨'])
            is_cmb = "æ‹›è¡Œ" in str(row.get('åˆ†ç±»', ''))
            is_third_party_payment = any(k in memo for k in ["æ”¯ä»˜å®", "å¾®ä¿¡", "è´¢ä»˜é€š", "Tenpay", "Alipay", "ç¾å›¢", "äº¬ä¸œ", "é“¶è”å¿«æ·"])
            
            if is_duplicate:
                skipped_count += 1
                continue
            
            # å¦‚æœä¸æ˜¯ä¸¥æ ¼é‡å¤ï¼Œä½†å±äº [æ‹›è¡Œ] + [ç¬¬ä¸‰æ–¹æ”¯ä»˜å…³é”®è¯] + [è´¦æœ¬é‡Œå·²æœ‰åŒå¤©åŒé‡‘é¢è®°å½•]
            # è¿™ç§æƒ…å†µä¹Ÿè¦è·³è¿‡ï¼Œé˜²æ­¢åŒé‡è®°è´¦
            # æ³¨æ„ï¼šè¿™é‡Œçš„é€»è¾‘å‡è®¾â€œåŒå¤©åŒé‡‘é¢â€å°±æ˜¯åŒä¸€ç¬”äº¤æ˜“ï¼Œå¯¹äºå°é¢é«˜é¢‘äº¤æ˜“ï¼ˆå¦‚ä¸€å¤©ä¹°ä¸¤æ¬¡3å—é’±çš„æ°´ï¼‰å¯èƒ½ä¼šè¯¯æ€ï¼Œ
            # ä½†å¯¹äºæ•´ç†â€œå¾¡ä¸‰å®¶â€æµæ°´æ¥è¯´ï¼Œè¯¯æ€æ¦‚ç‡ä½äºé‡å¤è®°è´¦çš„çƒ¦æ¼ã€‚
            if is_cmb and is_third_party_payment and key in existing_keys:
                 skipped_count += 1
                 continue

            added_rows.append(row)
            existing_keys.add(key) # é˜²æ­¢æœ¬æ‰¹æ¬¡å†…è‡ªæˆ‘é‡å¤

        if not added_rows:
            return old_df, 0, skipped_count
            
        return pd.concat([old_df, pd.DataFrame(added_rows)], ignore_index=True), len(added_rows), skipped_count

# --- AI å¤„ç†å‡½æ•° ---
def process_bill_image(image_file, api_key):
    if not api_key:
        return None, "æœªé…ç½® API Key"

    image_bytes = image_file.getvalue()
    base64_image = base64.b64encode(image_bytes).decode('utf-8')

    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    prompt = """
    è¯·è¯†åˆ«è¿™å¼ è´¦å•å›¾ç‰‡ã€‚æå–ä»¥ä¸‹å­—æ®µå¹¶ä»¥JSONæ ¼å¼è¿”å›ï¼š
    1. date (æ ¼å¼YYYY-MM-DD)
    2. amount (æ•°å­—ç±»å‹ï¼Œä¸è¦å¸¦è´§å¸ç¬¦å·)
    3. merchant (å•†æˆ·åæˆ–äº¤æ˜“è¯´æ˜)
    4. category (ä»ä»¥ä¸‹é€‰æ‹©æœ€æ¥è¿‘çš„: é¤é¥®, äº¤é€š, è´­ç‰©, å±…ä½, å¨±ä¹, å·¥èµ„, å…¶ä»–)
    5. type (æ”¯å‡º æˆ– æ”¶å…¥)
    
    ç›´æ¥è¿”å›JSONï¼Œä¸éœ€è¦ ```json æ ‡è®°ã€‚
    """

    payload = {
        "model": VISION_MODEL_NAME, 
        "messages": [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{base64_image}"
                        }
                    }
                ]
            }
        ],
        "max_tokens": 512
    }

    try:
        response = requests.post(
            "[https://api.siliconflow.cn/v1/chat/completions](https://api.siliconflow.cn/v1/chat/completions)",
            headers=headers,
            json=payload,
            timeout=45
        )
        if response.status_code == 200:
            result = response.json()
            content = result['choices'][0]['message']['content']
            clean_content = content.replace("```json", "").replace("```", "").strip()
            return json.loads(clean_content), None
        else:
            return None, f"API Error {response.status_code}: {response.text}"
    except Exception as e:
        return None, f"è¯·æ±‚å¼‚å¸¸: {str(e)}"

# --- ä¸»ç¨‹åº ---
def main():
    # 1. é…ç½®åŠ è½½
    st.sidebar.title("âš™ï¸ ä¸ªäººè´¢åŠ¡è®¾ç½®")
    
    sf_api_key = st.secrets.get("SILICONFLOW_API_KEY", "")
    github_token = st.secrets.get("GITHUB_TOKEN", "")
    github_repo = st.secrets.get("GITHUB_REPO", "")

    dm = DataManager(github_token, github_repo)
    
    if dm.use_github:
        st.sidebar.success(f"â˜ï¸ æ•°æ®å­˜å‚¨: GitHub ({github_repo})")
    else:
        st.sidebar.warning("ğŸ“‚ æ•°æ®å­˜å‚¨: æœ¬åœ°æ¨¡å¼")

    payday = st.sidebar.number_input("æ¯æœˆå‘è–ªæ—¥", 1, 31, 10)
    current_cash = st.sidebar.number_input("å½“å‰ç°é‡‘/ä½™é¢", value=3000.0)

    # 2. åŠ è½½æ•°æ®
    if 'ledger_data' not in st.session_state:
        df, sha = dm.load_data()
        st.session_state.ledger_data = df
        st.session_state.github_sha = sha

    # 3. è´¢åŠ¡æ¦‚è§ˆ
    st.title("ğŸ’° æç®€è´¦æœ¬")
    
    today = date.today()
    if today.day >= payday:
        next_pay_date = date(today.year + (1 if today.month == 12 else 0), 1 if today.month == 12 else today.month + 1, payday)
    else:
        next_pay_date = date(today.year, today.month, payday)
    
    days_left = (next_pay_date - today).days
    
    col1, col2, col3 = st.columns(3)
    col1.metric("å½“å‰ä½™é¢", f"Â¥{current_cash:,.2f}")
    col2.metric("è·ç¦»å‘å·¥èµ„", f"{days_left} å¤©")
    
    if days_left > 0:
        daily_budget = current_cash / days_left
        gap = daily_budget - DEFAULT_TARGET_SPEND
        col3.metric("æ¯æ—¥å¯ç”¨", f"Â¥{daily_budget:.1f}", 
                    f"{gap:+.1f} (vs Â¥{DEFAULT_TARGET_SPEND})",
                    delta_color="normal" if gap >= 0 else "inverse")
    else:
        col3.metric("æ¯æ—¥å¯ç”¨", "N/A", "ä»Šæ—¥å‘è–ªï¼")

    st.divider()

    # 4. è®°è´¦åŠŸèƒ½åŒº - ç»Ÿä¸€å…¥å£
    tab_auto, tab_manual = st.tabs(["ğŸ“¤ æ™ºèƒ½å¯¼å…¥ (å¤šæ–‡ä»¶/å›¾ç‰‡)", "âœï¸ æ‰‹åŠ¨è®°è´¦"])

    with tab_auto:
        st.markdown("""
        <small>æ”¯æŒæ ¼å¼ï¼š
        1. **å›¾ç‰‡** (jpg/png) -> AI è‡ªåŠ¨è¯†åˆ«
        2. **æ–‡ä»¶** (csv/xlsx/xls/pdf) -> æ‰¹é‡å¯¼å…¥å¾®ä¿¡/æ”¯ä»˜å®/æ‹›è¡Œè´¦å• (è‡ªåŠ¨åˆå¹¶å»é‡)
        </small>
        """, unsafe_allow_html=True)
        
        # å…è®¸ä¸Šä¼ å¤šä¸ªæ–‡ä»¶
        uploaded_files = st.file_uploader(
            "ç‚¹å‡»ä¸Šä¼  (æ”¯æŒå¤šé€‰)", 
            type=['png', 'jpg', 'jpeg', 'csv', 'xlsx', 'xls', 'pdf'], 
            key="unified_upload",
            accept_multiple_files=True
        )
        
        if uploaded_files:
            # æ–‡ä»¶åˆ†ç±»
            img_files = [f for f in uploaded_files if f.name.split('.')[-1].lower() in ['png', 'jpg', 'jpeg']]
            data_files = [f for f in uploaded_files if f.name.split('.')[-1].lower() in ['csv', 'xlsx', 'xls', 'pdf']]

            col_a, col_b = st.columns(2)
            
            # --- æ‰¹é‡å¤„ç†æ•°æ®æ–‡ä»¶ ---
            if data_files:
                with col_a:
                    st.info(f"æ£€æµ‹åˆ° {len(data_files)} ä¸ªæ•°æ®æ–‡ä»¶")
                    if st.button(f"æ‰¹é‡è§£æå¯¼å…¥", key="btn_import_batch"):
                        total_added = 0
                        total_skipped = 0
                        
                        with st.spinner("æ­£åœ¨æ‰¹é‡è§£æ..."):
                            batch_df = pd.DataFrame()
                            
                            for f in data_files:
                                df_new, err = BillParser.identify_and_parse(f)
                                if err:
                                    st.error(f"æ–‡ä»¶ {f.name} è§£æå¤±è´¥: {err}")
                                elif df_new is not None and not df_new.empty:
                                    batch_df = pd.concat([batch_df, df_new], ignore_index=True)
                            
                            if not batch_df.empty:
                                merged_df, added_count, skipped_count = BillParser.merge_and_deduplicate(
                                    st.session_state.ledger_data, batch_df
                                )
                                total_added += added_count
                                total_skipped += skipped_count
                                
                                if total_added > 0:
                                    if dm.save_data(merged_df, st.session_state.get('github_sha')):
                                        st.session_state.ledger_data = merged_df
                                        st.session_state.github_sha = dm.load_data()[1]
                                        st.success(f"ğŸ‰ æ‰¹é‡å¯¼å…¥å®Œæˆï¼æ–°å¢ {total_added} æ¡è®°å½•ã€‚")
                                        if total_skipped > 0:
                                            st.info(f"ğŸ›¡ï¸ è‡ªåŠ¨è·³è¿‡äº† {total_skipped} æ¡é‡å¤æˆ–é‡åˆè®°å½•")
                                        st.rerun()
                                    else:
                                        st.error("ä¿å­˜å¤±è´¥")
                                else:
                                    st.warning(f"æ‰€æœ‰è®°å½•å‡å·²å­˜åœ¨ (è·³è¿‡ {total_skipped} æ¡)ã€‚")
                            else:
                                st.warning("æ²¡æœ‰è§£æå‡ºæœ‰æ•ˆæ•°æ®ã€‚")

            # --- æ‰¹é‡/å•å¼  å›¾ç‰‡å¤„ç† ---
            if img_files:
                with col_b:
                    st.info(f"æ£€æµ‹åˆ° {len(img_files)} å¼ å›¾ç‰‡")
                    if 'ocr_queue' not in st.session_state:
                        st.session_state.ocr_queue = []
                        
                    if st.button(f"å¼€å§‹ AI è¯†åˆ« ({len(img_files)}å¼ )", key="btn_ocr_batch"):
                        if not sf_api_key:
                            st.error("è¯·é…ç½® SILICONFLOW_API_KEY")
                        else:
                            with st.spinner("AI æ­£åœ¨é€å¼ è¯»å–..."):
                                for img_f in img_files:
                                    data, err = process_bill_image(img_f, sf_api_key)
                                    if not err and data:
                                        data['_filename'] = img_f.name
                                        st.session_state.ocr_queue.append(data)
                                    else:
                                        st.error(f"{img_f.name} è¯†åˆ«å¤±è´¥: {err}")
                            st.rerun()

        # --- OCR ç»“æœç¡®è®¤é˜Ÿåˆ— ---
        if 'ocr_queue' in st.session_state and len(st.session_state.ocr_queue) > 0:
            st.divider()
            st.subheader(f"ğŸ” å¾…ç¡®è®¤ OCR ç»“æœ (å‰©ä½™ {len(st.session_state.ocr_queue)} ä¸ª)")
            
            current_ocr = st.session_state.ocr_queue[0]
            
            with st.container(border=True):
                st.caption(f"æ¥æºæ–‡ä»¶: {current_ocr.get('_filename', 'Unknown')}")
                with st.form("ocr_confirm_queue"):
                    c1, c2 = st.columns(2)
                    o_date = c1.date_input("æ—¥æœŸ", pd.to_datetime(current_ocr.get('date', str(date.today()))))
                    o_type = c2.selectbox("ç±»å‹", ["æ”¯å‡º", "æ”¶å…¥"], index=1 if current_ocr.get('type') == 'æ”¶å…¥' else 0)
                    o_amt = c1.number_input("é‡‘é¢", float(current_ocr.get('amount', 0)))
                    o_cat = c2.text_input("åˆ†ç±»", current_ocr.get('category', 'é¤é¥®'))
                    o_desc = st.text_input("å¤‡æ³¨", current_ocr.get('merchant', ''))
                    
                    col_submit, col_skip = st.columns([1, 1])
                    if col_submit.form_submit_button("âœ… ç¡®è®¤æ·»åŠ "):
                        new_row = {"æ—¥æœŸ": str(o_date), "ç±»å‹": o_type, "é‡‘é¢": o_amt, "å¤‡æ³¨": o_desc, "åˆ†ç±»": o_cat}
                        st.session_state.ledger_data = pd.concat([st.session_state.ledger_data, pd.DataFrame([new_row])], ignore_index=True)
                        dm.save_data(st.session_state.ledger_data, st.session_state.get('github_sha'))
                        st.session_state.github_sha = dm.load_data()[1]
                        st.session_state.ocr_queue.pop(0)
                        st.rerun()
                        
                    if col_skip.form_submit_button("ğŸ—‘ï¸ è·³è¿‡æ­¤æ¡"):
                        st.session_state.ocr_queue.pop(0)
                        st.rerun()

    # --- Manual Tab ---
    with tab_manual:
        with st.form("manual_form"):
            c_m1, c_m2 = st.columns(2)
            m_date = c_m1.date_input("æ—¥æœŸ", date.today())
            m_type = c_m2.selectbox("ç±»å‹", ["æ”¯å‡º", "æ”¶å…¥"])
            m_amt = c_m1.number_input("é‡‘é¢", step=1.0)
            m_cat = c_m2.selectbox("åˆ†ç±»", ["é¤é¥®", "äº¤é€š", "è´­ç‰©", "å±…ä½", "å¨±ä¹", "å·¥èµ„", "å…¶ä»–"])
            m_desc = st.text_input("å¤‡æ³¨")
            
            if st.form_submit_button("ğŸ’¾ ä¿å­˜è®°å½•"):
                new_row = {"æ—¥æœŸ": str(m_date), "ç±»å‹": m_type, "é‡‘é¢": m_amt, "å¤‡æ³¨": m_desc, "åˆ†ç±»": m_cat}
                st.session_state.ledger_data = pd.concat([st.session_state.ledger_data, pd.DataFrame([new_row])], ignore_index=True)
                dm.save_data(st.session_state.ledger_data, st.session_state.get('github_sha'))
                st.session_state.github_sha = dm.load_data()[1]
                st.rerun()

    st.divider()

    # 5. å†å²è´¦å• & å¯è§†åŒ–
    if not st.session_state.ledger_data.empty:
        st.subheader("ğŸ“Š å†å²è´¦å•")
        edited_df = st.data_editor(
            st.session_state.ledger_data,
            num_rows="dynamic",
            use_container_width=True,
            key="history_editor"
        )
        if st.button("ğŸ”„ åŒæ­¥è¡¨æ ¼ä¿®æ”¹"):
            if dm.save_data(edited_df, st.session_state.get('github_sha')):
                st.session_state.ledger_data = edited_df
                st.session_state.github_sha = dm.load_data()[1]
                st.success("åŒæ­¥æˆåŠŸ")
                st.rerun()
        
        st.subheader("ğŸ“ˆ æ¶ˆè´¹é€è§†")
        chart_df = st.session_state.ledger_data.copy()
        chart_df['é‡‘é¢'] = pd.to_numeric(chart_df['é‡‘é¢'], errors='coerce').fillna(0)
        chart_df['æ—¥æœŸ'] = pd.to_datetime(chart_df['æ—¥æœŸ']).dt.date
        expense_df = chart_df[chart_df['ç±»å‹'] == 'æ”¯å‡º']
        
        if not expense_df.empty:
            t1, t2 = st.tabs(["ğŸ“Š åˆ†ç±»å æ¯”", "ğŸ“‰ æ¯æ—¥è¶‹åŠ¿"])
            with t1:
                st.bar_chart(expense_df.groupby('åˆ†ç±»')['é‡‘é¢'].sum().sort_values(ascending=False), color="#FF4B4B")
            with t2:
                st.line_chart(expense_df.groupby('æ—¥æœŸ')['é‡‘é¢'].sum())
    else:
        st.info("æš‚æ— æ•°æ®")

    # 6. AI åˆ†æ
    with st.expander("ğŸ¤– AI è´¢åŠ¡åˆ†æ"):
        if st.button("åˆ†ææˆ‘çš„å¼€é”€"):
            if sf_api_key and not st.session_state.ledger_data.empty:
                with st.spinner("AI æ­£åœ¨æ€è€ƒ..."):
                    summary = st.session_state.ledger_data.to_string()
                    payload = {
                        "model": TEXT_MODEL_NAME, 
                        "messages": [{"role": "user", "content": f"åˆ†æè¿™ä»½è´¦å•ï¼ŒæŒ‡å‡ºé—®é¢˜ï¼š\n{summary}"}]
                    }
                    try:
                        r = requests.post("[https://api.siliconflow.cn/v1/chat/completions](https://api.siliconflow.cn/v1/chat/completions)", 
                                        headers={"Authorization": f"Bearer {sf_api_key}"}, json=payload)
                        st.markdown(r.json()['choices'][0]['message']['content'])
                    except Exception as e:
                        st.error(f"AI æœåŠ¡å¼‚å¸¸: {e}")

if __name__ == "__main__":
    main()
