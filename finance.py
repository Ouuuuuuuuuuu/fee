import streamlit as st
import pandas as pd
import datetime
from datetime import date
import requests
import json
import base64
from io import StringIO, BytesIO
import os
import fitz  # PyMuPDF
import re
from openai import OpenAI, APITimeoutError
import concurrent.futures
import time

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="AI è´¦æœ¬ ", page_icon="ğŸ’°", layout="wide")

# --- å¸¸é‡é…ç½® ---
DEFAULT_TARGET_SPEND = 60.0  # æ¯æ—¥ä½“é¢æ”¯å‡ºæ ‡å‡†
GITHUB_API_URL = "https://api.github.com"
VISION_MODEL_NAME = "Qwen/Qwen3-VL-8B-Instruct" 
TEXT_MODEL_NAME = "deepseek-ai/DeepSeek-V3.2"

# --- ç¼“å­˜èµ„æºï¼šè·å– LLM å®¢æˆ·ç«¯ ---
@st.cache_resource
def get_llm_client(api_key):
    return OpenAI(api_key=api_key, base_url="https://api.siliconflow.cn/v1")

# --- å·¥å…·å‡½æ•°ï¼šé²æ£’çš„ JSON æå– ---
def extract_json_from_text(text):
    """ä½¿ç”¨æ­£åˆ™ä»æ··åˆæ–‡æœ¬ä¸­æå– JSON æ•°ç»„æˆ–å¯¹è±¡"""
    text = text.replace("```json", "").replace("```", "").strip()
    
    # å°è¯•æå–æ•°ç»„ [...]
    match_array = re.search(r'\[.*\]', text, re.DOTALL)
    if match_array:
        try:
            return json.loads(match_array.group())
        except:
            pass
            
    # å°è¯•æå–å¯¹è±¡ {...}
    match_obj = re.search(r'\{.*\}', text, re.DOTALL)
    if match_obj:
        try:
            return json.loads(match_obj.group())
        except:
            pass

    # æœ€åçš„æ‰‹æ®µï¼šå°è¯•ç›´æ¥è§£æ
    try:
        return json.loads(text)
    except:
        return None

# --- å­˜å‚¨ç±» ---
class DataManager:
    """æ•°æ®ç®¡ç†ç±»ï¼Œæ”¯æŒ GitHub è¿œç¨‹å­˜å‚¨å’Œæœ¬åœ° CSV å­˜å‚¨"""
    def __init__(self, github_token=None, repo=None, filename="ledger.csv"):
        self.github_token = github_token
        # å…¼å®¹å®Œæ•´ URL æˆ– repo path
        if repo and repo.startswith("http"):
            self.repo = repo.rstrip("/").split("github.com/")[-1]
        else:
            self.repo = repo
        self.filename = filename
        self.use_github = bool(github_token and self.repo)

    def load_data(self):
        """åŠ è½½æ•°æ®ï¼Œè¿”å› DataFrame å’Œ SHA"""
        if self.use_github:
            return self._load_from_github()
        else:
            return self._load_from_local()

    def save_data(self, df, sha=None):
        """ä¿å­˜æ•°æ®"""
        # ç¡®ä¿æ•°æ®æ ¼å¼ç»Ÿä¸€
        if 'æ—¥æœŸ' in df.columns:
            df['æ—¥æœŸ'] = df['æ—¥æœŸ'].astype(str)
        if self.use_github:
            return self._save_to_github(df, sha)
        else:
            return self._save_to_local(df)

    def _load_from_local(self):
        if os.path.exists(self.filename):
            try:
                return pd.read_csv(self.filename), None
            except:
                pass
        return self._create_empty_df(), None

    def _save_to_local(self, df):
        df.to_csv(self.filename, index=False)
        return True

    # --- ä¼˜åŒ–ï¼šæ·»åŠ ç¼“å­˜ï¼Œé¿å…æ¯æ¬¡åˆ·æ–°é¡µé¢éƒ½è¯·æ±‚ GitHubï¼Œç¼“å­˜ 5 åˆ†é’Ÿ ---
    @st.cache_data(ttl=300, show_spinner=False)
    def _fetch_github_content(_self):
        """å†…éƒ¨å‡½æ•°ï¼šå®é™…æ‰§è¡Œç½‘ç»œè¯·æ±‚ï¼Œå•ç‹¬æ‹†åˆ†ä»¥æ”¯æŒç¼“å­˜"""
        headers = {
            "Authorization": f"token {_self.github_token}",
            "Accept": "application/vnd.github.v3+json"
        }
        url = f"{GITHUB_API_URL}/repos/{_self.repo}/contents/{_self.filename}"
        try:
            # timeout ä¿æŒ 60s
            response = requests.get(url, headers=headers, timeout=60)
            if response.status_code == 200:
                return response.json(), None
            elif response.status_code == 404:
                return None, 404
            else:
                return None, response.status_code
        except Exception as e:
            return None, str(e)

    def _load_from_github(self):
        # è°ƒç”¨å¸¦ç¼“å­˜çš„è¯»å–å‡½æ•°
        content, error = self._fetch_github_content()
        
        if content:
            csv_str = base64.b64decode(content['content']).decode('utf-8')
            try:
                df = pd.read_csv(StringIO(csv_str))
                expected_df = self._create_empty_df()
                for col in expected_df.columns:
                    if col not in df.columns:
                        df[col] = ""
                return df, content['sha']
            except pd.errors.EmptyDataError:
                return self._create_empty_df(), content['sha']
        elif error == 404:
            return self._create_empty_df(), None
        else:
            if error:
                st.error(f"GitHub è¯»å–é”™è¯¯: {error}")
            return self._create_empty_df(), None

    def _save_to_github(self, df, sha):
        start_time = time.time()
        headers = {
            "Authorization": f"token {self.github_token}",
            "Accept": "application/vnd.github.v3+json"
        }
        csv_str = df.to_csv(index=False)
        content_bytes = base64.b64encode(csv_str.encode('utf-8')).decode('utf-8')
        
        url = f"{GITHUB_API_URL}/repos/{self.repo}/contents/{self.filename}"
        data = {
            "message": f"Update ledger {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "content": content_bytes
        }
        if sha:
            data["sha"] = sha
            
        try:
            # å¢åŠ  timeout åˆ° 60ç§’
            response = requests.put(url, headers=headers, data=json.dumps(data), timeout=60)
            end_time = time.time()
            if st.session_state.get('debug_mode'):
                st.toast(f"â˜ï¸ GitHub ä¿å­˜è€—æ—¶: {end_time - start_time:.2f}s")
            
            if response.status_code in [200, 201]:
                # --- ä¼˜åŒ–ï¼šä¿å­˜æˆåŠŸåæ¸…é™¤è¯»å–ç¼“å­˜ï¼Œç¡®ä¿ä¸‹æ¬¡è¯»å–æ˜¯æ–°çš„ ---
                self._fetch_github_content.clear()
                return True
            else:
                st.error(f"GitHub ä¿å­˜å¤±è´¥: {response.status_code} - {response.text}")
                return False
        except Exception as e:
            st.error(f"ä¿å­˜æ—¶ç½‘ç»œå¼‚å¸¸: {e}")
            return False

    @staticmethod
    def _create_empty_df():
        return pd.DataFrame(columns=["æ—¥æœŸ", "ç±»å‹", "é‡‘é¢", "å¤‡æ³¨", "åˆ†ç±»"])

# --- æ™ºèƒ½è´¦å•è§£æç±» ---
class BillParser:
    @staticmethod
    def identify_and_parse(filename, file_bytes, api_key):
        """
        å¤„ç†å•ä¸ªæ–‡ä»¶å†…å®¹
        æ³¨æ„ï¼šè¿™é‡Œä¸å†æ¥æ”¶ Streamlit çš„ UploadedFile å¯¹è±¡ï¼Œè€Œæ˜¯æ¥æ”¶ (filename, file_bytes)
        ä»è€Œå½»åº•è§£å†³ 'missing ScriptRunContext' é—®é¢˜
        """
        t_start = time.time()
        debug_info = {}
        
        if not api_key:
            return None, "æœªé…ç½® API Key", {}

        filename = filename.lower()
        content_text = ""
        source_type = "æœªçŸ¥æ–‡ä»¶"
        
        try:
            # 1. æå–æ–‡æœ¬ (åŸºäº file_bytes)
            t_read_start = time.time()
            
            # ä½¿ç”¨ BytesIO åŒ…è£…äºŒè¿›åˆ¶æ•°æ®ï¼Œä½¿å…¶åƒæ–‡ä»¶ä¸€æ ·å¯è¯»
            file_stream = BytesIO(file_bytes)
            
            if filename.endswith('.csv'):
                source_type = "CSVè´¦å•"
                try:
                    content_text = file_bytes.decode('utf-8')
                except UnicodeDecodeError:
                    content_text = file_bytes.decode('gbk', errors='ignore')
            
            elif filename.endswith(('.xls', '.xlsx')):
                source_type = "Excelè´¦å•"
                try:
                    xls = pd.read_excel(file_stream, sheet_name=None)
                    text_parts = []
                    for sheet_name, df in xls.items():
                        text_parts.append(f"--- Sheet: {sheet_name} ---\n")
                        text_parts.append(df.to_csv(index=False))
                    content_text = "\n".join(text_parts)
                except Exception as e:
                    return None, f"Excel è¯»å–å¤±è´¥: {e}", debug_info

            elif filename.endswith('.pdf'):
                source_type = "PDFè´¦å•"
                try:
                    with fitz.open(stream=file_bytes, filetype="pdf") as doc:
                        text_parts = [page.get_text() for page in doc]
                        content_text = "\n".join(text_parts)
                except Exception as e:
                    return None, f"PDF è¯»å–å¤±è´¥: {e}", debug_info
            else:
                return None, "ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼", {}

            debug_info['read_time'] = time.time() - t_read_start
            debug_info['text_length'] = len(content_text)

            if not content_text.strip():
                return None, "æ— æ³•æå–æ–‡æœ¬å†…å®¹", debug_info
            
            # 2. AI è§£æ
            res_df, err, ai_debug = BillParser._call_ai_parser(content_text, source_type, api_key)
            debug_info.update(ai_debug)
            
            debug_info['total_time'] = time.time() - t_start
            return res_df, err, debug_info

        except Exception as e:
            return None, f"æ–‡ä»¶è§£æé”™è¯¯: {str(e)}", debug_info

    @staticmethod
    def _call_ai_parser(content_text, source_type, api_key):
        debug_info = {}
        t_ai_start = time.time()
        
        # å¼ºåŒ– Prompt
        system_prompt = """
        ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„è´¢åŠ¡æ•°æ®æå–ä¸“å®¶ã€‚è¯·ä»æ–‡æœ¬ä¸­æå–äº¤æ˜“æµæ°´ã€‚
        æ ¸å¿ƒåŸåˆ™ï¼šå®ç¼ºæ¯‹å‡ã€‚ç»å¯¹ç¦æ­¢æé€ ã€æ¨¡æ‹Ÿæˆ–æ¨æµ‹æ•°æ®ã€‚åªæå–æ–‡æœ¬ä¸­æ˜ç¡®å­˜åœ¨çš„äº¤æ˜“ã€‚
        
        è§„åˆ™ï¼š
        1. è¿”å›æ ‡å‡† JSON æ•°ç»„ `[{"date": "YYYY-MM-DD", "type": "æ”¯å‡º/æ”¶å…¥", "amount": 10.5, "merchant": "å•†æˆ·å", "category": "åˆ†ç±»"}, ...]`
        2. category ä»ä»¥ä¸‹é€‰å–ï¼š[é¤é¥®, äº¤é€š, è´­ç‰©, å±…ä½, å¨±ä¹, å·¥èµ„, ç†è´¢, åŒ»ç–—, å…¶ä»–]ã€‚
        3. ä»…æå–çœŸå®äº¤æ˜“ï¼Œå¿½ç•¥ä½™é¢ã€è¡¨å¤´ã€‚
        4. amount å¿…é¡»ä¸ºæ­£æ•° (float)ã€‚
        5. å¦‚æœæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆäº¤æ˜“ï¼Œå¿…é¡»è¿”å› []ï¼Œä¸è¦ç¼–é€ ã€‚
        """

        user_prompt = f"è¯·å¤„ç†è¿™ä»½ {source_type}ï¼Œå½“å‰å¹´ä»½é»˜è®¤ä¸º {datetime.datetime.now().year}ã€‚\næ•°æ®å†…å®¹å¦‚ä¸‹:\n{content_text}"

        client = get_llm_client(api_key)
        try:
            response = client.chat.completions.create(
                model=TEXT_MODEL_NAME,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=8192,
                temperature=0.0
            )
            debug_info['ai_latency'] = time.time() - t_ai_start
            
            t_parse_start = time.time()
            ai_content = response.choices[0].message.content
            data_list = extract_json_from_text(ai_content)
            debug_info['parse_time'] = time.time() - t_parse_start

            if data_list is None or not isinstance(data_list, list):
                return None, f"AI è¿”å›æ ¼å¼æ— æ³•è§£æ: {ai_content[:100]}...", debug_info
            
            if not data_list:
                return None, "æœªæå–åˆ°æœ‰æ•ˆæ•°æ®", debug_info

            df = pd.DataFrame(data_list)
            
            # æ ‡å‡†åŒ–åˆ—å
            col_map = {"date": "æ—¥æœŸ", "type": "ç±»å‹", "amount": "é‡‘é¢", "merchant": "å¤‡æ³¨", "category": "åˆ†ç±»"}
            df = df.rename(columns=col_map)
            
            # è¡¥å…¨ç¼ºå¤±åˆ—
            for col in col_map.values():
                if col not in df.columns:
                    df[col] = ""

            # æ•°æ®æ¸…æ´—
            df['é‡‘é¢'] = pd.to_numeric(df['é‡‘é¢'], errors='coerce').fillna(0)
            df['åˆ†ç±»'] = df['åˆ†ç±»'].fillna("å…¶ä»–")
            # ç®€å•æ—¥æœŸæ¸…æ´—
            df['æ—¥æœŸ'] = df['æ—¥æœŸ'].apply(lambda x: str(x).split(' ')[0])

            return df, None, debug_info

        except Exception as e:
            return None, f"AI è¯·æ±‚å¤±è´¥: {str(e)}", debug_info

    @staticmethod
    def merge_and_deduplicate(old_df, new_df):
        """åˆå¹¶å¹¶å»é‡"""
        if new_df is None or new_df.empty:
            return old_df, 0, 0

        # æ„é€ æŒ‡çº¹åˆ—
        def make_fingerprint(df):
            return df['æ—¥æœŸ'].astype(str) + "_" + \
                   df['é‡‘é¢'].astype(float).round(2).astype(str) + "_" + \
                   df['ç±»å‹'] + "_" + \
                   df['å¤‡æ³¨'].str.slice(0, 5)

        if old_df.empty:
            return new_df, len(new_df), 0

        old_df['_fp'] = make_fingerprint(old_df)
        new_df['_fp'] = make_fingerprint(new_df)
        
        existing_fps = set(old_df['_fp'].tolist())
        
        # ç­›é€‰æ–°è¡Œ
        to_add = new_df[~new_df['_fp'].isin(existing_fps)].copy()
        skipped_count = len(new_df) - len(to_add)
        
        # æ¸…ç†ä¸´æ—¶åˆ—
        if '_fp' in old_df.columns: del old_df['_fp']
        if '_fp' in to_add.columns: del to_add['_fp']
        
        final_df = pd.concat([old_df, to_add], ignore_index=True)
        # æŒ‰æ—¥æœŸé™åºæ’åº
        final_df = final_df.sort_values(by="æ—¥æœŸ", ascending=False).reset_index(drop=True)
        
        return final_df, len(to_add), skipped_count

# --- å›¾åƒå¤„ç† ---
def process_bill_image(filename, image_bytes, api_key):
    """
    å¤„ç†å•ä¸ªå›¾ç‰‡
    åŒæ ·ä¸å†æ¥æ”¶ UploadedFileï¼Œè€Œæ˜¯æ¥æ”¶ (filename, image_bytes)
    """
    if not api_key: return None, "æœªé…ç½® API Key", {}
    
    t_start = time.time()
    debug_info = {}
    
    try:
        base64_image = base64.b64encode(image_bytes).decode('utf-8')
        
        client = get_llm_client(api_key)
        prompt = "æå–è´¦å•ä¿¡æ¯ã€‚è¿”å›JSON: {date: 'YYYY-MM-DD', amount: float, merchant: string, category: string, type: 'æ”¯å‡º'|'æ”¶å…¥'}ã€‚"

        t_ai_start = time.time()
        response = client.chat.completions.create(
            model=VISION_MODEL_NAME,
            messages=[{
                "role": "user",
                "content": [
                    {"type": "text", "text": prompt},
                    {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{base64_image}"}}
                ]
            }],
            max_tokens=2048
        )
        debug_info['ai_latency'] = time.time() - t_ai_start
        
        content = response.choices[0].message.content
        data = extract_json_from_text(content)
        
        debug_info['total_time'] = time.time() - t_start
        
        if isinstance(data, list): data = data[0]
        if not data: return None, "æ— æ³•è¯†åˆ«å›¾ç‰‡å†…å®¹", debug_info
        
        return data, None, debug_info
    except Exception as e:
        return None, f"è§†è§‰è¯†åˆ«é”™è¯¯: {e}", debug_info

# --- ä¸»ç¨‹åº ---
def main():
    # 1. ä¾§è¾¹æ é…ç½®
    st.sidebar.title("âš™ï¸ è´¢åŠ¡è®¾ç½®")
    
    # --- è°ƒè¯•æ¨¡å¼å¼€å…³ ---
    st.session_state.debug_mode = st.sidebar.checkbox("ğŸ› ï¸ å¼€å¯è°ƒè¯•æ¨¡å¼", value=False)
    
    sf_api_key = st.secrets.get("SILICONFLOW_API_KEY", "")
    if not sf_api_key:
        sf_api_key = st.sidebar.text_input("SiliconFlow API Key", type="password")

    github_token = st.secrets.get("GITHUB_TOKEN", "")
    github_repo = st.secrets.get("GITHUB_REPO", "")

    dm = DataManager(github_token, github_repo)
    
    if dm.use_github:
        st.sidebar.success(f"â˜ï¸ å·²è¿æ¥ GitHub: {dm.repo}")
    else:
        st.sidebar.info("ğŸ“‚ ä½¿ç”¨æœ¬åœ°å­˜å‚¨ (åˆ·æ–°é¡µé¢åæ•°æ®å¯èƒ½ä¸¢å¤±)")

    payday = st.sidebar.number_input("ğŸ“… æ¯æœˆå‘è–ªæ—¥", 1, 31, 10)
    current_cash = st.sidebar.number_input("ğŸ’³ å½“å‰èµ„äº§ä½™é¢", value=3000.0)

    # 2. åˆå§‹åŒ– Session State
    if 'ledger_data' not in st.session_state:
        # è¿™é‡Œåªä¼šåœ¨ç¬¬ä¸€æ¬¡åŠ è½½æ—¶è°ƒç”¨ load_dataï¼Œæˆ–è€…ç¼“å­˜å¤±æ•ˆæ—¶
        with st.spinner("æ­£åœ¨åŠ è½½è´¦æœ¬æ•°æ®..."):
            df, sha = dm.load_data()
            st.session_state.ledger_data = df
            st.session_state.github_sha = sha

    # 3. é¡¶éƒ¨æ¦‚è§ˆ
    st.title("ğŸ’° AI æ™ºèƒ½è´¦æœ¬ Pro")
    
    today = date.today()
    if today.day >= payday:
        target_date = date(today.year + (1 if today.month == 12 else 0), 1 if today.month == 12 else today.month + 1, payday)
    else:
        target_date = date(today.year, today.month, payday)
    days_left = (target_date - today).days

    current_month_str = today.strftime("%Y-%m")
    df_current = st.session_state.ledger_data.copy()
    if not df_current.empty:
        df_current['tmp_date'] = pd.to_datetime(df_current['æ—¥æœŸ'], errors='coerce')
        mask = (df_current['tmp_date'].dt.strftime('%Y-%m') == current_month_str) & (df_current['ç±»å‹'] == 'æ”¯å‡º')
        month_spend = df_current.loc[mask, 'é‡‘é¢'].sum()
    else:
        month_spend = 0.0

    col1, col2, col3, col4 = st.columns(4)
    col1.metric("å½“å‰èµ„äº§", f"Â¥{current_cash:,.2f}")
    col2.metric("æœ¬æœˆå·²æ”¯å‡º", f"Â¥{month_spend:,.2f}")
    col3.metric("è·ç¦»å‘è–ª", f"{days_left} å¤©")
    
    if days_left > 0:
        daily_budget = current_cash / days_left
        col4.metric("æ¯æ—¥å¯ç”¨é¢„ç®—", f"Â¥{daily_budget:.0f}", 
                    delta=f"{daily_budget - DEFAULT_TARGET_SPEND:.0f}", 
                    delta_color="normal")

    st.divider()

    # 4. æ ¸å¿ƒåŠŸèƒ½åŒº
    tab_import, tab_manual, tab_analysis = st.tabs(["ğŸ“¥ æ™ºèƒ½å¯¼å…¥", "âœï¸ æ‰‹åŠ¨è®°è´¦", "ğŸ“Š æŠ¥è¡¨ä¸AI"])

    with tab_import:
        uploaded_files = st.file_uploader("ä¸Šä¼ è´¦å•æ–‡ä»¶ (PDF/Excel/CSV) æˆ– ç¥¨æ®å›¾ç‰‡", 
                                        accept_multiple_files=True,
                                        type=['png', 'jpg', 'csv', 'xlsx', 'pdf'])
        
        if uploaded_files:
            if st.button("ğŸš€ å¼€å§‹ AI è¯†åˆ«", type="primary"):
                if not sf_api_key:
                    st.error("è¯·å…ˆå¡«å†™ API Key")
                    st.stop()

                doc_files = [f for f in uploaded_files if f.name.split('.')[-1].lower() in ['csv', 'xlsx', 'xls', 'pdf']]
                img_files = [f for f in uploaded_files if f.name.split('.')[-1].lower() in ['png', 'jpg', 'jpeg']]

                batch_new_data = pd.DataFrame()
                
                # A. å¤„ç†æ–‡æ¡£ - æé«˜å¹¶å‘
                if doc_files:
                    st.caption(f"ğŸ“„ æ­£åœ¨å¹¶å‘åˆ†æ {len(doc_files)} ä¸ªæ–‡æ¡£...")
                    progress_bar = st.progress(0)
                    
                    # å…³é”®ä¿®æ”¹ï¼šåœ¨ä¸»çº¿ç¨‹è¯»å–æ–‡ä»¶å†…å®¹ï¼Œåªä¼ é€’çº¯æ•°æ®ç»™å­çº¿ç¨‹
                    # è¿™å½»åº•è§£å†³äº† ThreadPoolExecutor ä¸­çš„ Streamlit ä¸Šä¸‹æ–‡ä¸¢å¤±é—®é¢˜
                    doc_tasks = []
                    for f in doc_files:
                        doc_tasks.append({
                            "file_obj": f,             # ä»…ç”¨äºUIæ˜¾ç¤ºåå­—
                            "filename": f.name,        # çº¯å­—ç¬¦ä¸²
                            "bytes": f.getvalue()      # çº¯äºŒè¿›åˆ¶æ•°æ®
                        })

                    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
                        # æäº¤ä»»åŠ¡æ—¶ï¼Œåªä¼  name å’Œ bytes
                        future_map = {
                            executor.submit(BillParser.identify_and_parse, task["filename"], task["bytes"], sf_api_key): task["file_obj"] 
                            for task in doc_tasks
                        }
                        
                        for i, future in enumerate(concurrent.futures.as_completed(future_map)):
                            f_obj = future_map[future]
                            # è·å– debug_info
                            res, err, dbg = future.result()
                            
                            if st.session_state.debug_mode:
                                with st.expander(f"ğŸ”§ è°ƒè¯•: {f_obj.name}", expanded=True): # å±•å¼€æ–¹ä¾¿æŸ¥çœ‹
                                    st.json(dbg)
                            
                            if res is not None and not res.empty:
                                batch_new_data = pd.concat([batch_new_data, res], ignore_index=True)
                                st.toast(f"âœ… {f_obj.name} è§£ææˆåŠŸ")
                            else:
                                st.error(f"âŒ {f_obj.name}: {err}")
                            progress_bar.progress((i + 1) / len(doc_files))

                # B. å¤„ç†å›¾ç‰‡ - å¹¶è¡ŒåŒ–
                if img_files:
                    st.caption(f"ğŸ–¼ï¸ æ­£åœ¨å¹¶å‘è¯†åˆ« {len(img_files)} å¼ å›¾ç‰‡...")
                    img_progress = st.progress(0)
                    
                    # å…³é”®ä¿®æ”¹ï¼šå›¾ç‰‡ä¹Ÿä¸€æ ·ï¼Œä¸»çº¿ç¨‹è¯»å–
                    img_tasks = []
                    for img in img_files:
                        img_tasks.append({
                            "file_obj": img,
                            "filename": img.name,
                            "bytes": img.getvalue()
                        })

                    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
                        future_map = {
                            executor.submit(process_bill_image, task["filename"], task["bytes"], sf_api_key): task["file_obj"]
                            for task in img_tasks
                        }
                        
                        for i, future in enumerate(concurrent.futures.as_completed(future_map)):
                            img_obj = future_map[future]
                            res, err, dbg = future.result()
                            
                            if st.session_state.debug_mode:
                                with st.expander(f"ğŸ”§ è°ƒè¯•: {img_obj.name}", expanded=True):
                                    st.json(dbg)

                            if res:
                                row = {
                                    "æ—¥æœŸ": res.get('date', str(date.today())),
                                    "ç±»å‹": res.get('type', 'æ”¯å‡º'),
                                    "é‡‘é¢": res.get('amount', 0),
                                    "åˆ†ç±»": res.get('category', 'å…¶ä»–'),
                                    "å¤‡æ³¨": res.get('merchant', 'å›¾ç‰‡è¯†åˆ«')
                                }
                                batch_new_data = pd.concat([batch_new_data, pd.DataFrame([row])], ignore_index=True)
                                st.toast(f"âœ… {img_obj.name} è¯†åˆ«æˆåŠŸ")
                            else:
                                st.error(f"âŒ {img_obj.name}: {err}")
                            img_progress.progress((i + 1) / len(img_files))

                # C. åˆå¹¶å…¥åº“
                if not batch_new_data.empty:
                    merged, added, skipped = BillParser.merge_and_deduplicate(st.session_state.ledger_data, batch_new_data)
                    if added > 0:
                        if dm.save_data(merged, st.session_state.get('github_sha')):
                            st.session_state.ledger_data = merged
                            st.session_state.github_sha = dm.load_data()[1] # æ›´æ–° sha
                            st.balloons()
                            st.success(f"æˆåŠŸå¯¼å…¥ {added} æ¡è®°å½• (è‡ªåŠ¨å»é‡ {skipped} æ¡)")
                    else:
                        st.warning(f"æ‰€æœ‰è®°å½•å‡å·²å­˜åœ¨ (å»é‡ {skipped} æ¡)")
                else:
                    st.warning("æœªèƒ½æå–åˆ°æœ‰æ•ˆæ•°æ®")

    with tab_manual:
        with st.form("add_transaction"):
            c1, c2, c3 = st.columns(3)
            new_date = c1.date_input("æ—¥æœŸ", value=date.today())
            new_type = c2.selectbox("ç±»å‹", ["æ”¯å‡º", "æ”¶å…¥"])
            new_amt = c3.number_input("é‡‘é¢", min_value=0.01, step=1.0)
            
            c4, c5 = st.columns([1, 2])
            new_cat = c4.selectbox("åˆ†ç±»", ["é¤é¥®", "äº¤é€š", "è´­ç‰©", "å±…ä½", "å¨±ä¹", "åŒ»ç–—", "å·¥èµ„", "å…¶ä»–"])
            new_desc = c5.text_input("å¤‡æ³¨/å•†æˆ·")

            if st.form_submit_button("â• æ·»åŠ è®°å½•", use_container_width=True):
                new_row = pd.DataFrame([{
                    "æ—¥æœŸ": str(new_date), "ç±»å‹": new_type, "é‡‘é¢": new_amt,
                    "åˆ†ç±»": new_cat, "å¤‡æ³¨": new_desc
                }])
                merged, _, _ = BillParser.merge_and_deduplicate(st.session_state.ledger_data, new_row)
                if dm.save_data(merged, st.session_state.get('github_sha')):
                    st.session_state.ledger_data = merged
                    st.session_state.github_sha = dm.load_data()[1]
                    st.success("æ·»åŠ æˆåŠŸï¼")
                    st.rerun()

    with tab_analysis:
        if st.session_state.ledger_data.empty:
            st.info("æš‚æ— æ•°æ®ï¼Œè¯·å…ˆè®°è´¦ã€‚")
        else:
            st.subheader("ğŸ“ è´¦å•æ˜ç»†")
            
            edited_df = st.data_editor(
                st.session_state.ledger_data,
                use_container_width=True,
                num_rows="dynamic",
                column_config={
                    "æ—¥æœŸ": st.column_config.DateColumn("æ—¥æœŸ", format="YYYY-MM-DD"),
                    "é‡‘é¢": st.column_config.NumberColumn("é‡‘é¢", format="Â¥%.2f"),
                    "ç±»å‹": st.column_config.SelectboxColumn("ç±»å‹", options=["æ”¯å‡º", "æ”¶å…¥"]),
                    "åˆ†ç±»": st.column_config.SelectboxColumn("åˆ†ç±»", options=["é¤é¥®", "äº¤é€š", "è´­ç‰©", "å±…ä½", "å¨±ä¹", "åŒ»ç–—", "å·¥èµ„", "å…¶ä»–"]),
                },
                key="data_editor"
            )

            if st.button("ğŸ’¾ ä¿å­˜è¡¨æ ¼ä¿®æ”¹"):
                if not edited_df.equals(st.session_state.ledger_data):
                     if dm.save_data(edited_df, st.session_state.get('github_sha')):
                        st.session_state.ledger_data = edited_df
                        st.session_state.github_sha = dm.load_data()[1]
                        st.success("ä¿®æ”¹å·²åŒæ­¥è‡³äº‘ç«¯")
                else:
                    st.info("æ•°æ®æœªå‘ç”Ÿå˜åŒ–")

            st.divider()
            
            c_chart1, c_chart2 = st.columns(2)
            
            df_chart = st.session_state.ledger_data.copy()
            df_chart['é‡‘é¢'] = pd.to_numeric(df_chart['é‡‘é¢'])
            df_exp = df_chart[df_chart['ç±»å‹'] == 'æ”¯å‡º']
            
            with c_chart1:
                st.subheader("æ”¯å‡ºæ„æˆ")
                if not df_exp.empty:
                    pie_data = df_exp.groupby("åˆ†ç±»")['é‡‘é¢'].sum().reset_index()
                    st.bar_chart(pie_data, x="åˆ†ç±»", y="é‡‘é¢", color="åˆ†ç±»")
            
            with c_chart2:
                st.subheader("AI æ´å¯Ÿ")
                if sf_api_key:
                    if st.button("ğŸ¤– ç”Ÿæˆæœˆåº¦åˆ†ææŠ¥å‘Š"):
                        with st.spinner("AI æ­£åœ¨åˆ†ææ‚¨çš„æ¶ˆè´¹ä¹ æƒ¯..."):
                            summary_text = df_exp.to_csv(index=False)
                            client = get_llm_client(sf_api_key)
                            try:
                                res = client.chat.completions.create(
                                    model=TEXT_MODEL_NAME,
                                    messages=[
                                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸¥å‰ä½†å¹½é»˜çš„ç†è´¢é¡¾é—®ã€‚æ ¹æ®ç”¨æˆ·çš„æ”¯å‡ºæ•°æ®ï¼Œç®€çŸ­ç‚¹è¯„å…¶æ¶ˆè´¹ä¹ æƒ¯ï¼Œå¹¶ç»™å‡º3æ¡çœé’±å»ºè®®ã€‚"},
                                        {"role": "user", "content": f"æˆ‘çš„æ”¯å‡ºæ•°æ®:\n{summary_text}"}
                                    ],
                                    max_tokens=4096
                                )
                                st.markdown(res.choices[0].message.content)
                            except Exception as e:
                                st.error(str(e))
                else:
                    st.caption("é…ç½® API Key åè§£é” AI åˆ†æ")

if __name__ == "__main__":
    main()
