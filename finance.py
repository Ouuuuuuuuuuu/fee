import streamlit as st
import pandas as pd
import datetime
from datetime import date
import requests
import json
import base64
from io import StringIO, BytesIO
import os
import fitz  # PyMuPDF
import re
from openai import OpenAI
import concurrent.futures
import time
import math

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="AI æ™ºèƒ½è´¦æœ¬ Pro (å¤§æ–‡ä»¶ç‰ˆ)", page_icon="ğŸ’°", layout="wide")

# --- å¸¸é‡é…ç½® ---
DEFAULT_TARGET_SPEND = 60.0  # æ¯æ—¥ä½“é¢æ”¯å‡ºæ ‡å‡†
GITHUB_API_URL = "https://api.github.com"
VISION_MODEL_NAME = "Qwen/Qwen2.5-VL-72B-Instruct"  # å‡è®¾ä½¿ç”¨çš„è§†è§‰æ¨¡å‹
TEXT_MODEL_NAME = "deepseek-ai/DeepSeek-V3" # æ–‡æœ¬æ¨¡å‹
CHUNK_SIZE = 8000  # æ™ºèƒ½åˆ†ç‰‡é˜ˆå€¼ (å­—ç¬¦æ•°)ï¼Œé…åˆ LLM çš„ Context Window

# --- æ ¸å¿ƒå·¥å…·ï¼šOpenAI Client ---
def get_llm_client(api_key):
    # è¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹ base_urlï¼Œè¿™é‡Œé»˜è®¤ä½¿ç”¨ SiliconFlow æˆ–ç±»ä¼¼çš„å…¼å®¹æ¥å£
    return OpenAI(api_key=api_key, base_url="https://api.siliconflow.cn/v1")

# --- å·¥å…·å‡½æ•°ï¼šå¢å¼ºç‰ˆ JSON æå–ä¸æˆªæ–­ä¿®å¤ ---
def extract_json_from_text(text):
    """
    è¶…å¼ºå®¹é”™ JSON æå–å™¨ï¼š
    1. æå– markdown ä»£ç å—
    2. å¤„ç† JSON æˆªæ–­ï¼ˆç¼ºå°‘ ] çš„æƒ…å†µï¼‰
    3. æ¸…ç†æ³¨é‡Š
    è¿”å›: (data_list, error_msg)
    """
    if not text: 
        return None, "ç©ºå“åº”"
    
    # ä¿å­˜åŸå§‹æ–‡æœ¬ç”¨äºè°ƒè¯•
    original_preview = text[:200].replace('\n', '\\n')
    
    # 1. é¢„å¤„ç†ï¼šå°è¯•æå– Markdown ä»£ç å—
    try:
        text = text.strip()
        code_block_pattern = r"match_code = re.search(code_block_pattern, text, re.DOTALL)
        if match_code:
            text = match_code.group(1).strip()
        else:
            # å…œåº•ï¼šç§»é™¤å¯èƒ½çš„ markdown æ ‡è®°
            text = re.sub(r'```json\s*', '', text)
            text = re.sub(r'```\s*', '', text)
            text = text.strip()
            
        # 2. å¿«é€Ÿåˆ¤æ–­ç©ºæ•°ç»„
        if text == '[]':
            return [], None

        # 3. æ ¸å¿ƒä¿®å¤ï¼šå¤„ç†æˆªæ–­çš„ JSON
        # å¦‚æœç»“å°¾ä¸æ˜¯ ]ï¼Œå°è¯•å¯»æ‰¾æœ€åä¸€ä¸ªé—­åˆçš„å¤§æ‹¬å· } å¹¶è¡¥å…¨ ]
        if not text.endswith(']'):
            last_brace_index = text.rfind('}')
            if last_brace_index != -1:
                # æˆªå–åˆ°æœ€åä¸€ä¸ªå®Œæ•´å¯¹è±¡ï¼Œå¹¶è¡¥å…¨æ•°ç»„ç»“æŸç¬¦
                text = text[:last_brace_index+1] + ']'
            else:
                # è¿ä¸€ä¸ªå®Œæ•´å¯¹è±¡éƒ½æ²¡æœ‰
                return None, "æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSONå¯¹è±¡ç»“å°¾"

        # 4. å°è¯•å®šä½æ•°ç»„è¾¹ç•Œ (å¤„ç† AI å›å¤ä¸­åŒ…å«å‰åæ–‡çš„æƒ…å†µ)
        match_array = re.search(r'\[.*\]', text, re.DOTALL)
        if match_array:
            text_to_parse = match_array.group()
        else:
            text_to_parse = text
            
        # 5. æ¸…ç†å¸¸è§çš„ JS æ³¨é‡Š (// æˆ– /* */) é˜²æ­¢ json.loads å¤±è´¥
        text_to_parse = re.sub(r'//.*?\n', '\n', text_to_parse)
        text_to_parse = re.sub(r'/\*.*?\*/', '', text_to_parse, flags=re.DOTALL)
        
        # 6. æ­£å¼è§£æ
        result = json.loads(text_to_parse)
        
        if isinstance(result, (list, dict)):
            return result if isinstance(result, list) else [result], None
            
    except json.JSONDecodeError as e:
        return None, f"JSONè§£æå¤±è´¥ (ä½ç½® {e.pos}): {original_preview}...", None
    except Exception as e:
        return None, f"æœªçŸ¥è§£æé”™è¯¯: {str(e)}", None
    
    return None, f"æ— æ³•æå–æœ‰æ•ˆJSONæ•°æ®", None

# --- æ•°æ®ç®¡ç†ç±» ---
class DataManager:
    def __init__(self, github_token=None, repo=None, filename="ledger.csv"):
        self.github_token = github_token
        if repo and repo.startswith("http"):
            self.repo = repo.rstrip("/").split("github.com/")[-1]
        else:
            self.repo = repo
        self.filename = filename
        self.use_github = bool(github_token and self.repo)

    def load_data(self, force_refresh=False):
        if self.use_github:
            if force_refresh:
                self._fetch_github_content.clear()
            df, sha = self._load_from_github()
        else:
            df, sha = self._load_from_local()
        df = self._clean_df_types(df)
        return df, sha

    def save_data(self, df, sha=None):
        save_df = df.copy()
        if 'æ—¥æœŸ' in save_df.columns:
            save_df['æ—¥æœŸ'] = save_df['æ—¥æœŸ'].astype(str)
            
        if self.use_github:
            success, new_sha = self._save_to_github(save_df, sha)
            return success, new_sha
        else:
            return self._save_to_local(save_df), None

    @staticmethod
    def _clean_df_types(df):
        expected_cols = ["æ—¥æœŸ", "ç±»å‹", "é‡‘é¢", "å¤‡æ³¨", "åˆ†ç±»"]
        for col in expected_cols:
            if col not in df.columns:
                df[col] = ""
        df['é‡‘é¢'] = pd.to_numeric(df['é‡‘é¢'], errors='coerce').fillna(0.0)
        df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'], errors='coerce')
        df['æ—¥æœŸ'] = df['æ—¥æœŸ'].fillna(pd.Timestamp(date.today()))
        df['æ—¥æœŸ'] = df['æ—¥æœŸ'].dt.date
        df['ç±»å‹'] = df['ç±»å‹'].astype(str).replace('nan', 'æ”¯å‡º')
        df['åˆ†ç±»'] = df['åˆ†ç±»'].astype(str).replace('nan', 'å…¶ä»–')
        df['å¤‡æ³¨'] = df['å¤‡æ³¨'].astype(str).replace('nan', '')
        return df

    def _load_from_local(self):
        if os.path.exists(self.filename):
            try:
                return pd.read_csv(self.filename), None
            except:
                pass
        return self._create_empty_df(), None

    def _save_to_local(self, df):
        df.to_csv(self.filename, index=False)
        return True

    @st.cache_data(ttl=300, show_spinner=False)
    def _fetch_github_content(_self):
        headers = {
            "Authorization": f"token {_self.github_token}",
            "Accept": "application/vnd.github.v3+json"
        }
        url = f"{GITHUB_API_URL}/repos/{_self.repo}/contents/{_self.filename}"
        try:
            response = requests.get(url, headers=headers, timeout=30)
            if response.status_code == 200:
                return response.json(), None
            elif response.status_code == 404:
                return None, 404
            else:
                return None, response.status_code
        except Exception as e:
            return None, str(e)

    def _load_from_github(self):
        content, error = self._fetch_github_content()
        if content:
            try:
                csv_str = base64.b64decode(content['content']).decode('utf-8')
                df = pd.read_csv(StringIO(csv_str))
                return df, content['sha']
            except:
                return self._create_empty_df(), content['sha']
        return self._create_empty_df(), None

    def _save_to_github(self, df, sha):
        headers = {
            "Authorization": f"token {self.github_token}",
            "Accept": "application/vnd.github.v3+json"
        }
        url = f"{GITHUB_API_URL}/repos/{self.repo}/contents/{self.filename}"
        csv_str = df.to_csv(index=False)
        content_bytes = base64.b64encode(csv_str.encode('utf-8')).decode('utf-8')
        data = {
            "message": f"Update ledger {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            "content": content_bytes
        }
        if sha:
            data["sha"] = sha

        def do_put(payload):
            return requests.put(url, headers=headers, data=json.dumps(payload), timeout=30)

        try:
            resp = do_put(data)
            if resp.status_code in [200, 201]:
                self._fetch_github_content.clear()
                return True, resp.json()['content']['sha']
            elif resp.status_code in [409, 422]:
                if st.session_state.get('debug_mode'):
                    st.warning(f"âš ï¸ SHAå†²çª ({resp.status_code})ï¼Œå°è¯•è‡ªåŠ¨ä¿®å¤...")
                self._fetch_github_content.clear()
                latest, _ = self._fetch_github_content()
                if latest and 'sha' in latest:
                    data["sha"] = latest['sha']
                    retry_resp = do_put(data)
                    if retry_resp.status_code in [200, 201]:
                        self._fetch_github_content.clear()
                        return True, retry_resp.json()['content']['sha']
                return False, None
            else:
                return False, None
        except Exception:
            return False, None

    @staticmethod
    def _create_empty_df():
        return pd.DataFrame(columns=["æ—¥æœŸ", "ç±»å‹", "é‡‘é¢", "å¤‡æ³¨", "åˆ†ç±»"])

# --- AI è§£ææ ¸å¿ƒ (å«æ™ºèƒ½åˆ†ç‰‡) ---
class BillParser:
    
    @staticmethod
    def _split_text_safe(text, chunk_size=CHUNK_SIZE):
        """
        æ™ºèƒ½åˆ†ç‰‡ï¼šæŒ‰è¡Œåˆ‡å‰²ï¼Œç¡®ä¿ä¸æ‰“æ–­æ•°æ®è¡Œã€‚
        """
        lines = text.split('\n')
        chunks = []
        current_chunk = []
        current_len = 0
        
        for line in lines:
            line_len = len(line) + 1 # +1 for newline
            if current_len + line_len > chunk_size and current_chunk:
                chunks.append("\n".join(current_chunk))
                current_chunk = []
                current_len = 0
            
            current_chunk.append(line)
            current_len += line_len
            
        if current_chunk:
            chunks.append("\n".join(current_chunk))
            
        return chunks

    @staticmethod
    def _process_single_chunk(chunk_text, chunk_index, total_chunks, source_type, api_key):
        """å¤„ç†å•ä¸ªåˆ†ç‰‡"""
        client = get_llm_client(api_key)
        # é’ˆå¯¹åˆ†ç‰‡ä¼˜åŒ–çš„ Prompt
        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„è´¢åŠ¡æ•°æ®æå–ä¸“å®¶ã€‚
        ä»»åŠ¡ï¼šä»ä»¥ä¸‹æ–‡æœ¬ç‰‡æ®µä¸­æå–äº¤æ˜“è®°å½•ã€‚
        **æ³¨æ„ï¼šè¿™æ˜¯å®Œæ•´æ–‡ä»¶çš„ç¬¬ {chunk_index + 1}/{total_chunks} ä¸ªç‰‡æ®µï¼Œæ•°æ®å¯èƒ½åœ¨å¼€å¤´æˆ–ç»“å°¾è¢«æˆªæ–­ã€‚è¯·å°½å¯èƒ½æå–å®Œæ•´çš„è®°å½•ã€‚**
        
        è¾“å…¥æ–‡æœ¬ç±»å‹ï¼š{source_type}
        å½“å‰å¹´ä»½å‚è€ƒï¼š{datetime.datetime.now().year}
        
        **å¼ºåˆ¶è¦æ±‚**ï¼š
        1. å¿…é¡»è¿”å›**çº¯JSONæ•°ç»„**ã€‚
        2. æ ¼å¼ï¼š[{{"date":"YYYY-MM-DD","type":"æ”¯å‡º/æ”¶å…¥","amount":æ•°å­—,"merchant":"å•†æˆ·/å¤‡æ³¨","category":"åˆ†ç±»"}}]
        3. å¦‚æœç‰‡æ®µå†…æ— æœ‰æ•ˆå®Œæ•´æ•°æ®ï¼Œè¿”å› []
        4. ä¸è¦åŒ…å«ä»»ä½•Markdownæ ‡è®°æˆ–è§£é‡Šæ–‡å­—ã€‚

        æ–‡æœ¬å†…å®¹ï¼š
        {chunk_text}
        """
        
        t0 = time.time()
        try:
            resp = client.chat.completions.create(
                model=TEXT_MODEL_NAME,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=4096, # é¢„ç•™è¶³å¤Ÿçš„ Output Token
                temperature=0.0
            )
            raw_json = resp.choices[0].message.content
            data, err = extract_json_from_text(raw_json)
            
            return {
                "chunk_index": chunk_index,
                "data": data,
                "raw_response": raw_json, # ç”¨äºè°ƒè¯•
                "error": err,
                "time": time.time() - t0
            }
        except Exception as e:
            return {
                "chunk_index": chunk_index,
                "data": None,
                "raw_response": str(e),
                "error": str(e),
                "time": time.time() - t0
            }

    @staticmethod
    def identify_and_parse(filename, file_bytes, api_key):
        """æ™ºèƒ½å…¥å£ï¼šæ ¹æ®æ–‡ä»¶å¤§å°å†³å®šæ˜¯å¦åˆ†ç‰‡å¹¶å‘"""
        t_start = time.time()
        debug_log = {"file": filename, "steps": [], "chunks_info": []}
        
        try:
            # 1. è¯»å–å†…å®¹
            content_text = ""
            source_type = "æœªçŸ¥"
            file_stream = BytesIO(file_bytes)
            
            if filename.endswith('.csv'):
                source_type = "CSV"
                try: content_text = file_bytes.decode('utf-8')
                except: content_text = file_bytes.decode('gbk', errors='ignore')
            elif filename.endswith(('.xls', '.xlsx')):
                source_type = "Excel"
                xls = pd.read_excel(file_stream, sheet_name=None)
                parts = []
                for sname, sdf in xls.items():
                    parts.append(f"Sheet: {sname}\n{sdf.to_csv(index=False)}")
                content_text = "\n".join(parts)
            elif filename.endswith('.pdf'):
                source_type = "PDF"
                with fitz.open(stream=file_bytes, filetype="pdf") as doc:
                    content_text = "\n".join([p.get_text() for p in doc])
            
            total_chars = len(content_text)
            debug_log["steps"].append(f"è¯»å–å®Œæˆï¼Œæ€»å­—ç¬¦æ•°: {total_chars}")
            
            if not content_text.strip():
                return None, "å†…å®¹ä¸ºç©º", debug_log

            # 2. æ™ºèƒ½åˆ†ç‰‡ç­–ç•¥
            chunks = BillParser._split_text_safe(content_text, CHUNK_SIZE)
            total_chunks = len(chunks)
            debug_log["steps"].append(f"æ™ºèƒ½åˆ†ç‰‡: å…± {total_chunks} ä¸ªç‰‡æ®µ")

            all_data = []
            
            # 3. å¹¶å‘å¤„ç†åˆ†ç‰‡
            # é™åˆ¶å¹¶å‘æ•°é˜²æ­¢ API Rate Limit
            max_workers = min(5, total_chunks) 
            with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
                futures = [
                    executor.submit(BillParser._process_single_chunk, chunk, i, total_chunks, source_type, api_key)
                    for i, chunk in enumerate(chunks)
                ]
                
                for future in concurrent.futures.as_completed(futures):
                    res = future.result()
                    # è®°å½•è°ƒè¯•ä¿¡æ¯
                    chunk_log = {
                        "chunk": res['chunk_index'],
                        "status": "Success" if res['data'] is not None else "Failed",
                        "items_count": len(res['data']) if res['data'] else 0,
                        "error": res['error'],
                        "time": f"{res['time']:.2f}s",
                        "response_preview": res['raw_response'][:100] + "..." if res['raw_response'] else ""
                    }
                    # å¦‚æœå¼€å¯æ·±å±‚è°ƒè¯•ï¼Œä¿å­˜å®Œæ•´å“åº”
                    if st.session_state.get('debug_mode', False):
                        chunk_log["full_response"] = res['raw_response']
                        
                    debug_log["chunks_info"].append(chunk_log)
                    
                    if res['data']:
                        all_data.extend(res['data'])

            # 4. åˆå¹¶ä¸æ¸…æ´—
            if not all_data:
                return None, "æ‰€æœ‰åˆ†ç‰‡å‡æœªæå–åˆ°æœ‰æ•ˆæ•°æ®", debug_log
                
            df = pd.DataFrame(all_data)
            cols = {"date": "æ—¥æœŸ", "type": "ç±»å‹", "amount": "é‡‘é¢", "merchant": "å¤‡æ³¨", "category": "åˆ†ç±»"}
            df = df.rename(columns=cols)
            for c in cols.values():
                if c not in df.columns: df[c] = ""
            
            # åŸºç¡€æ¸…æ´—
            df['é‡‘é¢'] = pd.to_numeric(df['é‡‘é¢'], errors='coerce').fillna(0)
            df['æ—¥æœŸ'] = df['æ—¥æœŸ'].astype(str).apply(lambda x: x.split(' ')[0])
            
            debug_log["total_time"] = time.time() - t_start
            return df, None, debug_log

        except Exception as e:
            return None, str(e), debug_log

    @staticmethod
    def process_image(filename, image_bytes, api_key):
        """å›¾ç‰‡å¤„ç† (ä¿æŒåŸé€»è¾‘ï¼Œå›¾ç‰‡ä¸€èˆ¬ä¸åˆ‡åˆ†)"""
        t_start = time.time()
        debug_log = {"file": filename, "steps": [], "type": "Image"}
        
        try:
            b64_img = base64.b64encode(image_bytes).decode('utf-8')
            client = get_llm_client(api_key)
            
            resp = client.chat.completions.create(
                model=VISION_MODEL_NAME,
                messages=[{
                    "role": "user", 
                    "content": [
                        {"type": "text", "text": "æå–è´¦å•æ˜ç»†ã€‚è¿”å›çº¯JSONæ•°ç»„ï¼š[{date, type, amount, merchant, category}]"},
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64_img}"}}
                    ]
                }],
                max_tokens=2048
            )
            raw_json = resp.choices[0].message.content
            if st.session_state.get('debug_mode'):
                debug_log["full_response"] = raw_json
                
            data, parse_err = extract_json_from_text(raw_json)
            
            if parse_err: return None, parse_err, debug_log
            if isinstance(data, dict): data = [data]
            if not data: return None, "æ— æ•°æ®", debug_log
            
            df = pd.DataFrame(data)
            cols = {"date": "æ—¥æœŸ", "type": "ç±»å‹", "amount": "é‡‘é¢", "merchant": "å¤‡æ³¨", "category": "åˆ†ç±»"}
            df = df.rename(columns=cols)
            for c in cols.values(): 
                if c not in df.columns: df[c] = ""
            
            return df, None, debug_log
        except Exception as e:
            return None, str(e), debug_log

    @staticmethod
    def merge_data(old_df, new_df):
        if new_df is None or new_df.empty: return old_df, 0
        def get_fp(d): return d['æ—¥æœŸ'].astype(str) + d['é‡‘é¢'].astype(str) + d['å¤‡æ³¨'].str[:5]
        if old_df.empty: return new_df, len(new_df)
        old_fp = set(get_fp(old_df))
        new_df['_fp'] = get_fp(new_df)
        to_add = new_df[~new_df['_fp'].isin(old_fp)].drop(columns=['_fp'])
        if to_add.empty: return old_df, 0
        merged = pd.concat([old_df, to_add], ignore_index=True)
        merged = DataManager._clean_df_types(merged)
        merged = merged.sort_values('æ—¥æœŸ', ascending=False).reset_index(drop=True)
        return merged, len(to_add)

# --- ä¸»ç¨‹åº UI ---
def main():
    if 'debug_mode' not in st.session_state: st.session_state.debug_mode = False
    
    st.sidebar.title("âš™ï¸ è®¾ç½®")
    st.session_state.debug_mode = st.sidebar.checkbox("ğŸ æ·±åº¦è°ƒè¯•æ¨¡å¼", value=st.session_state.debug_mode, help="æ˜¾ç¤ºåˆ†ç‰‡è§£æè¯¦æƒ…å’ŒAIåŸå§‹å“åº”")
    
    api_key = st.secrets.get("SILICONFLOW_API_KEY") or st.sidebar.text_input("API Key (SiliconFlow/DeepSeek)", type="password")
    gh_token = st.secrets.get("GITHUB_TOKEN")
    gh_repo = st.secrets.get("GITHUB_REPO")
    
    dm = DataManager(gh_token, gh_repo)
    
    if dm.use_github:
        st.sidebar.success(f"å·²è¿æ¥: {dm.repo}")
        if st.sidebar.button("â˜ï¸ å¼ºåˆ¶æ‹‰å–"):
            with st.spinner("åŒæ­¥ä¸­..."):
                df, sha = dm.load_data(force_refresh=True)
                st.session_state.ledger_data = df
                st.session_state.github_sha = sha
                st.rerun()
    else:
        st.sidebar.warning("æœ¬åœ°æ¨¡å¼")

    payday = st.sidebar.number_input("å‘è–ªæ—¥", 1, 31, 10)
    current_asset = st.sidebar.number_input("å½“å‰èµ„äº§", value=3000.0)

    if 'ledger_data' not in st.session_state:
        df, sha = dm.load_data()
        st.session_state.ledger_data = df
        st.session_state.github_sha = sha

    st.title("ğŸ’° AI æ™ºèƒ½è´¦æœ¬ Pro (Max)")
    
    # é¡¶éƒ¨æŒ‡æ ‡
    df = st.session_state.ledger_data.copy()
    today = date.today()
    if not df.empty:
        if 'æ—¥æœŸ' not in df.columns: df['æ—¥æœŸ'] = []
        df['dt'] = pd.to_datetime(df['æ—¥æœŸ'], errors='coerce')
        mask = (df['dt'].dt.month == today.month) & (df['dt'].dt.year == today.year) & (df['ç±»å‹']=='æ”¯å‡º')
        month_spend = df.loc[mask, 'é‡‘é¢'].sum()
    else:
        month_spend = 0.0

    target_date = date(today.year + (1 if today.month==12 and today.day>=payday else 0), 
                      today.month if today.day < payday else (today.month % 12) + 1, payday)
    days_left = (target_date - today).days
    daily_budget = current_asset / max(1, days_left)

    c1, c2, c3, c4 = st.columns(4)
    c1.metric("èµ„äº§ä½™é¢", f"Â¥{current_asset:,.2f}")
    c2.metric("æœ¬æœˆå·²æ”¯", f"Â¥{month_spend:,.2f}")
    c3.metric("è·å‘è–ªæ—¥", f"{days_left} å¤©")
    c4.metric("æ¯æ—¥å¯ç”¨", f"Â¥{daily_budget:.0f}", delta=f"{daily_budget - DEFAULT_TARGET_SPEND:.0f}")
    st.divider()

    t_import, t_add, t_history, t_stats = st.tabs(["ğŸ“¥ æ™ºèƒ½å¯¼å…¥", "âœï¸ æ‰‹åŠ¨è®°è´¦", "ğŸ“‹ å†å²æ˜ç»†", "ğŸ“Š ç»Ÿè®¡æŠ¥è¡¨"])

    with t_import:
        st.info("ğŸ’¡ æç¤ºï¼šæ”¯æŒè¶…é•¿ CSV/æ–‡æœ¬è´¦å•ã€‚ç³»ç»Ÿä¼šè‡ªåŠ¨åˆ†ç‰‡å¹¶å‘å¤„ç†ï¼Œæ— éœ€æ‰‹åŠ¨æ‹†åˆ†ã€‚")
        files = st.file_uploader("æ”¯æŒ PDF/CSV/Excel/å›¾ç‰‡", accept_multiple_files=True)
        if files and st.button("ğŸš€ å¼€å§‹æ™ºèƒ½è§£æ", type="primary"):
            if not api_key:
                st.error("è¯·å…ˆé…ç½® API Key")
                st.stop()
            
            # é¢„è¯»å–
            tasks_doc, tasks_img = [], []
            for f in files:
                f.seek(0)
                bytes_data = f.read()
                item = {"name": f.name, "bytes": bytes_data}
                ext = f.name.split('.')[-1].lower()
                if ext in ['png', 'jpg', 'jpeg']: tasks_img.append(item)
                else: tasks_doc.append(item)
            
            new_df = pd.DataFrame()
            all_debug_logs = []
            
            progress_bar = st.progress(0)
            status_text = st.empty()
            total_files = len(tasks_doc) + len(tasks_img)
            completed_files = 0
            
            # è¿™é‡Œçš„ Executor ç”¨äºæ–‡ä»¶çº§å¹¶å‘ï¼ŒBillParser å†…éƒ¨è¿˜æœ‰åˆ†ç‰‡çº§å¹¶å‘
            # ä¸ºäº†é¿å…çº¿ç¨‹çˆ†ç‚¸ï¼Œè¿™é‡Œ max_workers è®¾å°ä¸€ç‚¹
            with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
                futures = {}
                for t in tasks_doc:
                    f = executor.submit(BillParser.identify_and_parse, t['name'], t['bytes'], api_key)
                    futures[f] = t['name']
                for t in tasks_img:
                    f = executor.submit(BillParser.process_image, t['name'], t['bytes'], api_key)
                    futures[f] = t['name']
                
                for future in concurrent.futures.as_completed(futures):
                    fname = futures[future]
                    try:
                        res, err, dbg = future.result()
                        all_debug_logs.append(dbg)
                        
                        if res is not None and not res.empty:
                            new_df = pd.concat([new_df, res], ignore_index=True)
                            st.toast(f"âœ… {fname} è§£ææˆåŠŸ ({len(res)}æ¡)")
                        else:
                            st.error(f"âŒ {fname}: {err}")
                    except Exception as e:
                        st.error(f"âŒ {fname} å¼‚å¸¸: {e}")
                    
                    completed_files += 1
                    progress_bar.progress(completed_files / total_files)
                    status_text.text(f"å¤„ç†è¿›åº¦: {completed_files}/{total_files}")

            # è°ƒè¯•çœ‹æ¿ (å…³é”®æ›´æ–°)
            if st.session_state.debug_mode:
                st.divider()
                st.subheader("ğŸ”¬ æ·±åº¦è°ƒè¯•çœ‹æ¿")
                for log in all_debug_logs:
                    with st.expander(f"æ–‡ä»¶: {log['file']} (è€—æ—¶ {log.get('total_time', 0):.2f}s)", expanded=False):
                        if 'chunks_info' in log:
                            # è¡¨æ ¼åŒ–æ˜¾ç¤ºåˆ†ç‰‡è¯¦æƒ…
                            chunk_df = pd.DataFrame(log['chunks_info'])
                            st.markdown("#### åˆ†ç‰‡å¤„ç†è¯¦æƒ…")
                            st.dataframe(chunk_df[['chunk', 'status', 'items_count', 'time', 'error']], use_container_width=True)
                            # åŸå§‹ JSON è¯¦æƒ…
                            st.markdown("#### å®Œæ•´è°ƒè¯•æ—¥å¿—")
                            st.json(log)
                        else:
                            st.json(log)

            # ä¿å­˜é€»è¾‘
            if not new_df.empty:
                merged_df, added = DataManager.merge_data(st.session_state.ledger_data, new_df)
                if added > 0:
                    with st.spinner("æ­£åœ¨ä¿å­˜è‡³äº‘ç«¯..."):
                        ok, new_sha = dm.save_data(merged_df, st.session_state.get('github_sha'))
                        if ok:
                            st.session_state.ledger_data = merged_df
                            st.session_state.github_sha = new_sha
                            st.success(f"ğŸ‰ æˆåŠŸå­˜å…¥ {added} æ¡æ–°è®°å½•ï¼")
                        else:
                            st.error("ä¿å­˜å¤±è´¥")
                else:
                    st.warning("æ‰€æœ‰è®°å½•å·²å­˜åœ¨")

    with t_add:
        with st.form("manual_add"):
            c1, c2, c3 = st.columns(3)
            d = c1.date_input("æ—¥æœŸ", date.today())
            t = c2.selectbox("ç±»å‹", ["æ”¯å‡º", "æ”¶å…¥"])
            a = c3.number_input("é‡‘é¢", min_value=0.01)
            c4, c5 = st.columns([1, 2])
            cat = c4.selectbox("åˆ†ç±»", ["é¤é¥®", "äº¤é€š", "è´­ç‰©", "å±…ä½", "å¨±ä¹", "åŒ»ç–—", "å·¥èµ„", "å…¶ä»–"])
            rem = c5.text_input("å¤‡æ³¨")
            if st.form_submit_button("ğŸ’¾ ä¿å­˜", use_container_width=True):
                row = pd.DataFrame([{"æ—¥æœŸ": str(d), "ç±»å‹": t, "é‡‘é¢": a, "åˆ†ç±»": cat, "å¤‡æ³¨": rem}])
                merged, _ = DataManager.merge_data(st.session_state.ledger_data, row)
                ok, new_sha = dm.save_data(merged, st.session_state.get('github_sha'))
                if ok:
                    st.session_state.ledger_data = merged
                    st.session_state.github_sha = new_sha
                    st.success("ä¿å­˜æˆåŠŸ")
                    st.rerun()

    with t_history:
        if not st.session_state.ledger_data.empty:
            st.session_state.ledger_data = DataManager._clean_df_types(st.session_state.ledger_data)
            edited_df = st.data_editor(
                st.session_state.ledger_data,
                use_container_width=True,
                num_rows="dynamic",
                key="history_editor",
                column_config={
                    "é‡‘é¢": st.column_config.NumberColumn(format="Â¥%.2f"),
                    "æ—¥æœŸ": st.column_config.DateColumn(format="YYYY-MM-DD"),
                    "ç±»å‹": st.column_config.SelectboxColumn(options=["æ”¯å‡º", "æ”¶å…¥"]),
                }
            )
            if st.button("ğŸ’¾ ä¿å­˜å˜æ›´"):
                if not edited_df.equals(st.session_state.ledger_data):
                    ok, new_sha = dm.save_data(edited_df, st.session_state.get('github_sha'))
                    if ok:
                        st.session_state.ledger_data = edited_df
                        st.session_state.github_sha = new_sha
                        st.success("æ›´æ–°æˆåŠŸ")
                else:
                    st.info("æ— å˜æ›´")
        else:
            st.info("æš‚æ— æ•°æ®")

    with t_stats:
        if not st.session_state.ledger_data.empty:
            df = st.session_state.ledger_data.copy()
            df['é‡‘é¢'] = pd.to_numeric(df['é‡‘é¢'], errors='coerce').fillna(0)
            df_exp = df[df['ç±»å‹'] == 'æ”¯å‡º']
            c1, c2 = st.columns(2)
            with c1:
                st.subheader("åˆ†ç±»å æ¯”")
                if not df_exp.empty:
                    st.bar_chart(df_exp.groupby("åˆ†ç±»")['é‡‘é¢'].sum())
            with c2:
                st.subheader("æ”¯å‡ºè¶‹åŠ¿")
                if not df_exp.empty:
                    st.line_chart(df_exp.groupby("æ—¥æœŸ")['é‡‘é¢'].sum())
            
            st.divider()
            if st.button("ğŸ¤– ç”Ÿæˆåˆ†ææŠ¥å‘Š"):
                if not api_key: st.error("éœ€ API Key")
                else:
                    with st.spinner("AI åˆ†æä¸­..."):
                        summary = df_exp.sort_values('æ—¥æœŸ', ascending=False).head(100).to_csv(index=False)
                        try:
                            client = get_llm_client(api_key)
                            res = client.chat.completions.create(
                                model=TEXT_MODEL_NAME,
                                messages=[
                                    {"role": "system", "content": "çŠ€åˆ©ç†è´¢å¸ˆã€‚åˆ†ææ¶ˆè´¹ç»“æ„ã€é¢„è­¦å¤§é¢æ”¯å‡ºã€ç»™çœé’±å»ºè®®ã€‚"},
                                    {"role": "user", "content": summary}
                                ]
                            )
                            st.markdown(res.choices[0].message.content)
                        except Exception as e:
                            st.error(str(e))

if __name__ == "__main__":
    main()
