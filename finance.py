import streamlit as st
import pandas as pd
import datetime
from datetime import date
import requests
import json
import base64
from io import StringIO, BytesIO
import os
import fitz  # PyMuPDF
import re
from openai import OpenAI
import concurrent.futures
import time
import plotly.express as px
import plotly.graph_objects as go

# ==================== é¡µé¢é…ç½®ä¸æ ·å¼ ====================
st.set_page_config(page_title="AI æ™ºèƒ½è´¦æœ¬ Pro (PDFè§†è§‰ç‰ˆ)", page_icon="ğŸ’°", layout="wide")

st.markdown("""
<style>
    .stApp { background-color: #f8f9fa; }
    div[data-testid="stMetricValue"] { font-size: 1.8rem; font-weight: bold; color: #1f2937; }
    div[data-testid="stMetricDelta"] { font-size: 0.9rem; margin-top: 5px; }
    .css-1d391kg { padding-top: 2rem; }
    section[data-testid="stSidebar"] > div { padding-top: 2rem; }
    .dataframe { font-size: 0.9rem; }
    /* è®© Tabs æ›´ç´§å‡‘ */
    .stTabs [data-baseweb="tab-list"] { gap: 10px; }
    .stTabs [data-baseweb="tab"] { height: 3rem; }
</style>
""", unsafe_allow_html=True)

# ==================== å¸¸é‡é…ç½® ====================
GITHUB_API_URL = "https://api.github.com"

# --- æ¨¡å‹è®¾ç½® ---
VISION_MODEL_NAME = "Qwen/Qwen3-VL-8B-Instruct"
TEXT_MODEL_NAME = "deepseek-ai/DeepSeek-V3.2"

CHUNK_SIZE = 12000 
BILL_CYCLE_DAY = 10

ALLOWED_CATEGORIES = [
    "é¤é¥®ç¾é£Ÿ", "äº¤é€šå‡ºè¡Œ", "è´­ç‰©æ¶ˆè´¹", "ç”Ÿæ´»æœåŠ¡", "åŒ»ç–—å¥åº·", 
    "å·¥èµ„æ”¶å…¥", "ç†è´¢æŠ•èµ„", "è½¬è´¦çº¢åŒ…", "å…¶ä»–"
]

# ==================== æ ¸å¿ƒå·¥å…·ä¸é€»è¾‘ ====================

def get_llm_client(api_key):
    return OpenAI(api_key=api_key, base_url="https://api.siliconflow.cn/v1")

def get_fiscal_range(current_date, cycle_day=BILL_CYCLE_DAY):
    # ... (ä¿æŒåŸæœ‰é€»è¾‘ä¸å˜)
    if isinstance(current_date, str):
        current_date = datetime.datetime.strptime(current_date, "%Y-%m-%d").date()
    elif isinstance(current_date, datetime.datetime):
        current_date = current_date.date()

    if current_date.day >= cycle_day:
        start_date = date(current_date.year, current_date.month, cycle_day)
        if current_date.month == 12:
            end_date = date(current_date.year + 1, 1, cycle_day) - datetime.timedelta(days=1)
        else:
            end_date = date(current_date.year, current_date.month + 1, cycle_day) - datetime.timedelta(days=1)
    else:
        if current_date.month == 1:
            start_date = date(current_date.year - 1, 12, cycle_day)
        else:
            start_date = date(current_date.year, current_date.month - 1, cycle_day)
        end_date = date(current_date.year, current_date.month, cycle_day) - datetime.timedelta(days=1)
    return start_date, end_date

# --- æ–°å¢ï¼šæ›´å¼ºå¤§çš„æ•°æ®åˆå¹¶ä¸è¦†ç›–é€»è¾‘ ---
def merge_data_with_overwrite(old_df, new_df):
    """
    ç­–ç•¥ï¼šå°†æ–°æ—§æ•°æ®åˆå¹¶ï¼ŒåŸºäºæŒ‡çº¹å»é‡ï¼Œä¿ç•™æœ€æ–°çš„ï¼ˆé€šè¿‡ keep='last' å®ç°è¦†ç›–ï¼‰ã€‚
    æŒ‡çº¹è§„åˆ™ï¼šæ—¥æœŸ(æ ‡å‡†åŒ–)+é‡‘é¢+å¤‡æ³¨å‰6ä½
    """
    if new_df is None or new_df.empty: return old_df, 0
    if old_df.empty: return new_df, len(new_df)
    
    # æ ‡å‡†åŒ–æ—¥æœŸæ ¼å¼ï¼Œç»Ÿä¸€ä¸º YYYY-MM-DD
    def normalize_date(d):
        if pd.isna(d): return ""
        s = str(d)
        # ä¿®å¤æ”¯ä»˜å®çš„ 2025/12/30 -> 2025-12-30
        s = s.replace('/', '-')
        s = s.split(' ')[0] # å»æ‰æ—¶é—´éƒ¨åˆ†
        return s

    # ç»Ÿä¸€æ¸…æ´—
    old_df_clean = old_df.copy()
    new_df_clean = new_df.copy()
    
    for df in [old_df_clean, new_df_clean]:
        df['æ—¥æœŸ'] = df['æ—¥æœŸ'].apply(normalize_date)
        df['å¤‡æ³¨'] = df['å¤‡æ³¨'].astype(str)
        df['é‡‘é¢'] = pd.to_numeric(df['é‡‘é¢'], errors='coerce').fillna(0)
    
    # ç”ŸæˆæŒ‡çº¹ (ä¸ºäº†å»é‡)
    # é’ˆå¯¹æ‹›å•†é“¶è¡Œç­‰åªæœ‰æ—¥æœŸçš„æƒ…å†µï¼Œä¾èµ–å¤‡æ³¨çš„å‰6ä½æ¥åŒºåˆ†åŒä¸€æ—¥åŒä¸€é‡‘é¢çš„ä¸åŒäº¤æ˜“
    def get_fp(d): 
        return d['æ—¥æœŸ'].astype(str) + "_" + d['é‡‘é¢'].astype(str) + "_" + d['å¤‡æ³¨'].str[:6]

    old_df_clean['_fp'] = get_fp(old_df_clean)
    new_df_clean['_fp'] = get_fp(new_df_clean)
    
    # åˆå¹¶å¹¶å»é‡
    merged_df = pd.concat([old_df_clean, new_df_clean], ignore_index=True)
    
    # æ ¸å¿ƒé€»è¾‘ï¼škeep='last' æ„å‘³ç€å¦‚æœ new_df é‡Œæœ‰å’Œ old_df ä¸€æ ·çš„æŒ‡çº¹ï¼Œnew_df çš„æ¡ç›®ä¼šè¦†ç›– old_df çš„
    final_df = merged_df.drop_duplicates(subset=['_fp'], keep='last').drop(columns=['_fp'])
    
    # æ’åºå¹¶è§„èŒƒåŒ–ç±»å‹
    final_df['æ—¥æœŸ'] = pd.to_datetime(final_df['æ—¥æœŸ'], errors='coerce').dt.date
    final_df = final_df.sort_values('æ—¥æœŸ', ascending=False).reset_index(drop=True)

    # è®¡ç®—æ–°å¢/æ›´æ–°æ•°é‡ï¼ˆç®€å•èµ·è§ï¼Œè¿”å›æ–°æ•°æ®æ¡æ•°ï¼‰
    return final_df, len(new_df)


def get_fund_realtime_valuation(fund_code):
    url = f"http://fundgz.1234567.com.cn/js/{fund_code}.js?rt={int(time.time()*1000)}"
    try:
        resp = requests.get(url, timeout=3)
        if resp.status_code == 200:
            content = resp.text
            match = re.search(r'jsonpgz\((.*?)\);', content)
            if match:
                data = json.loads(match.group(1))
                price = data.get('gsz') or data.get('dwjz')
                name = data.get('name')
                time_str = data.get('gztime') or data.get('jzrq')
                return float(price) if price else 0.0, name, time_str
    except Exception:
        pass
    return 0.0, None, None

# ==================== æ•°æ®ç®¡ç†ç±» ====================

class DataManager:
    def __init__(self, github_token=None, repo=None, filename="ledger.csv"):
        self.github_token = github_token
        if repo and repo.startswith("http"):
            self.repo = repo.rstrip("/").split("github.com/")[-1]
        else:
            self.repo = repo
        self.filename = filename
        self.use_github = bool(github_token and self.repo)

    def load_data(self, force_refresh=False):
        if self.use_github:
            if force_refresh: self._fetch_github_content.clear()
            df, sha = self._load_from_github()
        else:
            df, sha = self._load_from_local()

        if "ledger" in self.filename:
            df = self._clean_ledger_types(df)
        elif "funds" in self.filename:
            df = self._clean_fund_types(df)
        return df, sha

    def save_data(self, df, sha=None):
        save_df = df.copy()
        if "ledger" in self.filename and 'æ—¥æœŸ' in save_df.columns:
            save_df['æ—¥æœŸ'] = save_df['æ—¥æœŸ'].astype(str)
        if "funds" in self.filename and 'åŸºé‡‘ä»£ç ' in save_df.columns:
            save_df['åŸºé‡‘ä»£ç '] = save_df['åŸºé‡‘ä»£ç '].astype(str)

        if self.use_github:
            success, new_sha = self._save_to_github(save_df, sha)
            return success, new_sha
        else:
            return self._save_to_local(save_df), None

    @staticmethod
    def _clean_ledger_types(df):
        expected_cols = ["æ—¥æœŸ", "ç±»å‹", "é‡‘é¢", "å¤‡æ³¨", "åˆ†ç±»"]
        for col in expected_cols:
            if col not in df.columns: df[col] = ""
        df['é‡‘é¢'] = pd.to_numeric(df['é‡‘é¢'], errors='coerce').fillna(0.0)
        df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'], errors='coerce').dt.date
        df['æ—¥æœŸ'] = df['æ—¥æœŸ'].fillna(date.today())
        df['ç±»å‹'] = df['ç±»å‹'].astype(str).replace('nan', 'æ”¯å‡º')
        df['åˆ†ç±»'] = df['åˆ†ç±»'].astype(str).replace('nan', 'å…¶ä»–')
        df['å¤‡æ³¨'] = df['å¤‡æ³¨'].astype(str).replace('nan', '')
        return df.sort_values('æ—¥æœŸ', ascending=False).reset_index(drop=True)

    @staticmethod
    def _clean_fund_types(df):
        expected_cols = ["åŸºé‡‘ä»£ç ", "åŸºé‡‘åç§°", "æŒæœ‰ä»½é¢", "æˆæœ¬é‡‘é¢"]
        for col in expected_cols:
            if col not in df.columns: df[col] = ""
        df['åŸºé‡‘ä»£ç '] = df['åŸºé‡‘ä»£ç '].astype(str).str.replace(r'\.0$', '', regex=True).str.zfill(6)
        df['æŒæœ‰ä»½é¢'] = pd.to_numeric(df['æŒæœ‰ä»½é¢'], errors='coerce').fillna(0.0)
        df['æˆæœ¬é‡‘é¢'] = pd.to_numeric(df['æˆæœ¬é‡‘é¢'], errors='coerce').fillna(0.0)
        df['åŸºé‡‘åç§°'] = df['åŸºé‡‘åç§°'].astype(str)
        return df

    def _load_from_local(self):
        if os.path.exists(self.filename):
            try: return pd.read_csv(self.filename, dtype=str), None
            except: pass
        return self._create_empty_df(), None

    def _save_to_local(self, df):
        df.to_csv(self.filename, index=False)
        return True

    @st.cache_data(ttl=300, show_spinner=False)
    def _fetch_github_content(_self):
        # ... (ä¿æŒåŸæœ‰Github APIé€»è¾‘)
        headers = {"Authorization": f"token {_self.github_token}", "Accept": "application/vnd.github.v3+json"}
        url = f"{GITHUB_API_URL}/repos/{_self.repo}/contents/{_self.filename}"
        try:
            response = requests.get(url, headers=headers, timeout=30)
            if response.status_code == 200: return response.json(), None
            elif response.status_code == 404: return None, 404
            else: return None, response.status_code
        except Exception as e: return None, str(e)

    def _load_from_github(self):
        content, error = self._fetch_github_content()
        if content:
            try:
                csv_str = base64.b64decode(content['content']).decode('utf-8')
                df = pd.read_csv(StringIO(csv_str), dtype=str)
                return df, content['sha']
            except: return self._create_empty_df(), content['sha']
        return self._create_empty_df(), None

    def _save_to_github(self, df, sha):
        # ... (ä¿æŒåŸæœ‰Github APIé€»è¾‘)
        headers = {"Authorization": f"token {self.github_token}", "Accept": "application/vnd.github.v3+json"}
        url = f"{GITHUB_API_URL}/repos/{self.repo}/contents/{self.filename}"
        csv_str = df.to_csv(index=False)
        content_bytes = base64.b64encode(csv_str.encode('utf-8')).decode('utf-8')
        data = {"message": f"Update {self.filename}", "content": content_bytes}
        if sha: data["sha"] = sha
        try:
            resp = requests.put(url, headers=headers, data=json.dumps(data), timeout=30)
            if resp.status_code in [200, 201]:
                self._fetch_github_content.clear()
                return True, resp.json()['content']['sha']
            elif resp.status_code in [409, 422]:
                self._fetch_github_content.clear()
                latest_content, _ = self._fetch_github_content()
                if latest_content:
                    data["sha"] = latest_content['sha']
                    retry = requests.put(url, headers=headers, data=json.dumps(data), timeout=30)
                    if retry.status_code in [200, 201]:
                        self._fetch_github_content.clear()
                        return True, retry.json()['content']['sha']
                return False, None
        except: return False, None

    def _create_empty_df(self):
        if "ledger" in self.filename:
            return pd.DataFrame(columns=["æ—¥æœŸ", "ç±»å‹", "é‡‘é¢", "å¤‡æ³¨", "åˆ†ç±»"])
        elif "funds" in self.filename:
            return pd.DataFrame(columns=["åŸºé‡‘ä»£ç ", "åŸºé‡‘åç§°", "æŒæœ‰ä»½é¢", "æˆæœ¬é‡‘é¢"])
        return pd.DataFrame()

# ==================== AI è§£æå™¨ (é«˜åº¦ä¼˜åŒ–) ====================

class BillParser:
    @staticmethod
    def chunk_text_by_lines(text, max_chars=CHUNK_SIZE):
        if len(text) <= max_chars: return [text]
        lines = text.split('\n')
        chunks, current_chunk, current_len = [], [], 0
        for line in lines:
            line_len = len(line) + 1
            if current_len + line_len > max_chars:
                if current_chunk: chunks.append("\n".join(current_chunk))
                current_chunk, current_len = [line], line_len
            else:
                current_chunk.append(line); current_len += line_len
        if current_chunk: chunks.append("\n".join(current_chunk))
        return chunks

    @staticmethod
    def _pdf_to_images(file_bytes):
        images = []
        try:
            with fitz.open(stream=file_bytes, filetype="pdf") as doc:
                for page in doc:
                    # é€‚å½“æ”¾å¤§ä¿è¯æ¸…æ™°åº¦ï¼Œä½†ä¹Ÿå‹ç¼©ä¼ è¾“
                    pix = page.get_pixmap(matrix=fitz.Matrix(2.0, 2.0))
                    images.append(pix.tobytes("png"))
        except Exception as e:
            st.error(f"PDFè½¬å›¾ç‰‡é”™è¯¯: {e}")
        return images

    @staticmethod
    def _call_llm_for_text(text_chunk, api_key):
        client = get_llm_client(api_key)
        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªè´¢åŠ¡æ•°æ®æå–ä¸“å®¶ã€‚
        ä»»åŠ¡ï¼šè§£æä¸‹æ–¹çš„äº¤æ˜“è®°å½•æ–‡æœ¬ã€‚
        
        **æ ¼å¼è¦æ±‚**ï¼š
        è¯·ç›´æ¥è¿”å›ä¸€ä¸ªæ ‡å‡† JSON å¯¹è±¡ï¼ŒåŒ…å«ä¸€ä¸ªåä¸º "records" çš„æ•°ç»„ã€‚ä¸è¦ä½¿ç”¨ Markdown ä»£ç å—ã€‚
        å­—æ®µå®šä¹‰ï¼š
        - date: äº¤æ˜“æ—¥æœŸï¼Œæ ¼å¼ YYYY-MM-DD (å¤„ç† 2025/12/30 è¿™ç§æ ¼å¼)
        - type: "æ”¯å‡º" æˆ– "æ”¶å…¥"
        - amount: çº¯æ•°å­—é‡‘é¢
        - merchant: å•†æˆ·åæˆ–æ‘˜è¦
        - category: æ ¹æ® merchant ä» {ALLOWED_CATEGORIES} ä¸­é€‰æ‹©

        **æ³¨æ„**ï¼šä¸è¦é—æ¼ä»»ä½•è¡Œã€‚å¦‚æœé‡åˆ°ç›¸åŒæ—¥æœŸå’Œé‡‘é¢çš„äº¤æ˜“ï¼Œè¯·åŠ¡å¿…é€šè¿‡ merchant åŒºåˆ†ã€‚
        
        æ–‡æœ¬å†…å®¹ï¼š
        {text_chunk}
        """
        
        try:
            # ä½¿ç”¨ JSON Mode æé«˜æ•ˆç‡å’Œå‡†ç¡®æ€§
            resp = client.chat.completions.create(
                model=TEXT_MODEL_NAME, 
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"}, # å¼ºåˆ¶JSON
                max_tokens=4096, 
                temperature=0.1
            )
            content = resp.choices[0].message.content
            parsed = json.loads(content)
            return parsed.get("records", []), None
        except Exception as e: return None, str(e)

    @staticmethod
    def process_image(filename, image_bytes, api_key, mode="ledger"):
        try:
            b64_img = base64.b64encode(image_bytes).decode('utf-8')
            client = get_llm_client(api_key)

            if mode == "ledger":
                prompt_text = f"""
                åˆ†æè¿™å¼ è´¦å•å›¾ç‰‡ï¼ˆå¯èƒ½æ˜¯é“¶è¡Œ/æ”¯ä»˜å®/å¾®ä¿¡/æ‹›å•†é“¶è¡Œæµæ°´æˆªå›¾ï¼‰ã€‚
                
                **ä»»åŠ¡ç›®æ ‡**ï¼šæå–è¡¨æ ¼ä¸­æ‰€æœ‰äº¤æ˜“æ˜ç»†ã€‚
                
                **å…·ä½“è¦æ±‚**ï¼š
                1. **æ—¥æœŸæ ¼å¼å…¼å®¹**ï¼šå¦‚æœæ˜¯æ”¯ä»˜å®çš„ "2025/12/30"ï¼Œè½¬å½•ä¸º "2025-12-30"ã€‚å¦‚æœæ˜¯åªæœ‰æ—¥æœŸï¼ˆå¦‚æ‹›å•†é“¶è¡Œï¼‰ï¼Œè¯·æŒ‰ä¸Šä¸‹æ–‡é¡ºåºæ¨æ–­ï¼Œä¸éœ€è¦å…·ä½“æ—¶é—´ã€‚
                2. **é‡‘é¢å¤„ç†**ï¼šå¦‚æœ‰æ­£è´Ÿå·ï¼Œ"æ”¯å‡º" è®°ä¸ºæ­£æ•°ï¼Œ"æ”¶å…¥" è®°ä¸ºæ­£æ•°ï¼Œé€šè¿‡ type å­—æ®µåŒºåˆ†ã€‚
                3. **å»é‡/è¦†ç›–**ï¼šå¦‚æœåŒä¸€å¤©æœ‰å¤šç¬”ç›¸åŒé‡‘é¢çš„äº¤æ˜“ï¼Œè¯·åŠ¡å¿…åœ¨ merchant å­—æ®µä¸­ä¿ç•™å”¯ä¸€çš„æ‘˜è¦ä¿¡æ¯ï¼ˆå¦‚ "äº¤æ˜“1"ï¼Œ"äº¤æ˜“2" æˆ–ä¸åŒçš„åº—åï¼‰ï¼Œä»¥ä¾¿åç»­ç³»ç»ŸåŒºåˆ†ã€‚
                4. **åˆ†ç±»**ï¼šä»…ä» {ALLOWED_CATEGORIES} ä¸­é€‰ã€‚
                
                **è¾“å‡ºæ ¼å¼**ï¼š
                ç›´æ¥è¿”å›æ ‡å‡† JSON å¯¹è±¡ï¼Œä¸è¦ Markdownï¼š
                {{"records": [ {{ "date": "2023-01-01", "type": "æ”¯å‡º", "amount": 10.50, "merchant": "è‚¯å¾·åŸº", "category": "é¤é¥®ç¾é£Ÿ" }} ]}}
                """
            else:
                # --- åŸºé‡‘æ¨¡å¼ Prompt ---
                prompt_text = """
                æå–åŸºé‡‘æŒä»“ä¿¡æ¯ã€‚
                **å…³é”®**ï¼š
                1. æå– code (åŸºé‡‘ä»£ç ), name (åŸºé‡‘åç§°), share (æŒæœ‰ä»½é¢), cost (æŒä»“æˆæœ¬)ã€‚
                2. **ä¸¥ç¦**æå– "å¸‚å€¼" å­—æ®µã€‚æˆªå›¾ä¸Šå¦‚æœæœ‰ "æŒæœ‰å¸‚å€¼" æˆ– "å‚è€ƒå¸‚å€¼"ï¼Œè¯·å¿½ç•¥å®ƒã€‚
                3. å¦‚æœä»½é¢æ˜¾ç¤ºä¸º "10000.00"ï¼Œæå– 10000.00ã€‚
                
                è¾“å‡ºæ ¼å¼ï¼š
                {{"records": [ {{ "code": "000001", "name": "åå¤æˆé•¿", "share": 1000, "cost": 1050.00 }} ]}}
                """

            resp = client.chat.completions.create(
                model=VISION_MODEL_NAME,
                messages=[{
                    "role": "user", 
                    "content": [
                        {"type": "text", "text": prompt_text},
                        {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{b64_img}"}}
                    ]
                }],
                response_format={"type": "json_object"}, # å¼ºåˆ¶JSON
                max_tokens=2048
            )
            
            parsed = json.loads(resp.choices[0].message.content)
            data = parsed.get("records", [])
            
            if not data: return None, "æ— æ•°æ®", {}
            
            df = pd.DataFrame(data)

            if mode == "ledger":
                # å­—æ®µæ˜ å°„
                cols_map = {"date": "æ—¥æœŸ", "type": "ç±»å‹", "amount": "é‡‘é¢", "merchant": "å¤‡æ³¨", "category": "åˆ†ç±»"}
                df = df.rename(columns=cols_map)
                for c in cols_map.values(): 
                    if c not in df.columns: df[c] = ""
                # å¼ºåˆ¶æ¸…æ´—æ—¥æœŸä¸­çš„æ–œæ ï¼Œé˜²æ­¢å»é‡å¤±æ•ˆ
                df['æ—¥æœŸ'] = df['æ—¥æœŸ'].astype(str).str.replace('/', '-')
                df['æ—¥æœŸ'] = df['æ—¥æœŸ'].str.split(' ').str[0]
            else:
                # åŸºé‡‘å­—æ®µæ˜ å°„
                cols_map = {"code": "åŸºé‡‘ä»£ç ", "name": "åŸºé‡‘åç§°", "share": "æŒæœ‰ä»½é¢", "cost": "æˆæœ¬é‡‘é¢"}
                df = df.rename(columns=cols_map)
                for c in cols_map.values():
                    if c not in df.columns: df[c] = ""
                df['åŸºé‡‘ä»£ç '] = df['åŸºé‡‘ä»£ç '].astype(str).str.replace(r'\D', '', regex=True).str.zfill(6)

            return df, None, {}
        except Exception as e: return None, str(e), {}

    @staticmethod
    def identify_and_parse(filename, file_bytes, api_key):
        try:
            filename_lower = filename.lower()

            # --- åˆ†æ”¯ 1: PDF (å¹¶å‘å¤„ç†æ¯ä¸€é¡µ) ---
            if filename_lower.endswith('.pdf'):
                images = BillParser._pdf_to_images(file_bytes)
                if not images: return None, "PDFè½¬å›¾ç‰‡å¤±è´¥", {}

                final_pdf_df = pd.DataFrame()
                
                # ä½¿ç”¨å¹¶å‘å¤„ç†æ¯ä¸€é¡µï¼Œæé«˜PDFå¤„ç†é€Ÿåº¦
                with st.status(f"æ­£åœ¨å¤„ç† PDF (å…± {len(images)} é¡µ)...", expanded=False) as status:
                    with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
                        future_to_page = {
                            executor.submit(BillParser.process_image, f"page_{i}", img, api_key, "ledger"): i 
                            for i, img in enumerate(images)
                        }
                        
                        for future in concurrent.futures.as_completed(future_to_page):
                            page_idx = future_to_page[future]
                            try:
                                res, err, _ = future.result()
                                if res is not None and not res.empty:
                                    final_pdf_df = pd.concat([final_pdf_df, res], ignore_index=True)
                                    status.update(label=f"å¤„ç†é¡µ {page_idx+1} å®Œæˆ", state="running")
                            except Exception as e:
                                st.toast(f"ç¬¬ {page_idx+1} é¡µå¤„ç†å¤±è´¥: {e}", icon="âš ï¸")
                    
                    status.update(label="PDF å¤„ç†å®Œæˆ", state="complete", expanded=False)

                if final_pdf_df.empty: return None, "PDFæœªæå–åˆ°æ•°æ®", {}
                return final_pdf_df, None, {}

            # --- åˆ†æ”¯ 2: å›¾ç‰‡ (ç›´æ¥è§†è§‰) ---
            if filename_lower.endswith(('.png', '.jpg', 'jpeg')):
                return BillParser.process_image(filename, file_bytes, api_key, mode="ledger")

            # --- åˆ†æ”¯ 3: æ–‡æœ¬ç±» ---
            content_text = ""
            if filename_lower.endswith('.csv'):
                try: content_text = file_bytes.decode('utf-8')
                except: content_text = file_bytes.decode('gbk', errors='ignore')
            elif filename_lower.endswith(('.xls', '.xlsx')):
                xls = pd.read_excel(BytesIO(file_bytes), sheet_name=None)
                content_text = "\n".join([f"{s}\n{d.to_csv(index=False)}" for s, d in xls.items()])
            
            if not content_text.strip(): return None, "ç©ºæ–‡ä»¶", {}
            
            chunks = BillParser.chunk_text_by_lines(content_text, CHUNK_SIZE)
            all_data = []

            with st.status("æ­£åœ¨è§£ææ–‡æœ¬æ•°æ®..."):
                with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
                    futures = {executor.submit(BillParser._call_llm_for_text, chunk, api_key): chunk for chunk in chunks}
                    for future in concurrent.futures.as_completed(futures):
                        try:
                            res, err = future.result()
                            if not err and res:
                                all_data.extend(res)
                        except: continue

            if not all_data: return None, "æœªæå–åˆ°æ•°æ®", {}
            
            df = pd.DataFrame(all_data)
            cols = {"date": "æ—¥æœŸ", "type": "ç±»å‹", "amount": "é‡‘é¢", "merchant": "å¤‡æ³¨", "category": "åˆ†ç±»"}
            df = df.rename(columns=cols)
            for c in cols.values(): 
                if c not in df.columns: df[c] = ""
            
            # æ–‡æœ¬æ¸…æ´—
            df['æ—¥æœŸ'] = df['æ—¥æœŸ'].astype(str).str.replace('/', '-')
            df['æ—¥æœŸ'] = df['æ—¥æœŸ'].str.split(' ').str[0]
            df['é‡‘é¢'] = pd.to_numeric(df['é‡‘é¢'], errors='coerce').fillna(0)
            
            return df, None, {}

        except Exception as e: return None, str(e), {}

# ==================== ä¸»ç¨‹åºé€»è¾‘ ====================

def main():
    if 'debug_mode' not in st.session_state: st.session_state.debug_mode = False

    # ä¾§è¾¹æ 
    with st.sidebar:
        st.title("âš™ï¸ è®¾ç½®")
        api_key = st.secrets.get("SILICONFLOW_API_KEY") or st.text_input("SiliconFlow API Key", type="password")
        gh_token = st.secrets.get("GITHUB_TOKEN")
        gh_repo = st.secrets.get("GITHUB_REPO")
        
        if gh_token and gh_repo:
            st.success("äº‘ç«¯å·²è¿æ¥")
            if st.button("â˜ï¸ å¼ºåˆ¶åŒæ­¥äº‘ç«¯", use_container_width=True):
                with st.spinner("åŒæ­¥ä¸­..."):
                    dm_ledger = DataManager(gh_token, gh_repo, "ledger.csv")
                    st.session_state.ledger_data, st.session_state.ledger_sha = dm_ledger.load_data(force_refresh=True)
                    dm_funds = DataManager(gh_token, gh_repo, "funds.csv")
                    st.session_state.fund_data, st.session_state.fund_sha = dm_funds.load_data(force_refresh=True)
                    st.rerun()

    # æ•°æ®ç®¡ç†å™¨åˆå§‹åŒ–
    dm_ledger = DataManager(gh_token, gh_repo, "ledger.csv")
    dm_funds = DataManager(gh_token, gh_repo, "funds.csv")

    # æ•°æ®åŠ è½½
    if 'ledger_data' not in st.session_state:
        df, sha = dm_ledger.load_data()
        st.session_state.ledger_data = df
        st.session_state.ledger_sha = sha
    if 'fund_data' not in st.session_state:
        df, sha = dm_funds.load_data()
        st.session_state.fund_data = df
        st.session_state.fund_sha = sha
    if 'fund_prices' not in st.session_state: st.session_state.fund_prices = {}

    st.title("ğŸ’° AI æ™ºèƒ½è´¦æœ¬ Pro (PDFè§†è§‰ç‰ˆ)")
    
    # è´¢åŠ¡å‘¨æœŸ
    default_start, default_end = get_fiscal_range(date.today())
    with st.container():
        col_d1 = st.columns([1])[0]
        with col_d1:
            st.caption(f"å½“å‰ç»Ÿè®¡å‘¨æœŸ (æ¯æœˆ{BILL_CYCLE_DAY}å·åˆ‡åˆ†)")
            date_range = st.date_input("é€‰æ‹©ç»Ÿè®¡æ—¶é—´æ®µ", value=(default_start, default_end), format="YYYY-MM-DD", label_visibility="collapsed")

    # é¡¶éƒ¨æŒ‡æ ‡è®¡ç®—
    df_ledger = st.session_state.ledger_data.copy()
    df_funds = st.session_state.fund_data.copy()

    cash_net = current_income = current_expense = 0.0
    df_period = pd.DataFrame()

    if not df_ledger.empty:
        df_ledger['é‡‘é¢'] = pd.to_numeric(df_ledger['é‡‘é¢'], errors='coerce').fillna(0)
        cash_net = df_ledger[df_ledger['ç±»å‹']=='æ”¶å…¥']['é‡‘é¢'].sum() - df_ledger[df_ledger['ç±»å‹']=='æ”¯å‡º']['é‡‘é¢'].sum()
        if len(date_range) == 2:
            df_ledger['dt'] = pd.to_datetime(df_ledger['æ—¥æœŸ'], errors='coerce').dt.date
            mask_period = (df_ledger['dt'] >= date_range[0]) & (df_ledger['dt'] <= date_range[1])
            df_period = df_ledger[mask_period]
            current_income = df_period[df_period['ç±»å‹']=='æ”¶å…¥']['é‡‘é¢'].sum()
            current_expense = df_period[df_period['ç±»å‹']=='æ”¯å‡º']['é‡‘é¢'].sum()

    # åŸºé‡‘å¸‚å€¼è®¡ç®—
    fund_total_value = 0.0
    if not df_funds.empty:
        df_funds['æŒæœ‰ä»½é¢'] = pd.to_numeric(df_funds['æŒæœ‰ä»½é¢'], errors='coerce').fillna(0)
        for _, row in df_funds.iterrows():
            code = str(row['åŸºé‡‘ä»£ç '])
            if code in st.session_state.fund_prices:
                fund_total_value += row['æŒæœ‰ä»½é¢'] * st.session_state.fund_prices[code]['price']

    st.divider()
    c1, c2, c3, c4 = st.columns(4, gap="medium")
    c1.metric("ğŸ’° æ€»å‡€èµ„äº§", f"Â¥{cash_net + fund_total_value:,.2f}")
    c2.metric("ğŸ“… æœ¬æœŸæ”¯å‡º", f"Â¥{current_expense:,.2f}", delta_color="inverse")
    c3.metric("ğŸ“… æœ¬æœŸæ”¶å…¥", f"Â¥{current_income:,.2f}")
    c4.metric("ğŸ“ˆ åŸºé‡‘å¸‚å€¼", f"Â¥{fund_total_value:,.2f}")

    # Tab å¯¼èˆª
    t_import, t_add, t_history, t_funds, t_stats = st.tabs(["ğŸ“¥ è´¦å•å¯¼å…¥", "âœï¸ æ‰‹åŠ¨è®°è´¦", "ğŸ“‹ å†å²æ˜ç»†", "ğŸ“ˆ åŸºé‡‘æŒä»“", "ğŸ“Š æŠ¥è¡¨"])

    with t_import:
        st.info("ğŸ’¡ **æ™ºèƒ½è¯†åˆ«å‡çº§**ï¼šæ”¯æŒè‡ªåŠ¨è¯†åˆ«æ”¯ä»˜å®(YYYY/MM/DD)ã€å¾®ä¿¡(YYYY-MM-DD)åŠæ‹›å•†é“¶è¡Œæ ¼å¼ã€‚**é‡å¤ä¸Šä¼ ä¼šè‡ªåŠ¨è¦†ç›–æ—§æ•°æ®**ã€‚")
        files = st.file_uploader("ä¸Šä¼ è´¦å• (PDF/å›¾ç‰‡/CSV/Excel)", accept_multiple_files=True)
        
        if files and st.button("ğŸš€ å¼€å§‹è¯†åˆ«ä¸åˆå¹¶", type="primary", use_container_width=True):
            if not api_key: st.error("è¯·é…ç½® API Key"); st.stop()

            all_new_df = pd.DataFrame()
            
            # ä½¿ç”¨ Status å±•ç¤ºæ€»ä½“è¿›åº¦
            with st.status("æ­£åœ¨æ‰¹é‡å¤„ç†æ–‡ä»¶...", expanded=True) as status:
                # é™åˆ¶å¹¶å‘æ–‡ä»¶æ•°ï¼Œé˜²æ­¢è¿‡è½½
                max_files_process = min(len(files), 5) 
                
                with concurrent.futures.ThreadPoolExecutor(max_workers=3) as executor:
                    future_to_file = {}
                    for f in files:
                        f.seek(0)
                        file_bytes = f.read()
                        # æäº¤ä»»åŠ¡
                        future = executor.submit(BillParser.identify_and_parse, f.name, file_bytes, api_key)
                        future_to_file[future] = f.name
                    
                    # æ”¶é›†ç»“æœ
                    for future in concurrent.futures.as_completed(future_to_file):
                        filename = future_to_file[future]
                        try:
                            res, err, _ = future.result()
                            if res is not None and not res.empty:
                                all_new_df = pd.concat([all_new_df, res], ignore_index=True)
                                st.write(f"âœ… `{filename}` æå–æˆåŠŸ")
                            else:
                                st.write(f"âš ï¸ `{filename}` æ— æ•°æ®æˆ–å¤±è´¥: {err}")
                        except Exception as e:
                            st.write(f"âŒ `{filename}` å¤„ç†å¼‚å¸¸: {e}")
                
                status.update(label="æ‰€æœ‰æ–‡ä»¶è¯†åˆ«å®Œæˆ", state="complete", expanded=False)

            if not all_new_df.empty:
                # è°ƒç”¨æ–°çš„è¦†ç›–å¼åˆå¹¶é€»è¾‘
                st.info("æ­£åœ¨è¿›è¡Œæ•°æ®å»é‡ä¸åˆå¹¶...")
                # è¿™é‡Œæ¨¡æ‹Ÿä¸€ç‚¹å»¶è¿Ÿè®©ç”¨æˆ·çœ‹åˆ°å¤„ç†è¿‡ç¨‹
                time.sleep(0.5)
                
                merged_df, change_count = merge_data_with_overwrite(st.session_state.ledger_data, all_new_df)
                
                ok, sha = dm_ledger.save_data(merged_df, st.session_state.get('ledger_sha'))
                if ok:
                    st.session_state.ledger_data = merged_df
                    st.session_state.ledger_sha = sha
                    st.success(f"æ•°æ®æ›´æ–°æˆåŠŸï¼å…±å¤„ç† {len(all_new_df)} æ¡æ•°æ®ã€‚")
                    st.rerun()
                else: st.error("ä¿å­˜å¤±è´¥")
            else:
                st.warning("æœªæå–åˆ°ä»»ä½•æœ‰æ•ˆæ•°æ®ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æ ¼å¼æˆ– API Keyã€‚")

    with t_add:
        with st.form("manual", clear_on_submit=True):
            st.subheader("å¿«é€Ÿè®°è´¦")
            c1, c2, c3 = st.columns(3)
            d = c1.date_input("æ—¥æœŸ", value=date.today(), label_visibility="collapsed")
            t = c2.selectbox("ç±»å‹", ["æ”¯å‡º", "æ”¶å…¥"], label_visibility="collapsed")
            a = c3.number_input("é‡‘é¢", min_value=0.01, step=0.01, label_visibility="collapsed")
            c4, c5 = st.columns([1, 2])
            cat = c4.selectbox("åˆ†ç±»", ALLOWED_CATEGORIES, label_visibility="collapsed")
            rem = c5.text_input("å¤‡æ³¨", placeholder="æ¶ˆè´¹å†…å®¹...", label_visibility="collapsed")
            
            submitted = st.form_submit_button("ä¿å­˜è®°å½•", use_container_width=True)
            if submitted:
                row = pd.DataFrame([{"æ—¥æœŸ":str(d),"ç±»å‹":t,"é‡‘é¢":a,"åˆ†ç±»":cat,"å¤‡æ³¨":rem}])
                merged, _ = merge_data_with_overwrite(st.session_state.ledger_data, row)
                ok, sha = dm_ledger.save_data(merged, st.session_state.get('ledger_sha'))
                if ok: 
                    st.session_state.ledger_data = merged
                    st.session_state.ledger_sha = sha
                    st.success("ä¿å­˜æˆåŠŸ")
                    st.rerun()

    with t_history:
        if st.session_state.ledger_data.empty: st.info("æš‚æ— æ•°æ®")
        else:
            df_show = st.session_state.ledger_data.copy()
            # æ˜¾ç¤ºæ—¶å†æ¸…æ´—ä¸€ä¸‹æ ¼å¼å¥½çœ‹ç‚¹
            df_show['æ—¥æœŸ'] = pd.to_datetime(df_show['æ—¥æœŸ']).dt.strftime('%Y-%m-%d')
            
            edited_df = st.data_editor(
                df_show, 
                use_container_width=True, 
                num_rows="dynamic",
                column_order=["æ—¥æœŸ", "ç±»å‹", "åˆ†ç±»", "é‡‘é¢", "å¤‡æ³¨"],
                key="editor_history",
                column_config={
                    "æ—¥æœŸ": st.column_config.DateColumn("æ—¥æœŸ", format="YYYY-MM-DD"),
                    "åˆ†ç±»": st.column_config.SelectboxColumn(options=ALLOWED_CATEGORIES),
                    "é‡‘é¢": st.column_config.NumberColumn(format="%.2f"),
                    "ç±»å‹": st.column_config.SelectboxColumn(options=["æ”¯å‡º", "æ”¶å…¥"])
                }
            )
            if st.button("ğŸ’¾ ä¿å­˜è¡¨æ ¼ä¿®æ”¹", use_container_width=True):
                # ä¿å­˜å‰è¦å…ˆè½¬æ¢å›æ ‡å‡†æ ¼å¼
                save_df = edited_df.copy()
                save_df['æ—¥æœŸ'] = pd.to_datetime(save_df['æ—¥æœŸ']).dt.date
                ok, sha = dm_ledger.save_data(save_df, st.session_state.get('ledger_sha'))
                if ok:
                    st.session_state.ledger_data = save_df
                    st.session_state.ledger_sha = sha
                    st.success("ä¿®æ”¹å·²ä¿å­˜")
                    time.sleep(0.5); st.rerun()

    with t_funds:
        c_f1, c_f2 = st.columns([1, 3])
        with c_f1:
            # --- åŸºé‡‘å¯¼å…¥ (åªè¯†åˆ«ä»½é¢) ---
            st.subheader("ğŸ“¸ å¯¼å…¥æŒä»“")
            fund_files = st.file_uploader("ä¸Šä¼ æˆªå›¾", type=['png', 'jpg', 'jpeg'], accept_multiple_files=True)
            if fund_files and st.button("è¯†åˆ«æŒä»“", use_container_width=True):
                if not api_key: st.error("è¯·é…ç½® API Key"); st.stop()
                new_funds = pd.DataFrame()
                with st.status("æ­£åœ¨è¯†åˆ«..."):
                    for f in fund_files:
                        f.seek(0)
                        res, err, _ = BillParser.process_image(f.name, f.read(), api_key, mode="fund")
                        if res is not None and not res.empty: 
                            new_funds = pd.concat([new_funds, res], ignore_index=True)
                
                if not new_funds.empty:
                    # åŸºé‡‘ä¹Ÿä½¿ç”¨è¦†ç›–é€»è¾‘ï¼šç›¸åŒä»£ç çš„åŸºé‡‘ï¼Œè¦†ç›–åç§°ã€ä»½é¢å’Œæˆæœ¬
                    old_funds = st.session_state.fund_data
                    # åˆå¹¶
                    combined = pd.concat([old_funds, new_funds], ignore_index=True)
                    # å»é‡ï¼šä»¥åŸºé‡‘ä»£ç ä¸ºå”¯ä¸€é”®ï¼Œä¿ç•™æœ€æ–°çš„
                    final_funds = combined.drop_duplicates(subset=['åŸºé‡‘ä»£ç '], keep='last')
                    
                    ok, sha = dm_funds.save_data(final_funds, st.session_state.get('fund_sha'))
                    if ok:
                        st.session_state.fund_data = final_funds
                        st.success("æŒä»“ä¿¡æ¯å·²æ›´æ–°")
                        st.rerun()

        with c_f2:
            # --- åŸºé‡‘åˆ—è¡¨ä¸è¡Œæƒ…åˆ·æ–° ---
            sub_c1, sub_c2 = st.columns([4, 1])
            sub_c1.subheader("ğŸ“ˆ æŒä»“è¯¦æƒ…")
            if sub_c2.button("ğŸ”„ åˆ·æ–°", use_container_width=True):
                if st.session_state.fund_data.empty: pass
                else:
                    codes = st.session_state.fund_data['åŸºé‡‘ä»£ç '].unique()
                    progress = st.progress(0)
                    new_prices = {}
                    with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
                        future_to_code = {executor.submit(get_fund_realtime_valuation, code): code for code in codes}
                        for i, future in enumerate(concurrent.futures.as_completed(future_to_code)):
                            code, val, name, t_str = future.result()
                            if val > 0: new_prices[code] = {"price": val, "name": name, "time": t_str}
                            progress.progress((i+1)/len(codes))
                    st.session_state.fund_prices.update(new_prices)
                    st.rerun()

            if st.session_state.fund_data.empty: st.info("æš‚æ— æŒä»“")
            else:
                display_data = []
                for _, row in st.session_state.fund_data.iterrows():
                    code = str(row['åŸºé‡‘ä»£ç '])
                    share = float(row['æŒæœ‰ä»½é¢'])
                    cost = float(row['æˆæœ¬é‡‘é¢'])
                    curr_price = 0
                    
                    if code in st.session_state.fund_prices:
                        curr_price = st.session_state.fund_prices[code]['price']
                        
                    mkt_value = share * curr_price if curr_price > 0 else 0
                    profit = mkt_value - cost if (mkt_value > 0 and cost > 0) else 0
                    name = st.session_state.fund_prices.get(code, {}).get('name', row['åŸºé‡‘åç§°'])

                    display_data.append({
                        "åŸºé‡‘ä»£ç ": code, "åŸºé‡‘åç§°": name,
                        "æŒæœ‰ä»½é¢": share, "æœ€æ–°å‡€å€¼": curr_price,
                        "æŒä»“å¸‚å€¼": mkt_value, "ç›ˆäº": profit
                    })
                st.data_editor(pd.DataFrame(display_data), use_container_width=True, column_config={"ç›ˆäº": st.column_config.NumberColumn(format="%.2f")}, disabled=["æœ€æ–°å‡€å€¼", "æŒä»“å¸‚å€¼", "ç›ˆäº"])

    with t_stats:
        if df_period.empty: st.info("æœ¬æœŸæ— æ•°æ®")
        else:
            df_exp = df_period[df_period['ç±»å‹'] == 'æ”¯å‡º']
            col_chart1, col_chart2 = st.columns(2)
            with col_chart1:
                if not df_exp.empty:
                    fig_pie = px.pie(df_exp, values='é‡‘é¢', names='åˆ†ç±»', hole=0.4)
                    st.plotly_chart(fig_pie, use_container_width=True)
                else: st.info("æ— æ”¯å‡º")
            with col_chart2:
                df_sorted = df_ledger.sort_values('æ—¥æœŸ')
                df_sorted['net'] = df_sorted.apply(lambda x: x['é‡‘é¢'] if x['ç±»å‹']=='æ”¶å…¥' else -x['é‡‘é¢'], axis=1)
                df_sorted['asset'] = df_sorted['net'].cumsum()
                st.plotly_chart(px.line(df_sorted, x='æ—¥æœŸ', y='asset'), use_container_width=True)

if __name__ == "__main__":
    main()
